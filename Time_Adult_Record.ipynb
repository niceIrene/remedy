{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7oMgFj8Ve2Oq"
   },
   "source": [
    "# SET UP\n",
    "\n",
    "In this notebook, the results of differing sizes of the dataset can be measured through sampling the dataset and then running the various remedy methods and identification methods to determine the overall time that these methods take. \n",
    "\n",
    "The notebook is divided into the setup where data is read in (and change the number of records to sample), contains the helper functions (for more details, see demo file and refer to paper), and the cells to measure the time for the remedy and identification methods. \n",
    "\n",
    "For more details, refer to the paper.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "772S9aKIfv-9"
   },
   "source": [
    "## Imports and Dataset processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Evnn9BbOfEme"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "import copy\n",
    "from sympy import Symbol\n",
    "from sympy.solvers import solve\n",
    "pd.options.mode.chained_assignment = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 485
    },
    "id": "_vP0FN3vfgjA",
    "outputId": "3358b1cd-9a26-4b25-83a7-2d86d1079189",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-d1026097-da0f-4f87-bd8c-9fef45e4d5f4\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>educational-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>226802</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>89814</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>336951</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>160323</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>198693</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45217</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>257302</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45218</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>154374</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45219</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>151910</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45220</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>201490</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45221</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>287927</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45222 rows Ã— 15 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d1026097-da0f-4f87-bd8c-9fef45e4d5f4')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-d1026097-da0f-4f87-bd8c-9fef45e4d5f4 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-d1026097-da0f-4f87-bd8c-9fef45e4d5f4');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "       age  workclass  fnlwgt  education  educational-num  marital-status  \\\n",
       "0        0          2  226802          1                7               4   \n",
       "1        1          2   89814         11                9               2   \n",
       "2        0          1  336951          7               12               2   \n",
       "3        1          2  160323         15               10               2   \n",
       "4        1          2  198693          0                6               4   \n",
       "...    ...        ...     ...        ...              ...             ...   \n",
       "45217    0          2  257302          7               12               2   \n",
       "45218    1          2  154374         11                9               2   \n",
       "45219    2          2  151910         11                9               6   \n",
       "45220    0          2  201490         11                9               4   \n",
       "45221    2          3  287927         11                9               2   \n",
       "\n",
       "       occupation  relationship  race  gender  capital-gain  capital-loss  \\\n",
       "0               6             3     2       1             0             0   \n",
       "1               4             0     4       1             0             0   \n",
       "2              10             0     4       1             0             0   \n",
       "3               6             0     2       1             1             0   \n",
       "4               7             1     4       1             0             0   \n",
       "...           ...           ...   ...     ...           ...           ...   \n",
       "45217          12             5     4       0             0             0   \n",
       "45218           6             0     4       1             0             0   \n",
       "45219           0             4     4       0             0             0   \n",
       "45220           0             3     4       1             0             0   \n",
       "45221           3             5     4       0             1             0   \n",
       "\n",
       "       hours-per-week  native-country  income  \n",
       "0                   0              38       0  \n",
       "1                   1              38       0  \n",
       "2                   0              38       1  \n",
       "3                   0              38       1  \n",
       "4                   0              38       0  \n",
       "...               ...             ...     ...  \n",
       "45217               0              38       0  \n",
       "45218               0              38       1  \n",
       "45219               0              38       0  \n",
       "45220               0              38       0  \n",
       "45221               0              38       1  \n",
       "\n",
       "[45222 rows x 15 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/niceIrene/remedy/main/datasets/CleanAdult_numerical_cat.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 485
    },
    "id": "MMomhsWE_Bl7",
    "outputId": "d67ac594-fa30-4dc2-f21b-df2be0570c4e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-20facd6b-cc72-407f-83f1-ca3f8ba34f23\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>educational-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>226802</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>89814</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>336951</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>160323</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>198693</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48217</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>145636</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48218</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>155233</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48219</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>123335</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48220</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>378322</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48221</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>194668</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48222 rows Ã— 15 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-20facd6b-cc72-407f-83f1-ca3f8ba34f23')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-20facd6b-cc72-407f-83f1-ca3f8ba34f23 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-20facd6b-cc72-407f-83f1-ca3f8ba34f23');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "       age  workclass  fnlwgt  education  educational-num  marital-status  \\\n",
       "0        0          2  226802          1                7               4   \n",
       "1        1          2   89814         11                9               2   \n",
       "2        0          1  336951          7               12               2   \n",
       "3        1          2  160323         15               10               2   \n",
       "4        1          2  198693          0                6               4   \n",
       "...    ...        ...     ...        ...              ...             ...   \n",
       "48217    1          2  145636         11                9               2   \n",
       "48218    2          2  155233         15               10               0   \n",
       "48219    0          2  123335          0                6               4   \n",
       "48220    0          2  378322         11                9               4   \n",
       "48221    1          2  194668         14               15               4   \n",
       "\n",
       "       occupation  relationship  race  gender  capital-gain  capital-loss  \\\n",
       "0               6             3     2       1             0             0   \n",
       "1               4             0     4       1             0             0   \n",
       "2              10             0     4       1             0             0   \n",
       "3               6             0     2       1             1             0   \n",
       "4               7             1     4       1             0             0   \n",
       "...           ...           ...   ...     ...           ...           ...   \n",
       "48217           3             0     4       1             1             0   \n",
       "48218           0             4     4       0             0             0   \n",
       "48219          11             3     4       0             0             0   \n",
       "48220           2             1     4       1             0             1   \n",
       "48221           9             1     4       1             1             0   \n",
       "\n",
       "       hours-per-week  native-country  income  \n",
       "0                   0              38       0  \n",
       "1                   1              38       0  \n",
       "2                   0              38       1  \n",
       "3                   0              38       1  \n",
       "4                   0              38       0  \n",
       "...               ...             ...     ...  \n",
       "48217               1              38       1  \n",
       "48218               0              38       0  \n",
       "48219               0              38       0  \n",
       "48220               1              38       0  \n",
       "48221               0              38       1  \n",
       "\n",
       "[48222 rows x 15 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change the value here for the number of records\n",
    "value = 6000\n",
    "data = data.sample(value)\n",
    "\n",
    "#for 48K records\n",
    "# data2 = data.sample(3000)\n",
    "# data = pd.concat([data, data2], ignore_index=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NNSifBfwflg8"
   },
   "outputs": [],
   "source": [
    "# get training and testing set\n",
    "\n",
    "\n",
    "\n",
    "columns_all = ['age', 'workclass', 'education', 'educational-num', 'marital-status', \n",
    "                                            'occupation', 'relationship', 'race','gender', 'capital-gain', 'capital-loss',\n",
    "                                            'hours-per-week', 'native-country']\n",
    "columns_compas = ['age', 'education',  'marital-status', 'occupation', \n",
    "                  'relationship', 'race','gender', 'native-country']\n",
    "# compas_y = 'class'\n",
    "compas_y = 'income'\n",
    "def split_train_test(data,test_ratio):\n",
    "    np.random.seed(42)\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    return data.iloc[train_indices],data.iloc[test_indices]\n",
    "\n",
    "def get_train_test(data, split, list_cols, y_label):\n",
    "  all_list = copy.deepcopy(list_cols)\n",
    "  all_list.append(y_label)\n",
    "  data = pd.DataFrame(data, columns = all_list)\n",
    "  train_set,test_set = split_train_test(data,split)\n",
    "  print(len(train_set), \"train +\", len(test_set), \"test\")\n",
    "  train_x = pd.DataFrame(train_set, columns = list_cols)\n",
    "  train_label = train_set[y_label]\n",
    "  test_x = pd.DataFrame(test_set, columns = list_cols)\n",
    "  test_label = test_set[y_label]\n",
    "  return train_x, test_x, train_label, test_label, train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WJQsq95bfpEH",
    "outputId": "cf8fd718-54c1-49db-c171-d80cc236a89c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33756 train + 14466 test\n",
      "       age  workclass  education  educational-num  marital-status  occupation  \\\n",
      "8206     1          2         12               14               0           3   \n",
      "18067    2          0         11                9               2           2   \n",
      "47552    0          2         11                9               4          10   \n",
      "33923    2          2         11                9               4           6   \n",
      "37540    1          2         11                9               2           6   \n",
      "...    ...        ...        ...              ...             ...         ...   \n",
      "41179    1          4          9               13               4          12   \n",
      "18233    1          1          9               13               0           9   \n",
      "9029     1          4         11                9               0           2   \n",
      "48212    1          2          9               13               0           5   \n",
      "36804    1          4         12               14               2          11   \n",
      "\n",
      "       relationship  race  gender  capital-gain  capital-loss  hours-per-week  \\\n",
      "8206              1     4       0             1             0               0   \n",
      "18067             0     4       1             0             0               0   \n",
      "47552             1     1       1             0             0               0   \n",
      "33923             1     4       1             1             0               0   \n",
      "37540             0     4       1             0             0               0   \n",
      "...             ...   ...     ...           ...           ...             ...   \n",
      "41179             1     4       0             0             0               0   \n",
      "18233             4     2       0             0             0               0   \n",
      "9029              1     4       1             1             0               1   \n",
      "48212             1     4       1             0             1               1   \n",
      "36804             0     4       1             1             0               1   \n",
      "\n",
      "       native-country  income  predicted  \n",
      "8206               38       1          1  \n",
      "18067              38       1          0  \n",
      "47552              29       0          0  \n",
      "33923              38       0          0  \n",
      "37540              38       1          0  \n",
      "...               ...     ...        ...  \n",
      "41179              38       0          0  \n",
      "18233              38       0          0  \n",
      "9029               38       1          0  \n",
      "48212              38       1          0  \n",
      "36804              38       1          1  \n",
      "\n",
      "[14466 rows x 15 columns]\n",
      "       age  workclass  education  educational-num  marital-status  occupation  \\\n",
      "8206     1          2         12               14               0           3   \n",
      "18067    2          0         11                9               2           2   \n",
      "47552    0          2         11                9               4          10   \n",
      "33923    2          2         11                9               4           6   \n",
      "37540    1          2         11                9               2           6   \n",
      "...    ...        ...        ...              ...             ...         ...   \n",
      "41179    1          4          9               13               4          12   \n",
      "18233    1          1          9               13               0           9   \n",
      "9029     1          4         11                9               0           2   \n",
      "48212    1          2          9               13               0           5   \n",
      "36804    1          4         12               14               2          11   \n",
      "\n",
      "       relationship  race  gender  capital-gain  capital-loss  hours-per-week  \\\n",
      "8206              1     4       0             1             0               0   \n",
      "18067             0     4       1             0             0               0   \n",
      "47552             1     1       1             0             0               0   \n",
      "33923             1     4       1             1             0               0   \n",
      "37540             0     4       1             0             0               0   \n",
      "...             ...   ...     ...           ...           ...             ...   \n",
      "41179             1     4       0             0             0               0   \n",
      "18233             4     2       0             0             0               0   \n",
      "9029              1     4       1             1             0               1   \n",
      "48212             1     4       1             0             1               1   \n",
      "36804             0     4       1             1             0               1   \n",
      "\n",
      "       native-country  \n",
      "8206               38  \n",
      "18067              38  \n",
      "47552              29  \n",
      "33923              38  \n",
      "37540              38  \n",
      "...               ...  \n",
      "41179              38  \n",
      "18233              38  \n",
      "9029               38  \n",
      "48212              38  \n",
      "36804              38  \n",
      "\n",
      "[14466 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "train_x, test_x, train_label, test_label, train_set, test_set  = get_train_test(data, 0.3, columns_all, compas_y)\n",
    "\n",
    "###################\n",
    "\n",
    "### about decision tree\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param = {'criterion': ['gini', 'entropy'], 'max_depth': [10, 20, 30, 40, 50, 100], 'random_state':[17]}\n",
    "\n",
    "grid = DecisionTreeClassifier(criterion=\"entropy\",max_depth=6, random_state=17)\n",
    "grid.fit(train_x, train_label)\n",
    "\n",
    "data_all = pd.concat([train_x,test_x])\n",
    "data_predict = grid.predict(data_all)\n",
    "test_predict = grid.predict(test_x)\n",
    "data['predicted'] = data_predict\n",
    "\n",
    "test_set['predicted'] = test_predict\n",
    "print(test_set)\n",
    "print(test_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f-rnfp9XfuKM"
   },
   "outputs": [],
   "source": [
    "def fpr_onegroup(true, predict):\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    for i in range(len(true)):\n",
    "        if (true[i] == 0 and predict[i] == 1):\n",
    "            fp += 1 \n",
    "        if(true[i] == 0 and predict[i] == 0):\n",
    "            tn += 1\n",
    "    return fp/(fp+tn)\n",
    "\n",
    "\n",
    "def fnr_onegroup(true, predict):\n",
    "    fn = 0\n",
    "    tp = 0\n",
    "    for i in range(len(true)):\n",
    "        if (true[i] == 1 and predict[i] == 0):\n",
    "            fn += 1 \n",
    "        if(true[i] == 1 and predict[i] == 1):\n",
    "            tp += 1\n",
    "    return fn/(fn+tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "axb85-QngAPX",
    "outputId": "163e3c65-59f6-4c64-84c8-cd5d2feaaa44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fpr is  0.06504290063659009\n",
      "fnr is  0.4910394265232975\n"
     ]
    }
   ],
   "source": [
    "fpr = fpr_onegroup(list(test_label), test_predict)\n",
    "print(\"fpr is \" , fpr)\n",
    "\n",
    "fnr = fnr_onegroup(list(test_label), test_predict)\n",
    "print(\"fnr is \" , fnr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7qzANv0KigX3",
    "outputId": "026172d6-5c02-441d-e0a1-317833104288"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is  0.8281487626157887\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(test_label, test_predict)\n",
    "print(\"accuracy is \" , accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kf1ffb1bggPp"
   },
   "source": [
    "## For entire dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aU9MirNCglkA"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "def get_unfair_group(list_parse, entire = 1):\n",
    "  unfair_group = []\n",
    "  unfair_dict = {}\n",
    "  names = []\n",
    "  for col in columns_compas:\n",
    "    found = False\n",
    "    for item in list_parse:\n",
    "      attr_given = item.split(\"=\")[0]\n",
    "      if col == attr_given:\n",
    "        unfair_group.append(int(item.split(\"=\")[1]))\n",
    "        names.append(attr_given)\n",
    "        unfair_dict[attr_given] = int(item.split(\"=\")[1])\n",
    "        found = True\n",
    "  # if use the entire dataset\n",
    "  if entire:\n",
    "    return unfair_group, names, columns_compas, unfair_dict\n",
    "\n",
    "  return unfair_group, names, list(set(columns_compas).symmetric_difference(set(names))), unfair_dict\n",
    "def candidate_groups(skew_candidates, unfair_dict, ordering, names):\n",
    "  candidate_combos = []\n",
    "  candidate_ind = {}\n",
    "  num = 0\n",
    "  for i in range(len(skew_candidates)+1):\n",
    "    temp_candidate = list(itertools.combinations(skew_candidates, i))\n",
    "    for tc in temp_candidate:\n",
    "      candidate_ind[num] = list(tc)\n",
    "      num += 1\n",
    "  return candidate_ind\n",
    "\n",
    "def name_val_dict(train_set,names):\n",
    "  names_values = {}\n",
    "  for n in names:\n",
    "    names_values[n] = list(train_set[n].unique())\n",
    "  return names_values\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "ZgdhgqYZjTci",
    "outputId": "c8a48fa1-ecf6-4ccb-c9d1-c247ca1de9c8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-d78dd154-ce31-4bfd-a6b6-bf2b3b74b5b6\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8734</th>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8735</th>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8736</th>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8737</th>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8738</th>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8739 rows Ã— 10 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d78dd154-ce31-4bfd-a6b6-bf2b3b74b5b6')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-d78dd154-ce31-4bfd-a6b6-bf2b3b74b5b6 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-d78dd154-ce31-4bfd-a6b6-bf2b3b74b5b6');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "      age  education  marital-status  occupation  relationship  race  gender  \\\n",
       "0       0          0               0           2             3     4       1   \n",
       "1       0          0               0           5             1     4       0   \n",
       "2       0          0               0           5             1     4       1   \n",
       "3       0          0               0           5             3     4       1   \n",
       "4       0          0               0           5             4     4       0   \n",
       "...   ...        ...             ...         ...           ...   ...     ...   \n",
       "8734    2         15               6          11             4     4       0   \n",
       "8735    2         15               6          12             1     4       0   \n",
       "8736    2         15               6          12             4     4       0   \n",
       "8737    2         15               6          13             1     4       0   \n",
       "8738    2         15               6          13             1     4       1   \n",
       "\n",
       "      native-country  income  cnt  \n",
       "0                 38       0    1  \n",
       "1                 38       0    1  \n",
       "2                 38       0    1  \n",
       "3                 38       0    1  \n",
       "4                 38       0    2  \n",
       "...              ...     ...  ...  \n",
       "8734              38       1    1  \n",
       "8735              38       0    4  \n",
       "8736              38       0    2  \n",
       "8737              38       0    1  \n",
       "8738              38       0    1  \n",
       "\n",
       "[8739 rows x 10 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_temp(train_set, names, y_label):\n",
    "  names2 = copy.deepcopy(names)\n",
    "  names2.append(y_label)\n",
    "  temp = train_set[names2]\n",
    "  temp['cnt'] = 0\n",
    "  temp2 = temp.groupby(names2)['cnt'].count().reset_index()\n",
    "  temp2['cnt'].sum()\n",
    "  return temp2, names\n",
    "temp2, names = get_temp(train_set, columns_compas, compas_y)\n",
    "temp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uuEIYpHwjg_H"
   },
   "outputs": [],
   "source": [
    "def get_temp_g(train_set, names, y_label):\n",
    "  names2 = copy.deepcopy(names)\n",
    "  names2.append(y_label)\n",
    "  temp = train_set[names2]\n",
    "  temp['cnt'] = 0\n",
    "  temp_g = temp.groupby(names)['cnt'].count().reset_index()\n",
    "  return temp, temp_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FMrAfuoDtjAQ",
    "outputId": "07a5360c-49c1-4fcf-d85f-ec5cf168b4d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] [] ['age', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'gender', 'native-country'] {}\n",
      "{0: [], 1: ['age'], 2: ['education'], 3: ['marital-status'], 4: ['occupation'], 5: ['relationship'], 6: ['race'], 7: ['gender'], 8: ['native-country'], 9: ['age', 'education'], 10: ['age', 'marital-status'], 11: ['age', 'occupation'], 12: ['age', 'relationship'], 13: ['age', 'race'], 14: ['age', 'gender'], 15: ['age', 'native-country'], 16: ['education', 'marital-status'], 17: ['education', 'occupation'], 18: ['education', 'relationship'], 19: ['education', 'race'], 20: ['education', 'gender'], 21: ['education', 'native-country'], 22: ['marital-status', 'occupation'], 23: ['marital-status', 'relationship'], 24: ['marital-status', 'race'], 25: ['marital-status', 'gender'], 26: ['marital-status', 'native-country'], 27: ['occupation', 'relationship'], 28: ['occupation', 'race'], 29: ['occupation', 'gender'], 30: ['occupation', 'native-country'], 31: ['relationship', 'race'], 32: ['relationship', 'gender'], 33: ['relationship', 'native-country'], 34: ['race', 'gender'], 35: ['race', 'native-country'], 36: ['gender', 'native-country'], 37: ['age', 'education', 'marital-status'], 38: ['age', 'education', 'occupation'], 39: ['age', 'education', 'relationship'], 40: ['age', 'education', 'race'], 41: ['age', 'education', 'gender'], 42: ['age', 'education', 'native-country'], 43: ['age', 'marital-status', 'occupation'], 44: ['age', 'marital-status', 'relationship'], 45: ['age', 'marital-status', 'race'], 46: ['age', 'marital-status', 'gender'], 47: ['age', 'marital-status', 'native-country'], 48: ['age', 'occupation', 'relationship'], 49: ['age', 'occupation', 'race'], 50: ['age', 'occupation', 'gender'], 51: ['age', 'occupation', 'native-country'], 52: ['age', 'relationship', 'race'], 53: ['age', 'relationship', 'gender'], 54: ['age', 'relationship', 'native-country'], 55: ['age', 'race', 'gender'], 56: ['age', 'race', 'native-country'], 57: ['age', 'gender', 'native-country'], 58: ['education', 'marital-status', 'occupation'], 59: ['education', 'marital-status', 'relationship'], 60: ['education', 'marital-status', 'race'], 61: ['education', 'marital-status', 'gender'], 62: ['education', 'marital-status', 'native-country'], 63: ['education', 'occupation', 'relationship'], 64: ['education', 'occupation', 'race'], 65: ['education', 'occupation', 'gender'], 66: ['education', 'occupation', 'native-country'], 67: ['education', 'relationship', 'race'], 68: ['education', 'relationship', 'gender'], 69: ['education', 'relationship', 'native-country'], 70: ['education', 'race', 'gender'], 71: ['education', 'race', 'native-country'], 72: ['education', 'gender', 'native-country'], 73: ['marital-status', 'occupation', 'relationship'], 74: ['marital-status', 'occupation', 'race'], 75: ['marital-status', 'occupation', 'gender'], 76: ['marital-status', 'occupation', 'native-country'], 77: ['marital-status', 'relationship', 'race'], 78: ['marital-status', 'relationship', 'gender'], 79: ['marital-status', 'relationship', 'native-country'], 80: ['marital-status', 'race', 'gender'], 81: ['marital-status', 'race', 'native-country'], 82: ['marital-status', 'gender', 'native-country'], 83: ['occupation', 'relationship', 'race'], 84: ['occupation', 'relationship', 'gender'], 85: ['occupation', 'relationship', 'native-country'], 86: ['occupation', 'race', 'gender'], 87: ['occupation', 'race', 'native-country'], 88: ['occupation', 'gender', 'native-country'], 89: ['relationship', 'race', 'gender'], 90: ['relationship', 'race', 'native-country'], 91: ['relationship', 'gender', 'native-country'], 92: ['race', 'gender', 'native-country'], 93: ['age', 'education', 'marital-status', 'occupation'], 94: ['age', 'education', 'marital-status', 'relationship'], 95: ['age', 'education', 'marital-status', 'race'], 96: ['age', 'education', 'marital-status', 'gender'], 97: ['age', 'education', 'marital-status', 'native-country'], 98: ['age', 'education', 'occupation', 'relationship'], 99: ['age', 'education', 'occupation', 'race'], 100: ['age', 'education', 'occupation', 'gender'], 101: ['age', 'education', 'occupation', 'native-country'], 102: ['age', 'education', 'relationship', 'race'], 103: ['age', 'education', 'relationship', 'gender'], 104: ['age', 'education', 'relationship', 'native-country'], 105: ['age', 'education', 'race', 'gender'], 106: ['age', 'education', 'race', 'native-country'], 107: ['age', 'education', 'gender', 'native-country'], 108: ['age', 'marital-status', 'occupation', 'relationship'], 109: ['age', 'marital-status', 'occupation', 'race'], 110: ['age', 'marital-status', 'occupation', 'gender'], 111: ['age', 'marital-status', 'occupation', 'native-country'], 112: ['age', 'marital-status', 'relationship', 'race'], 113: ['age', 'marital-status', 'relationship', 'gender'], 114: ['age', 'marital-status', 'relationship', 'native-country'], 115: ['age', 'marital-status', 'race', 'gender'], 116: ['age', 'marital-status', 'race', 'native-country'], 117: ['age', 'marital-status', 'gender', 'native-country'], 118: ['age', 'occupation', 'relationship', 'race'], 119: ['age', 'occupation', 'relationship', 'gender'], 120: ['age', 'occupation', 'relationship', 'native-country'], 121: ['age', 'occupation', 'race', 'gender'], 122: ['age', 'occupation', 'race', 'native-country'], 123: ['age', 'occupation', 'gender', 'native-country'], 124: ['age', 'relationship', 'race', 'gender'], 125: ['age', 'relationship', 'race', 'native-country'], 126: ['age', 'relationship', 'gender', 'native-country'], 127: ['age', 'race', 'gender', 'native-country'], 128: ['education', 'marital-status', 'occupation', 'relationship'], 129: ['education', 'marital-status', 'occupation', 'race'], 130: ['education', 'marital-status', 'occupation', 'gender'], 131: ['education', 'marital-status', 'occupation', 'native-country'], 132: ['education', 'marital-status', 'relationship', 'race'], 133: ['education', 'marital-status', 'relationship', 'gender'], 134: ['education', 'marital-status', 'relationship', 'native-country'], 135: ['education', 'marital-status', 'race', 'gender'], 136: ['education', 'marital-status', 'race', 'native-country'], 137: ['education', 'marital-status', 'gender', 'native-country'], 138: ['education', 'occupation', 'relationship', 'race'], 139: ['education', 'occupation', 'relationship', 'gender'], 140: ['education', 'occupation', 'relationship', 'native-country'], 141: ['education', 'occupation', 'race', 'gender'], 142: ['education', 'occupation', 'race', 'native-country'], 143: ['education', 'occupation', 'gender', 'native-country'], 144: ['education', 'relationship', 'race', 'gender'], 145: ['education', 'relationship', 'race', 'native-country'], 146: ['education', 'relationship', 'gender', 'native-country'], 147: ['education', 'race', 'gender', 'native-country'], 148: ['marital-status', 'occupation', 'relationship', 'race'], 149: ['marital-status', 'occupation', 'relationship', 'gender'], 150: ['marital-status', 'occupation', 'relationship', 'native-country'], 151: ['marital-status', 'occupation', 'race', 'gender'], 152: ['marital-status', 'occupation', 'race', 'native-country'], 153: ['marital-status', 'occupation', 'gender', 'native-country'], 154: ['marital-status', 'relationship', 'race', 'gender'], 155: ['marital-status', 'relationship', 'race', 'native-country'], 156: ['marital-status', 'relationship', 'gender', 'native-country'], 157: ['marital-status', 'race', 'gender', 'native-country'], 158: ['occupation', 'relationship', 'race', 'gender'], 159: ['occupation', 'relationship', 'race', 'native-country'], 160: ['occupation', 'relationship', 'gender', 'native-country'], 161: ['occupation', 'race', 'gender', 'native-country'], 162: ['relationship', 'race', 'gender', 'native-country'], 163: ['age', 'education', 'marital-status', 'occupation', 'relationship'], 164: ['age', 'education', 'marital-status', 'occupation', 'race'], 165: ['age', 'education', 'marital-status', 'occupation', 'gender'], 166: ['age', 'education', 'marital-status', 'occupation', 'native-country'], 167: ['age', 'education', 'marital-status', 'relationship', 'race'], 168: ['age', 'education', 'marital-status', 'relationship', 'gender'], 169: ['age', 'education', 'marital-status', 'relationship', 'native-country'], 170: ['age', 'education', 'marital-status', 'race', 'gender'], 171: ['age', 'education', 'marital-status', 'race', 'native-country'], 172: ['age', 'education', 'marital-status', 'gender', 'native-country'], 173: ['age', 'education', 'occupation', 'relationship', 'race'], 174: ['age', 'education', 'occupation', 'relationship', 'gender'], 175: ['age', 'education', 'occupation', 'relationship', 'native-country'], 176: ['age', 'education', 'occupation', 'race', 'gender'], 177: ['age', 'education', 'occupation', 'race', 'native-country'], 178: ['age', 'education', 'occupation', 'gender', 'native-country'], 179: ['age', 'education', 'relationship', 'race', 'gender'], 180: ['age', 'education', 'relationship', 'race', 'native-country'], 181: ['age', 'education', 'relationship', 'gender', 'native-country'], 182: ['age', 'education', 'race', 'gender', 'native-country'], 183: ['age', 'marital-status', 'occupation', 'relationship', 'race'], 184: ['age', 'marital-status', 'occupation', 'relationship', 'gender'], 185: ['age', 'marital-status', 'occupation', 'relationship', 'native-country'], 186: ['age', 'marital-status', 'occupation', 'race', 'gender'], 187: ['age', 'marital-status', 'occupation', 'race', 'native-country'], 188: ['age', 'marital-status', 'occupation', 'gender', 'native-country'], 189: ['age', 'marital-status', 'relationship', 'race', 'gender'], 190: ['age', 'marital-status', 'relationship', 'race', 'native-country'], 191: ['age', 'marital-status', 'relationship', 'gender', 'native-country'], 192: ['age', 'marital-status', 'race', 'gender', 'native-country'], 193: ['age', 'occupation', 'relationship', 'race', 'gender'], 194: ['age', 'occupation', 'relationship', 'race', 'native-country'], 195: ['age', 'occupation', 'relationship', 'gender', 'native-country'], 196: ['age', 'occupation', 'race', 'gender', 'native-country'], 197: ['age', 'relationship', 'race', 'gender', 'native-country'], 198: ['education', 'marital-status', 'occupation', 'relationship', 'race'], 199: ['education', 'marital-status', 'occupation', 'relationship', 'gender'], 200: ['education', 'marital-status', 'occupation', 'relationship', 'native-country'], 201: ['education', 'marital-status', 'occupation', 'race', 'gender'], 202: ['education', 'marital-status', 'occupation', 'race', 'native-country'], 203: ['education', 'marital-status', 'occupation', 'gender', 'native-country'], 204: ['education', 'marital-status', 'relationship', 'race', 'gender'], 205: ['education', 'marital-status', 'relationship', 'race', 'native-country'], 206: ['education', 'marital-status', 'relationship', 'gender', 'native-country'], 207: ['education', 'marital-status', 'race', 'gender', 'native-country'], 208: ['education', 'occupation', 'relationship', 'race', 'gender'], 209: ['education', 'occupation', 'relationship', 'race', 'native-country'], 210: ['education', 'occupation', 'relationship', 'gender', 'native-country'], 211: ['education', 'occupation', 'race', 'gender', 'native-country'], 212: ['education', 'relationship', 'race', 'gender', 'native-country'], 213: ['marital-status', 'occupation', 'relationship', 'race', 'gender'], 214: ['marital-status', 'occupation', 'relationship', 'race', 'native-country'], 215: ['marital-status', 'occupation', 'relationship', 'gender', 'native-country'], 216: ['marital-status', 'occupation', 'race', 'gender', 'native-country'], 217: ['marital-status', 'relationship', 'race', 'gender', 'native-country'], 218: ['occupation', 'relationship', 'race', 'gender', 'native-country'], 219: ['age', 'education', 'marital-status', 'occupation', 'relationship', 'race'], 220: ['age', 'education', 'marital-status', 'occupation', 'relationship', 'gender'], 221: ['age', 'education', 'marital-status', 'occupation', 'relationship', 'native-country'], 222: ['age', 'education', 'marital-status', 'occupation', 'race', 'gender'], 223: ['age', 'education', 'marital-status', 'occupation', 'race', 'native-country'], 224: ['age', 'education', 'marital-status', 'occupation', 'gender', 'native-country'], 225: ['age', 'education', 'marital-status', 'relationship', 'race', 'gender'], 226: ['age', 'education', 'marital-status', 'relationship', 'race', 'native-country'], 227: ['age', 'education', 'marital-status', 'relationship', 'gender', 'native-country'], 228: ['age', 'education', 'marital-status', 'race', 'gender', 'native-country'], 229: ['age', 'education', 'occupation', 'relationship', 'race', 'gender'], 230: ['age', 'education', 'occupation', 'relationship', 'race', 'native-country'], 231: ['age', 'education', 'occupation', 'relationship', 'gender', 'native-country'], 232: ['age', 'education', 'occupation', 'race', 'gender', 'native-country'], 233: ['age', 'education', 'relationship', 'race', 'gender', 'native-country'], 234: ['age', 'marital-status', 'occupation', 'relationship', 'race', 'gender'], 235: ['age', 'marital-status', 'occupation', 'relationship', 'race', 'native-country'], 236: ['age', 'marital-status', 'occupation', 'relationship', 'gender', 'native-country'], 237: ['age', 'marital-status', 'occupation', 'race', 'gender', 'native-country'], 238: ['age', 'marital-status', 'relationship', 'race', 'gender', 'native-country'], 239: ['age', 'occupation', 'relationship', 'race', 'gender', 'native-country'], 240: ['education', 'marital-status', 'occupation', 'relationship', 'race', 'gender'], 241: ['education', 'marital-status', 'occupation', 'relationship', 'race', 'native-country'], 242: ['education', 'marital-status', 'occupation', 'relationship', 'gender', 'native-country'], 243: ['education', 'marital-status', 'occupation', 'race', 'gender', 'native-country'], 244: ['education', 'marital-status', 'relationship', 'race', 'gender', 'native-country'], 245: ['education', 'occupation', 'relationship', 'race', 'gender', 'native-country'], 246: ['marital-status', 'occupation', 'relationship', 'race', 'gender', 'native-country'], 247: ['age', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'gender'], 248: ['age', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'native-country'], 249: ['age', 'education', 'marital-status', 'occupation', 'relationship', 'gender', 'native-country'], 250: ['age', 'education', 'marital-status', 'occupation', 'race', 'gender', 'native-country'], 251: ['age', 'education', 'marital-status', 'relationship', 'race', 'gender', 'native-country'], 252: ['age', 'education', 'occupation', 'relationship', 'race', 'gender', 'native-country'], 253: ['age', 'marital-status', 'occupation', 'relationship', 'race', 'gender', 'native-country'], 254: ['education', 'marital-status', 'occupation', 'relationship', 'race', 'gender', 'native-country'], 255: ['age', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'gender', 'native-country']}\n",
      "{'age': [0, 1, 2], 'education': [11, 7, 0, 15, 9, 1, 8, 12, 14, 10, 4, 5, 6, 2, 3, 13], 'marital-status': [4, 2, 5, 6, 0, 3, 1], 'occupation': [4, 3, 2, 6, 9, 11, 0, 7, 5, 13, 10, 12, 8, 1], 'relationship': [3, 0, 1, 4, 2, 5], 'race': [4, 2, 0, 1, 3], 'gender': [1, 0], 'native-country': [38, 1, 8, 25, 28, 20, 37, 2, 30, 35, 4, 34, 26, 7, 23, 12, 3, 33, 29, 21, 36, 5, 10, 19, 14, 22, 32, 16, 39, 31, 11, 24, 18, 17, 13, 27, 9, 40, 0, 6, 15]}\n"
     ]
    }
   ],
   "source": [
    "unfair_group, unfair_names, skew_candidates, unfair_dict = get_unfair_group([])\n",
    "print(unfair_group, unfair_names, skew_candidates, unfair_dict)\n",
    "all_names = candidate_groups(skew_candidates, unfair_dict, columns_compas, unfair_names)\n",
    "names_values = name_val_dict(train_set, names)\n",
    "print(all_names)\n",
    "print(names_values)\n",
    "all_names_lst = list(all_names.keys())[len(columns_compas)+1:]\n",
    "all_names_lst.reverse()\n",
    "# all_names_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TttueBgvg53C"
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g4TwHPK4hd0A"
   },
   "source": [
    "### General Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wvC9RSgZkD80"
   },
   "outputs": [],
   "source": [
    "def get_one_degree_neighbors(temp2, names, group_lst):\n",
    "    result = []\n",
    "    for i in range(len(group_lst)):\n",
    "        d = copy.copy(temp2)\n",
    "        for k in range(len(group_lst)):\n",
    "            if k != i:\n",
    "                d = d[d[names[k]] == group_lst[k]]\n",
    "            else:\n",
    "                d = d[d[names[k]] != group_lst[k]]\n",
    "        \n",
    "        result.append(d)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sGQalez0kLkE"
   },
   "outputs": [],
   "source": [
    "# compute the pos/neg ration of this neighbor\n",
    "def compute_neighbors(group_lst, result):\n",
    "    # compute the ratio of positive and negative records\n",
    "    start2 = time.time()\n",
    "    pos = 0\n",
    "    neg = 0 \n",
    "    for r in result:\n",
    "        total  = r['cnt'].sum()\n",
    "        r = r[r[compas_y] == 1]\n",
    "        pos += r['cnt'].sum()\n",
    "        neg += total - r['cnt'].sum()\n",
    "    if(neg == 0):\n",
    "        return (pos, neg, -1)\n",
    "    end2 = time.time()\n",
    "    \n",
    "    return(pos, neg, pos/neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijabVWcBjtCh"
   },
   "outputs": [],
   "source": [
    "def compute_diff_add_and_remove(group_lst, temp2, need_positive_or_negative, label, names):\n",
    "    d = copy.copy(temp2)\n",
    "    for i in range(len(group_lst)):\n",
    "        d = d[d[names[i]] == group_lst[i]]\n",
    "        \n",
    "    total =  d['cnt'].sum()\n",
    "    # Total here was 0: here, errors when this is commented out\n",
    "    if total == 0:\n",
    "      return -1\n",
    "    d = d[d[label] == 1]\n",
    "    pos = d['cnt'].sum()\n",
    "   \n",
    "    neg = total - pos\n",
    "    result = get_one_degree_neighbors(temp2,names, group_lst)\n",
    "    neighbors = compute_neighbors(group_lst, result)\n",
    "    if(need_positive_or_negative == 1):\n",
    "        # need pos\n",
    "        x = Symbol('x')\n",
    "      \n",
    "        try:\n",
    "          diff = solve((pos + x)/ (neg - x) - neighbors[2])[0]\n",
    "        except:\n",
    "          return -1\n",
    "            \n",
    "    else:\n",
    "        #need negative\n",
    "        x = Symbol('x')\n",
    "        try:\n",
    "          diff = solve((pos - x)/ (neg + x) - neighbors[2])[0]\n",
    "        except:\n",
    "          return -1\n",
    "   \n",
    "    return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TH7YGCuh69D7"
   },
   "outputs": [],
   "source": [
    "def compute_diff_add(group_lst, temp2, names, label_y, need_positive_or_negative):\n",
    "\n",
    "    d = copy.copy(temp2)\n",
    "    \n",
    "    for i in range(len(group_lst)):\n",
    "\n",
    "        d = d[d[names[i]] == group_lst[i]]\n",
    "    total =  d['cnt'].sum()\n",
    "    d = d[d[label_y] == 1]\n",
    "    pos = d['cnt'].sum()\n",
    "    neg = total - pos\n",
    "    result = get_one_degree_neighbors(temp2, names, group_lst)\n",
    "    neighbors = compute_neighbors(group_lst, result)\n",
    "    if(need_positive_or_negative == 1):\n",
    "        # need pos\n",
    "\n",
    "        x = Symbol('x')\n",
    "        try:\n",
    "          diff = solve((pos + x)/ neg -  neighbors[2])[0]\n",
    "        except:\n",
    "          return -1\n",
    "      \n",
    "    else:\n",
    "        #need negative\n",
    "      \n",
    "        x = Symbol('x')\n",
    "        try:\n",
    "          diff = solve(pos/ (neg + x) -  neighbors[2])[0]\n",
    "        except:\n",
    "          return -1\n",
    "    return diff\n",
    "\n",
    "def compute_diff_remove(group_lst, temp2, names, label_y, need_positive_or_negative):\n",
    "    d = copy.copy(temp2)\n",
    "    for i in range(len(group_lst)):\n",
    "      \n",
    "        d = d[d[names[i]] == group_lst[i]]\n",
    "    total =  d['cnt'].sum()\n",
    "    d = d[d[label_y] == 1]\n",
    "    pos = d['cnt'].sum()\n",
    "    neg = total - pos\n",
    "    result = get_one_degree_neighbors(temp2, names, group_lst)\n",
    "    neighbors = compute_neighbors(group_lst, result)\n",
    "    if(need_positive_or_negative == 1):\n",
    "        # need pos, remove some neg\n",
    "        x = Symbol('x')\n",
    "        try:\n",
    "          diff = solve( pos/ (neg - x) -  neighbors[2])[0]\n",
    "        except:\n",
    "          return -1\n",
    "       \n",
    "    else:\n",
    "        #need negative\n",
    "        x = Symbol('x')\n",
    "        try:\n",
    "          diff = solve((pos -x )/ neg -  neighbors[2])[0]\n",
    "        except:\n",
    "          return -1 \n",
    "      \n",
    "    return diff\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4iEQ83nxkQr4"
   },
   "source": [
    "### Optimized Helper Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vuxfZHSlhXNR"
   },
   "outputs": [],
   "source": [
    "# helper function for optimized\n",
    "def compute_neighbors_opt(group_lst,lst_of_counts, pos, neg):\n",
    "\n",
    "    times = len(group_lst)\n",
    "    pos_cnt = 0\n",
    "    neg_cnt = 0\n",
    "    for i in range(times):\n",
    "        df_groupby = lst_of_counts[i]\n",
    "        temp_group_lst_pos = copy.copy(group_lst)\n",
    "        temp_group_lst_neg = copy.copy(group_lst)\n",
    "        del temp_group_lst_pos[i]\n",
    "        del temp_group_lst_neg[i]\n",
    "        # count positive\n",
    "        temp_group_lst_pos.append(1)\n",
    "        group_tuple_pos = tuple(temp_group_lst_pos)\n",
    "        if group_tuple_pos in df_groupby.keys():\n",
    "            pos_cnt += df_groupby[group_tuple_pos]\n",
    "        else:\n",
    "            pos_cnt += 0\n",
    "        # count negative\n",
    "        temp_group_lst_neg.append(0)\n",
    "        group_tuple_neg = tuple(temp_group_lst_neg)\n",
    "        if group_tuple_neg in df_groupby.keys():\n",
    "            neg_cnt += df_groupby[group_tuple_neg]\n",
    "        else:\n",
    "            neg_cnt += 0\n",
    "    pos_val = pos_cnt - times* pos\n",
    "    neg_val = neg_cnt - times* neg\n",
    "   \n",
    "\n",
    "    if neg_val == -1 or (neg_val == 0 and pos_val == 0):\n",
    "        return (pos_val, neg_val, -1)\n",
    "    if pos_val == 0 or neg_val == 0:\n",
    "        return (pos_val, neg_val, 0)\n",
    "\n",
    "\n",
    "    return (pos_val, neg_val, pos_val/neg_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J8Iny4kJhm4V"
   },
   "outputs": [],
   "source": [
    "# get the list of neighbors\n",
    "def get_one_degree_neighbors_opt(group_lst):\n",
    "    start1 = time.time()\n",
    "    result = []\n",
    "    for i in range(len(group_lst)):\n",
    "        d = copy.copy(group_lst)\n",
    "        d[i] = 'x'\n",
    "        result.append(d)\n",
    "    end1 = time.time()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1MmwA0X5hofE"
   },
   "outputs": [],
   "source": [
    "def determine_problematic_opt(group_lst, names, temp2, lst_of_counts, label, threshold= 0.3):\n",
    "    #0: ok group, 1: need negative records, 2: need positive records\n",
    "    d = copy.copy(temp2)\n",
    "    for i in range(len(group_lst)):\n",
    "        d = d[d[names[i]] == group_lst[i]]\n",
    "    total =  d['cnt'].sum()\n",
    "    d = d[d[label] == 1]\n",
    "    pos = d['cnt'].sum()\n",
    "    neg = total - pos\n",
    "    neighbors = compute_neighbors_opt(group_lst,lst_of_counts, pos, neg)\n",
    "    if(neighbors[2] == -1):\n",
    "        # there is no neighbors\n",
    "        return 0\n",
    "    if(total > 30):\n",
    "        # need to be large enough, need to adjust with different datasets.\n",
    "        if neg == 0:\n",
    "            if (pos > neighbors[2]):\n",
    "                return 1\n",
    "            if(pos <= neighbors[2]):\n",
    "                return 0\n",
    "        if (pos/(neg) - neighbors[2] > threshold):\n",
    "            # too many positive records\n",
    "            return 1\n",
    "        if (neighbors[2] - pos/(neg) > threshold):\n",
    "            return 2\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A76VlG-bhqqS"
   },
   "outputs": [],
   "source": [
    "def compute_problematic_opt(temp2, temp_g, names, label, lst_of_counts):\n",
    "    need_pos = []\n",
    "    need_neg = []\n",
    "    for index, row in temp_g.iterrows():\n",
    "        group_lst = []\n",
    "        for n in names:\n",
    "            group_lst.append(row[n])\n",
    "        problematic = determine_problematic_opt(group_lst, names, temp2, lst_of_counts,label)\n",
    "\n",
    "        if(problematic == 1):\n",
    "            if group_lst not in need_neg:\n",
    "                need_neg.append(group_lst)\n",
    "        if(problematic == 2):\n",
    "            if group_lst not in need_pos:\n",
    "                need_pos.append(group_lst)\n",
    "    return need_pos, need_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tZd2c6PAhtTH"
   },
   "outputs": [],
   "source": [
    "# build the list of X00\n",
    "def compute_lst_of_counts(temp, names, label):\n",
    "    # get the list of group-by attributes\n",
    "    lst_of_counts = []\n",
    "    for i in range(len(names)):\n",
    "        grp_names = copy.copy(names)\n",
    "        del grp_names[i]\n",
    "        grp_names.append(label)\n",
    "        temp_df = temp.groupby(grp_names)['cnt'].count()\n",
    "        lst_of_counts.append(temp_df)\n",
    "    return lst_of_counts\n",
    "    \n",
    "def get_tuple(group_lst):\n",
    "    return tuple(group_lst) \n",
    "\n",
    "\n",
    "def get_temp_g(train_set, names, y_label):\n",
    "  names2 = copy.deepcopy(names)\n",
    "  names2.append(y_label)\n",
    "  temp = train_set[names2]\n",
    "  temp['cnt'] = 0\n",
    "  temp_g = temp.groupby(names)['cnt'].count().reset_index()\n",
    "  return temp, temp_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnQ09ffAkalO"
   },
   "source": [
    "### Remedy Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vMrjlEciiv--"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "def pref_sampling_opt(train_set, cols_given, label, need_pos, need_neg):\n",
    "    if len(need_pos)+ len(need_neg) > 0:\n",
    "        temp_train_x = pd.DataFrame(train_set, columns = columns_all)\n",
    "        temp_train_label = pd.DataFrame(train_set, columns = [label])\n",
    "        temp_train_label = temp_train_label[label]\n",
    "        temp_train_label = temp_train_label.astype('int')\n",
    "        mnb = MultinomialNB()\n",
    "        mnb = mnb.fit(temp_train_x, temp_train_label)\n",
    "        probs = mnb.predict_proba(temp_train_x)[:,0]\n",
    "        train_set[\"prob\"] = abs(probs - 0.5)\n",
    "    new_train_set = pd.DataFrame(columns = list(train_set.columns))\n",
    "    updated_pos = 0\n",
    "    for i in need_pos:\n",
    "        # needs to updated more positive records\n",
    "        \n",
    "        temp_df = copy.deepcopy(train_set)\n",
    "        for n in range(len(i)):\n",
    "          temp_df = temp_df[temp_df[cols_given[n]] == i[n]]\n",
    "        # update the skew and diff\n",
    "        idx = list(temp_df.index)\n",
    "        train_set.loc[idx, 'skewed'] = 1\n",
    "        idx_pos = list(temp_df[(getattr(temp_df, label) == 1)].index)\n",
    "        if(len(idx_pos) == 0):\n",
    "          # if there is no positive\n",
    "          idx_neg = list(temp_df[(getattr(temp_df, label) == 0)].index)\n",
    "          neg_ranked = train_set.loc[idx_neg].sort_values(by=\"prob\", ascending=True)\n",
    "          new_train_set = pd.concat([new_train_set, neg_ranked], ignore_index=True)\n",
    "          continue\n",
    "        idx_neg = list(temp_df[(getattr(temp_df, label) == 0)].index)\n",
    "        pos_ranked = train_set.loc[idx_pos].sort_values(by=\"prob\", ascending=True)\n",
    "        neg_ranked = train_set.loc[idx_neg].sort_values(by=\"prob\", ascending=True)\n",
    "        diff = compute_diff_add_and_remove(i, temp2,  1, compas_y, names)\n",
    "        if diff == -1:\n",
    "          new_train_set = pd.concat([new_train_set, pos_ranked], ignore_index=True)\n",
    "          new_train_set = pd.concat([new_train_set, neg_ranked], ignore_index=True)\n",
    "          continue\n",
    "        train_set.loc[idx, 'diff'] = int(diff)\n",
    "        cnt = int(train_set.loc[idx_pos[0]][\"diff\"])\n",
    "        updated_pos += cnt * 2 \n",
    "        # add more records when there are not enough available records\n",
    "        new_train_set = pd.concat([new_train_set, pos_ranked], ignore_index=True)\n",
    "        temp_cnt = cnt\n",
    "        if len(pos_ranked) >= temp_cnt:\n",
    "            new_train_set = pd.concat([new_train_set,pos_ranked[0:cnt]], ignore_index=True)\n",
    "        else:\n",
    "            while(temp_cnt > 0 ):\n",
    "                new_train_set = pd.concat([new_train_set,pos_ranked[0:temp_cnt]], ignore_index=True) \n",
    "            # duplicate the dataframe\n",
    "                temp_cnt = temp_cnt - len(pos_ranked)\n",
    "        # duplicate the top cnt records from the pos\n",
    "        # remove the top cnt records from the neg\n",
    "        if cnt == 0:\n",
    "          new_train_set = pd.concat([new_train_set, neg_ranked], ignore_index=True)\n",
    "        else:\n",
    "          new_train_set = pd.concat([new_train_set, neg_ranked[cnt-1:-1]], ignore_index=True)\n",
    "    updated_neg = 0\n",
    "    # adding more records to the need_neg set\n",
    "    for i in need_neg:\n",
    "        # list of idx belongs to this group\n",
    "        temp_df = copy.deepcopy(train_set)\n",
    "        for n in range(len(i)):\n",
    "          temp_df = temp_df[temp_df[cols_given[n]] == i[n]]\n",
    "        # update the skew and diff\n",
    "        idx = list(temp_df.index)\n",
    "        train_set.loc[idx, 'skewed'] = 1\n",
    "        idx_pos = list(temp_df[(getattr(temp_df, label) == 1)].index)\n",
    "        idx_neg = list(temp_df[(getattr(temp_df, label) == 0)].index)\n",
    "        if(len(idx_neg) == 0):\n",
    "          pos_ranked = train_set.loc[idx_pos].sort_values(by=\"prob\", ascending=True)\n",
    "          new_train_set = pd.concat([new_train_set, pos_ranked], ignore_index=True)\n",
    "          continue\n",
    "        pos_ranked = train_set.loc[idx_pos].sort_values(by=\"prob\", ascending=True)\n",
    "        neg_ranked = train_set.loc[idx_neg].sort_values(by=\"prob\", ascending=True)\n",
    "        diff = compute_diff_add_and_remove(i, temp2, 0, compas_y, names)\n",
    "        if diff == -1:\n",
    "          new_train_set = pd.concat([new_train_set, neg_ranked], ignore_index=True)\n",
    "          new_train_set = pd.concat([new_train_set, pos_ranked], ignore_index=True)\n",
    "          continue\n",
    "        train_set.loc[idx, 'diff'] = int(diff)\n",
    "        cnt = int(train_set.loc[idx_pos[0]][\"diff\"])\n",
    "        updated_neg += cnt * 2 \n",
    "        # add more records when there are not enough available records\n",
    "        new_train_set = pd.concat([new_train_set, neg_ranked], ignore_index=True)\n",
    "        temp_cnt = cnt\n",
    "        if len(neg_ranked) >= temp_cnt:\n",
    "            new_train_set = pd.concat([new_train_set,neg_ranked[0:cnt]], ignore_index=True)\n",
    "        else:\n",
    "            while(temp_cnt > 0 ):\n",
    "                new_train_set = pd.concat([new_train_set,neg_ranked[0:temp_cnt]], ignore_index=True) \n",
    "            # duplicate the dataframe\n",
    "                temp_cnt = temp_cnt - len(neg_ranked)\n",
    "        # duplicate the top cnt records from the pos\n",
    "        # remove the top cnt records from the neg\n",
    "        if cnt ==0:\n",
    "          new_train_set = pd.concat([new_train_set, pos_ranked], ignore_index=True)\n",
    "        \n",
    "        else:\n",
    "          new_train_set = pd.concat([new_train_set, pos_ranked[cnt-1:-1]], ignore_index=True)\n",
    "\n",
    "    # add the other irrelavant items:\n",
    "    idx_irr = list(train_set[train_set['skewed'] == 0].index)\n",
    "    irr_df = train_set.loc[idx_irr]\n",
    "    new_train_set = pd.concat([new_train_set, irr_df], ignore_index=True)\n",
    "\n",
    "    new_train_set.reset_index()\n",
    "    return new_train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h9m0e5GbVL-d"
   },
   "outputs": [],
   "source": [
    "def round_int(x):\n",
    "    if x in [float(\"-inf\"),float(\"inf\")]: return 0\n",
    "    return int(round(x))\n",
    "\n",
    "\n",
    "def make_remove(d, group_lst, diff, names, label_y, need_positive_or_negative):\n",
    "    temp = copy.deepcopy(d)\n",
    "    for i in range(len(group_lst)):\n",
    "        att_name = names[i]\n",
    "        temp = temp[(temp[att_name] == group_lst[i])]\n",
    "    temp = temp[(temp[label_y] == need_positive_or_negative)]\n",
    "    # randomly generated diff samples\n",
    "        #generated = temp\n",
    "        # the number needed is more than the not needed numbers.\n",
    "    if(diff>len(temp)):\n",
    "        diff = len(temp)\n",
    "    generated = temp.sample(n = diff, replace = False, axis = 0)\n",
    "    return generated.index\n",
    "\n",
    "\n",
    "def naive_downsampling(d, temp2, names, need_pos, need_neg, label_y):\n",
    "    # add more records for all groups\n",
    "    # The smote algorithm to boost the coverage\n",
    "    for r in need_pos:\n",
    "    # add more positive records\n",
    "        # determine how many points to add\n",
    "        diff = compute_diff_remove(r, temp2, names, label_y, need_positive_or_negative = 1)\n",
    "        if diff == -1:\n",
    "          continue\n",
    "        diff = round_int(diff)\n",
    "        # add more records\n",
    "        samples_to_remove = make_remove(d, r, diff, names, label_y, need_positive_or_negative = 0)\n",
    "        d.drop(index  = samples_to_remove, inplace = True)\n",
    "    for k in need_neg:\n",
    "        diff = compute_diff_remove(k, temp2, names, label_y, need_positive_or_negative = 0)\n",
    "        if diff == -1:\n",
    "          continue\n",
    "        diff = round_int(diff)\n",
    "        samples_to_remove = make_remove(d, k, diff, names, label_y, need_positive_or_negative = 1)\n",
    "        d.drop(index  = samples_to_remove, inplace = True)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tINoeYnSnWn4"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "def round_int(x):\n",
    "    if x in [float(\"-inf\"),float(\"inf\")]: return 0\n",
    "    return int(round(x))\n",
    "\n",
    "def get_depromotion(d, diff, group_lst, names, label_y, flag_depro):\n",
    "    input_test = pd.DataFrame(d, columns = columns_compas)\n",
    "    clf = MultinomialNB()\n",
    "    temp_train_label = pd.DataFrame(d, columns = [label_y])\n",
    "    temp_train_label = temp_train_label[label_y]\n",
    "    temp_train_label = temp_train_label.astype('int')\n",
    "    clf = clf.fit(input_test, temp_train_label)\n",
    "    prob  = clf.predict_proba(input_test)[:,0]\n",
    "    select = copy.deepcopy(d)\n",
    "    select['prob'] = prob # the higher the probablity is, the more likely for it to be 0\n",
    "    # filter out those belongs to this group\n",
    "    for i in range(len(group_lst)):\n",
    "        att_name = names[i]\n",
    "        select = select[(select[att_name] == group_lst[i])]\n",
    "    select = select[(select[label_y] == flag_depro)]\n",
    "    # rank them according to the probability\n",
    "    # filp the records and remove the records from d\n",
    "    if (flag_depro == 0):\n",
    "        select.sort_values(by=\"prob\", ascending=True, inplace=True)\n",
    "        select[label_y] = 1\n",
    "    else:\n",
    "        select.sort_values(by=\"prob\", ascending=False, inplace=True)\n",
    "        select[label_y] = 0\n",
    "    head = select.head(diff)\n",
    "    index_list = []\n",
    "    index_list = list(head.index)\n",
    "    d.drop(index_list,inplace = True)\n",
    "    head.drop(columns = ['prob'],inplace = True)\n",
    "    return head\n",
    "\n",
    "\n",
    "\n",
    "def naive_massaging(d, temp2, names, need_pos, need_neg,label_y):\n",
    "    # add more records for all groups\n",
    "    # The smote algorithm to boost the coverage\n",
    "    for r in need_pos:\n",
    "        # print(\"adding more positive\")\n",
    "    # add more positive records\n",
    "        # determine how many points to add\n",
    "        diff = compute_diff_add_and_remove(r, temp2, 1, label_y, names)\n",
    "        if diff == -1:\n",
    "          continue\n",
    "        diff =  round_int(diff)\n",
    "        # add more records\n",
    "        #0 for promotion\n",
    "        samples_to_add = get_depromotion(d, diff, r, names, label_y, flag_depro = 0)\n",
    "        d = pd.concat([d, samples_to_add])\n",
    "    for k in need_neg:\n",
    "        diff = compute_diff_add_and_remove(k, temp2, 0, label_y, names)\n",
    "        if diff == -1:\n",
    "          continue\n",
    "        diff =  round_int(diff)\n",
    "        #1 for demotion\n",
    "        samples_to_add = get_depromotion(d, diff, k, names, label_y, flag_depro = 1)\n",
    "        d = pd.concat([d, samples_to_add])\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SGdoCnWLlkc4"
   },
   "source": [
    "# OPTIMIZED ALGORITHM + REMEDY METHODS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YABr1WysUiVb"
   },
   "outputs": [],
   "source": [
    "def candidate_groups(skew_candidates):\n",
    "  candidate_combos = []\n",
    "  candidate_ind = {}\n",
    "  num = 0\n",
    "  for i in range(len(skew_candidates)+1):\n",
    "    temp_candidate = list(itertools.combinations(skew_candidates, i))\n",
    "    for tc in temp_candidate:\n",
    "      candidate_ind[num] = list(tc)\n",
    "      num += 1\n",
    "  return candidate_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rrShsLQLV3-8"
   },
   "outputs": [],
   "source": [
    "filter_count = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4BZb2fmjl74h",
    "outputId": "b3ed6fe5-cecb-4d31-e9e9-b3160a1b6adb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "started pref sampling\n",
      "end down sampling\n",
      "end massage sampling\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "new_train_data = copy.deepcopy(train_set)\n",
    "import csv\n",
    "\n",
    "with open('adult_records.csv', 'a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    minus = 0\n",
    "    all_names = candidate_groups(columns_compas[:len(columns_compas)- minus])\n",
    "    all_names_lst = list(all_names.keys())[len(columns_compas)-minus+1:]\n",
    "    all_names_lst.reverse()\n",
    "    list_write = []\n",
    "    list_write.append(\"Adult\")\n",
    "    list_write.append(\"Pref_opt\")\n",
    "    list_write.append(data.shape[0])\n",
    "    list_write.append(train_set.shape[0])\n",
    "    list_write.append(columns_compas[:len(columns_compas)-minus])\n",
    "    list_write.append(len(columns_compas[:len(columns_compas)-minus]))\n",
    "    excute_time = 0\n",
    "    excute_time1 = 0\n",
    "    print(\"started pref sampling\")\n",
    "    for a in all_names_lst:\n",
    "        temp2, names = get_temp(new_train_data, all_names[a], compas_y)\n",
    "        temp, temp_g = get_temp_g(new_train_data, names, compas_y)\n",
    "        temp_g = temp_g[temp_g['cnt'] > filter_count]\n",
    "        lst_of_counts = compute_lst_of_counts(temp, names, compas_y)\n",
    "        start = time.time()    \n",
    "        need_pos, need_neg = compute_problematic_opt(temp2, temp_g, names, compas_y, lst_of_counts)\n",
    "        end = time.time()\n",
    "        excute_time += end - start\n",
    "\n",
    "        new_train_data['skewed'] = 0\n",
    "        new_train_data[\"diff\"] = 0\n",
    "        \n",
    "        start1 = time.time()    \n",
    "        new_train_data = pref_sampling_opt(new_train_data, names, compas_y, need_pos, need_neg)\n",
    "        end1 = time.time()\n",
    "        excute_time1 += end1 - start1   \n",
    "    list_write.append(excute_time)\n",
    "    list_write.append(excute_time1)\n",
    "    writer.writerow(list_write)\n",
    "\n",
    "    print(\"end pref sampling\")\n",
    "\n",
    "    new_train_data = copy.deepcopy(train_set)\n",
    "    list_write = []\n",
    "    list_write.append(\"Adult\")\n",
    "    list_write.append(\"Downsampling_opt\")\n",
    "    list_write.append(data.shape[0])\n",
    "    list_write.append(train_set.shape[0])\n",
    "    list_write.append(columns_compas[:len(columns_compas)-minus])\n",
    "    list_write.append(len(columns_compas[:len(columns_compas)-minus]))\n",
    "    excute_time = 0\n",
    "    excute_time1 = 0\n",
    "    print(\"start down sampling\")\n",
    "    for a in all_names_lst:\n",
    "      temp2, names = get_temp(new_train_data, all_names[a], compas_y)\n",
    "      temp, temp_g = get_temp_g(new_train_data, names, compas_y)\n",
    "      temp_g = temp_g[temp_g['cnt'] > filter_count]\n",
    "      lst_of_counts = compute_lst_of_counts(temp, names, compas_y)\n",
    "      start = time.time()    \n",
    "      need_pos, need_neg = compute_problematic_opt(temp2, temp_g, names, compas_y, lst_of_counts)\n",
    "      end = time.time()\n",
    "      excute_time += end - start\n",
    "      new_train_data['skewed'] = 0\n",
    "      new_train_data[\"diff\"] = 0\n",
    "\n",
    "      start1 = time.time()\n",
    "      new_train_data = naive_downsampling(new_train_data, temp2, names, need_pos, need_neg, compas_y)\n",
    "      end1 = time.time()\n",
    "      excute_time1 += end1 - start1   \n",
    "    list_write.append(excute_time)\n",
    "    list_write.append(excute_time1)\n",
    "    writer.writerow(list_write)\n",
    "    print(\"end down sampling\")\n",
    "\n",
    "    new_train_data = copy.deepcopy(train_set)\n",
    "    list_write = []\n",
    "    list_write.append(\"Adult\")\n",
    "    list_write.append(\"Massaging_opt\")\n",
    "    list_write.append(data.shape[0])\n",
    "    list_write.append(train_set.shape[0])\n",
    "    list_write.append(columns_compas[:len(columns_compas)-minus])\n",
    "    list_write.append(len(columns_compas[:len(columns_compas)-minus]))\n",
    "    excute_time = 0\n",
    "    excute_time1 = 0\n",
    "    print(\"start massaging\")\n",
    "    for a in all_names_lst:\n",
    "      temp2, names = get_temp(new_train_data, all_names[a], compas_y)\n",
    "      temp, temp_g = get_temp_g(new_train_data, names, compas_y)\n",
    "      temp_g = temp_g[temp_g['cnt'] > filter_count]\n",
    "      lst_of_counts = compute_lst_of_counts(temp, names, compas_y)\n",
    "      start = time.time()    \n",
    "      need_pos, need_neg = compute_problematic_opt(temp2, temp_g, names, compas_y, lst_of_counts)\n",
    "      end = time.time()\n",
    "      excute_time += end - start\n",
    "\n",
    "      new_train_data['skewed'] = 0\n",
    "      new_train_data[\"diff\"] = 0\n",
    "      start1 = time.time()\n",
    "      new_train_data = naive_massaging(new_train_data, temp2, names, need_pos, need_neg, compas_y)\n",
    "      end1 = time.time()\n",
    "      excute_time1 += end1 - start1   \n",
    "    list_write.append(excute_time)\n",
    "    list_write.append(excute_time1)\n",
    "    writer.writerow(list_write)\n",
    "    print(\"end massage sampling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yiD96IqJ_BmS"
   },
   "source": [
    "# NAIVE ALGORITHM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Zy21oUFr4kI"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "new_train_data = copy.deepcopy(train_set)\n",
    "import csv\n",
    "\n",
    "# get the list of numbers of the given group\n",
    "def get_one_degree_neighbors(temp2, names, group_lst):\n",
    "    result = []\n",
    "    for i in range(len(group_lst)):\n",
    "        d = copy.copy(temp2)\n",
    "        for k in range(len(group_lst)):\n",
    "            if k != i:\n",
    "                d = d[d[names[k]] == group_lst[k]]\n",
    "            else:\n",
    "                d = d[d[names[k]] != group_lst[k]]\n",
    "        result.append(d)\n",
    "    return result\n",
    "\n",
    "def determine_problematic(group_lst, temp2, result, label, threshold= 0.3):\n",
    "    # return a value for a given group about whether it is a problematic group\n",
    "    #0: ok group, 1: need negative records, 2: need positive records\n",
    "    d = copy.copy(temp2)\n",
    "    for i in range(len(group_lst)):\n",
    "        d = d[d[names[i]] == group_lst[i]]\n",
    "    total =  d['cnt'].sum()\n",
    "    d = d[d[label] == 1]\n",
    "    pos = d['cnt'].sum()\n",
    "    neg = total - pos\n",
    " \n",
    "    neighbors = compute_neighbors(group_lst, result)\n",
    "    if(neighbors[2] == -1):\n",
    "        # there is no neighbors\n",
    "        return 0\n",
    "    if(total > 10):\n",
    "        # need to be large enough\n",
    "        if (pos/(neg+1) - neighbors[2] > threshold):\n",
    "            # too many positive records\n",
    "            return 1\n",
    "        if (neighbors[2] - pos/(neg+1) > threshold):\n",
    "            # too many negative records\n",
    "            return 2\n",
    "    return 0\n",
    "\n",
    "with open('adult_records.csv', 'a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    minus = 0\n",
    "    # get groups for the right number of attributes\n",
    "    all_names = candidate_groups(columns_compas[:len(columns_compas)- minus])\n",
    "    all_names_lst = list(all_names.keys())[len(columns_compas)-minus+1:]\n",
    "    all_names_lst.reverse()\n",
    "    \n",
    "    # add all of the settings to a list to write to results csv \n",
    "    list_write = []\n",
    "    list_write.append(\"Adult\")\n",
    "    list_write.append(\"Pref_naive\")\n",
    "    list_write.append(data.shape[0])\n",
    "    list_write.append(train_set.shape[0])\n",
    "    list_write.append(columns_compas[:len(columns_compas)-minus])\n",
    "    list_write.append(len(columns_compas[:len(columns_compas)-minus]))\n",
    "    excute_time = 0\n",
    "    excute_time1 = 0\n",
    "    for a in all_names_lst:\n",
    "      need_pos = []\n",
    "      need_neg = []\n",
    "      temp2, names = get_temp(new_train_data, all_names[a], compas_y)\n",
    "      temp, temp_g = get_temp_g(new_train_data, names, compas_y)\n",
    "      # naive algorithm do not have a filter here.\n",
    "      # temp_g = temp_g[temp_g['cnt'] > filter_count]\n",
    "      # start finding the set of problematic regions\n",
    "      start = time.time()\n",
    "      for index, row in temp_g.iterrows():\n",
    "        group_lst = []\n",
    "        for n in names:\n",
    "          group_lst.append(row[n])\n",
    "        # get neighbors \n",
    "        result = get_one_degree_neighbors(temp2, names, group_lst)\n",
    "        # get if belong in need/pos group\n",
    "        pos_neg_val = determine_problematic(group_lst, temp2, result, compas_y, threshold= 0.3)\n",
    "        # if needs neg vals \n",
    "        if pos_neg_val == 1:\n",
    "          need_neg.append(group_lst)\n",
    "      # if needs pos vals\n",
    "        if pos_neg_val == 2:\n",
    "          need_pos.append(group_lst)\n",
    "      end = time.time()\n",
    "      excute_time += end - start\n",
    "        \n",
    "    # if the naive algorithm is taking too long, break at 200 secs (timeout)\n",
    "      if excute_time > 200:\n",
    "        break\n",
    "      new_train_data['skewed'] = 0\n",
    "      new_train_data[\"diff\"] = 0  \n",
    "      # call pref sampling \n",
    "      start1 = time.time()    \n",
    "      new_train_data = pref_sampling_opt(new_train_data, names, compas_y, need_pos, need_neg)\n",
    "      end1 = time.time()\n",
    "      excute_time1 += end1 - start1   \n",
    "\n",
    "    list_write.append(excute_time)\n",
    "    list_write.append(excute_time1)\n",
    "    writer.writerow(list_write)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
