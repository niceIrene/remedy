{"cells":[{"cell_type":"markdown","metadata":{"id":"7oMgFj8Ve2Oq"},"source":["# DecisionTreeClassifer\n"]},{"cell_type":"markdown","metadata":{"id":"772S9aKIfv-9"},"source":["#Imports and Dataset processing"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":3676,"status":"ok","timestamp":1702399201381,"user":{"displayName":"Yin Lin","userId":"15130785411332062006"},"user_tz":300},"id":"Evnn9BbOfEme"},"outputs":[],"source":["import pandas as pd\n","import time\n","import numpy as np\n","from sklearn.metrics import accuracy_score\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.svm import SVC\n","from sklearn import metrics\n","import copy\n","from sympy import Symbol\n","from sympy.solvers import solve\n","pd.options.mode.chained_assignment = None\n","import csv"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":472,"status":"ok","timestamp":1702402278229,"user":{"displayName":"Yin Lin","userId":"15130785411332062006"},"user_tz":300},"id":"_vP0FN3vfgjA","colab":{"base_uri":"https://localhost:8080/","height":443},"outputId":"fe70ea27-0a8f-42b5-e4ba-57320c85a7cb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["       decile3  decile1  sex  lsat  ugpa grad bar1  fulltime  fam_inc   age  \\\n","0         10.0     10.0  1.0  44.0   3.5    Y    P       1.0      5.0 -62.0   \n","1          4.0      5.0  1.0  29.0   3.5    Y    P       1.0      4.0 -62.0   \n","2          2.0      3.0  2.0  36.0   3.5    Y    P       1.0      1.0 -58.0   \n","3          4.0      7.0  2.0  39.0   3.5    Y    P       1.0      4.0 -51.0   \n","4          8.0      9.0  2.0  48.0   3.5    Y    P       1.0      4.0 -61.0   \n","...        ...      ...  ...   ...   ...  ...  ...       ...      ...   ...   \n","22402      1.0      3.0  2.0  26.5   1.8    Y    F       1.0      2.0 -62.0   \n","22403      1.0      3.0  2.0  19.7   1.8    Y    F       1.0      3.0 -57.0   \n","22404      8.0      7.0  2.0  36.0   1.8    Y    P       2.0      3.0 -59.0   \n","22405     10.0     10.0  2.0  44.0   1.5    Y    P       2.0      3.0 -51.0   \n","22406      8.0      9.0  2.0  29.5   1.6    Y    P       1.0      3.0 -57.0   \n","\n","       gender  race1  tier  \n","0      female  white   4.0  \n","1      female  white   2.0  \n","2        male  white   3.0  \n","3        male  white   3.0  \n","4        male  white   5.0  \n","...       ...    ...   ...  \n","22402    male  black   1.0  \n","22403    male  black   1.0  \n","22404    male  black   3.0  \n","22405    male  white   3.0  \n","22406    male  white   1.0  \n","\n","[22407 rows x 13 columns]"],"text/html":["\n","  <div id=\"df-897b6f25-cb1a-4527-93d4-71e07545c645\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>decile3</th>\n","      <th>decile1</th>\n","      <th>sex</th>\n","      <th>lsat</th>\n","      <th>ugpa</th>\n","      <th>grad</th>\n","      <th>bar1</th>\n","      <th>fulltime</th>\n","      <th>fam_inc</th>\n","      <th>age</th>\n","      <th>gender</th>\n","      <th>race1</th>\n","      <th>tier</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>10.0</td>\n","      <td>10.0</td>\n","      <td>1.0</td>\n","      <td>44.0</td>\n","      <td>3.5</td>\n","      <td>Y</td>\n","      <td>P</td>\n","      <td>1.0</td>\n","      <td>5.0</td>\n","      <td>-62.0</td>\n","      <td>female</td>\n","      <td>white</td>\n","      <td>4.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4.0</td>\n","      <td>5.0</td>\n","      <td>1.0</td>\n","      <td>29.0</td>\n","      <td>3.5</td>\n","      <td>Y</td>\n","      <td>P</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>-62.0</td>\n","      <td>female</td>\n","      <td>white</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>36.0</td>\n","      <td>3.5</td>\n","      <td>Y</td>\n","      <td>P</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>-58.0</td>\n","      <td>male</td>\n","      <td>white</td>\n","      <td>3.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4.0</td>\n","      <td>7.0</td>\n","      <td>2.0</td>\n","      <td>39.0</td>\n","      <td>3.5</td>\n","      <td>Y</td>\n","      <td>P</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>-51.0</td>\n","      <td>male</td>\n","      <td>white</td>\n","      <td>3.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>8.0</td>\n","      <td>9.0</td>\n","      <td>2.0</td>\n","      <td>48.0</td>\n","      <td>3.5</td>\n","      <td>Y</td>\n","      <td>P</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>-61.0</td>\n","      <td>male</td>\n","      <td>white</td>\n","      <td>5.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>22402</th>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>26.5</td>\n","      <td>1.8</td>\n","      <td>Y</td>\n","      <td>F</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>-62.0</td>\n","      <td>male</td>\n","      <td>black</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>22403</th>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>19.7</td>\n","      <td>1.8</td>\n","      <td>Y</td>\n","      <td>F</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>-57.0</td>\n","      <td>male</td>\n","      <td>black</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>22404</th>\n","      <td>8.0</td>\n","      <td>7.0</td>\n","      <td>2.0</td>\n","      <td>36.0</td>\n","      <td>1.8</td>\n","      <td>Y</td>\n","      <td>P</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>-59.0</td>\n","      <td>male</td>\n","      <td>black</td>\n","      <td>3.0</td>\n","    </tr>\n","    <tr>\n","      <th>22405</th>\n","      <td>10.0</td>\n","      <td>10.0</td>\n","      <td>2.0</td>\n","      <td>44.0</td>\n","      <td>1.5</td>\n","      <td>Y</td>\n","      <td>P</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>-51.0</td>\n","      <td>male</td>\n","      <td>white</td>\n","      <td>3.0</td>\n","    </tr>\n","    <tr>\n","      <th>22406</th>\n","      <td>8.0</td>\n","      <td>9.0</td>\n","      <td>2.0</td>\n","      <td>29.5</td>\n","      <td>1.6</td>\n","      <td>Y</td>\n","      <td>P</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>-57.0</td>\n","      <td>male</td>\n","      <td>white</td>\n","      <td>1.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>22407 rows Ã— 13 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-897b6f25-cb1a-4527-93d4-71e07545c645')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-897b6f25-cb1a-4527-93d4-71e07545c645 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-897b6f25-cb1a-4527-93d4-71e07545c645');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-0acaab26-7054-4320-870e-157b161b0b11\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0acaab26-7054-4320-870e-157b161b0b11')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-0acaab26-7054-4320-870e-157b161b0b11 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":26}],"source":["url = \"https://raw.githubusercontent.com/niceIrene/remedy/main/datasets/bar_pass_prediction.csv\"\n","data = pd.read_csv(url)\n","# data = data.drop(columns = ['poutcome'])\n","# data = data.drop(columns = ['ID', ''])\n","# data.dtypes\n","data = data.drop(columns = ['ID', 'decile1b', 'pass_bar', 'other', 'gpa', 'zgpa', 'zfygpa',  'cluster',  'bar1_yr', 'bar2', 'bar2_yr', 'bar_passed','race', 'race2', 'asian', 'black', 'hisp', 'Dropout', 'male', 'bar', 'parttime', 'index6040', 'indxgrp', 'indxgrp2', 'dnn_bar_pass_prediction', 'DOB_yr'])\n","\n","# data['bar_passed'].value_counts()\n","data"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":155,"status":"ok","timestamp":1702402279555,"user":{"displayName":"Yin Lin","userId":"15130785411332062006"},"user_tz":300},"id":"vl-yZiIAq3OF"},"outputs":[],"source":["data = data.dropna()"]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":141,"status":"ok","timestamp":1702402280886,"user":{"displayName":"Yin Lin","userId":"15130785411332062006"},"user_tz":300},"id":"-8PY5wRALI7V","colab":{"base_uri":"https://localhost:8080/"},"outputId":"37fcf624-983e-4a12-8450-37b1af8aaebe"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["P    18140\n","F     2287\n","Name: bar1, dtype: int64"]},"metadata":{},"execution_count":28}],"source":["data[\"bar1\"].value_counts()\n","# data[\"ugpa\"].describe()"]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":302,"status":"ok","timestamp":1702402282400,"user":{"displayName":"Yin Lin","userId":"15130785411332062006"},"user_tz":300},"id":"bzpRuZ7AoV9o","colab":{"base_uri":"https://localhost:8080/","height":443},"outputId":"d423148a-53d5-4ebe-ca0c-ac024c65c352"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["       decile3  decile1  sex  lsat  ugpa grad bar1  fulltime  fam_inc   age  \\\n","9          5.0      7.0  1.0  28.0   3.5    Y    F       1.0      2.0 -62.0   \n","10         2.0      3.0  1.0  33.0   3.5    Y    F       1.0      4.0 -65.0   \n","11         5.0      6.0  1.0  36.0   3.5    Y    P       1.0      3.0 -62.0   \n","17         3.0      5.0  1.0  33.0   3.5    Y    F       1.0      4.0 -62.0   \n","28         7.0      7.0  1.0  30.0   3.5    Y    F       1.0      3.0 -61.0   \n","...        ...      ...  ...   ...   ...  ...  ...       ...      ...   ...   \n","22393      1.0      2.0  1.0  39.5   1.7    Y    F       1.0      4.0 -54.0   \n","22400      6.0      9.0  2.0  31.0   1.8    Y    F       1.0      2.0 -53.0   \n","22401      1.0      1.0  2.0  25.0   1.8    Y    F       1.0      3.0 -46.0   \n","22402      1.0      3.0  2.0  26.5   1.8    Y    F       1.0      2.0 -62.0   \n","22403      1.0      3.0  2.0  19.7   1.8    Y    F       1.0      3.0 -57.0   \n","\n","       gender  race1  tier  \n","9      female   hisp   1.0  \n","10     female  white   2.0  \n","11     female  white   4.0  \n","17     female  white   5.0  \n","28     female  white   2.0  \n","...       ...    ...   ...  \n","22393  female  white   3.0  \n","22400    male  black   1.0  \n","22401    male  other   2.0  \n","22402    male  black   1.0  \n","22403    male  black   1.0  \n","\n","[4574 rows x 13 columns]"],"text/html":["\n","  <div id=\"df-2216be5e-e681-450d-919d-844b18b031a0\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>decile3</th>\n","      <th>decile1</th>\n","      <th>sex</th>\n","      <th>lsat</th>\n","      <th>ugpa</th>\n","      <th>grad</th>\n","      <th>bar1</th>\n","      <th>fulltime</th>\n","      <th>fam_inc</th>\n","      <th>age</th>\n","      <th>gender</th>\n","      <th>race1</th>\n","      <th>tier</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>9</th>\n","      <td>5.0</td>\n","      <td>7.0</td>\n","      <td>1.0</td>\n","      <td>28.0</td>\n","      <td>3.5</td>\n","      <td>Y</td>\n","      <td>F</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>-62.0</td>\n","      <td>female</td>\n","      <td>hisp</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>1.0</td>\n","      <td>33.0</td>\n","      <td>3.5</td>\n","      <td>Y</td>\n","      <td>F</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>-65.0</td>\n","      <td>female</td>\n","      <td>white</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>5.0</td>\n","      <td>6.0</td>\n","      <td>1.0</td>\n","      <td>36.0</td>\n","      <td>3.5</td>\n","      <td>Y</td>\n","      <td>P</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>-62.0</td>\n","      <td>female</td>\n","      <td>white</td>\n","      <td>4.0</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>3.0</td>\n","      <td>5.0</td>\n","      <td>1.0</td>\n","      <td>33.0</td>\n","      <td>3.5</td>\n","      <td>Y</td>\n","      <td>F</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>-62.0</td>\n","      <td>female</td>\n","      <td>white</td>\n","      <td>5.0</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>7.0</td>\n","      <td>7.0</td>\n","      <td>1.0</td>\n","      <td>30.0</td>\n","      <td>3.5</td>\n","      <td>Y</td>\n","      <td>F</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>-61.0</td>\n","      <td>female</td>\n","      <td>white</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>22393</th>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>39.5</td>\n","      <td>1.7</td>\n","      <td>Y</td>\n","      <td>F</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>-54.0</td>\n","      <td>female</td>\n","      <td>white</td>\n","      <td>3.0</td>\n","    </tr>\n","    <tr>\n","      <th>22400</th>\n","      <td>6.0</td>\n","      <td>9.0</td>\n","      <td>2.0</td>\n","      <td>31.0</td>\n","      <td>1.8</td>\n","      <td>Y</td>\n","      <td>F</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>-53.0</td>\n","      <td>male</td>\n","      <td>black</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>22401</th>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>25.0</td>\n","      <td>1.8</td>\n","      <td>Y</td>\n","      <td>F</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>-46.0</td>\n","      <td>male</td>\n","      <td>other</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>22402</th>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>26.5</td>\n","      <td>1.8</td>\n","      <td>Y</td>\n","      <td>F</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>-62.0</td>\n","      <td>male</td>\n","      <td>black</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>22403</th>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>19.7</td>\n","      <td>1.8</td>\n","      <td>Y</td>\n","      <td>F</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>-57.0</td>\n","      <td>male</td>\n","      <td>black</td>\n","      <td>1.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>4574 rows Ã— 13 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2216be5e-e681-450d-919d-844b18b031a0')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-2216be5e-e681-450d-919d-844b18b031a0 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-2216be5e-e681-450d-919d-844b18b031a0');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-21a18906-3fb1-456f-9d02-edde4498381e\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-21a18906-3fb1-456f-9d02-edde4498381e')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-21a18906-3fb1-456f-9d02-edde4498381e button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":29}],"source":["# # make it a balanced dataset\n","\n","\n","data[\"bar1\"].value_counts()\n","\n","rows_to_remove = data[data['bar1'] == 'P'].sample(n=15853).index\n","data = data.drop(rows_to_remove)\n","\n","\n","data"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":281,"status":"ok","timestamp":1702402265164,"user":{"displayName":"Yin Lin","userId":"15130785411332062006"},"user_tz":300},"id":"Nr_VLyiBLRsl","colab":{"base_uri":"https://localhost:8080/","height":548},"outputId":"49313c57-5778-4710-dbff-33c56e7f81f0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["       decile3  decile1  sex   lsat     ugpa grad bar1  fulltime  fam_inc  \\\n","9          5.0      7.0  1.0    <33  3.2-3.5    Y    F       1.0      2.0   \n","10         2.0      3.0  1.0    <33  3.2-3.5    Y    F       1.0      4.0   \n","11         5.0      6.0  1.0  33-37  3.2-3.5    Y    P       1.0      3.0   \n","17         3.0      5.0  1.0    <33  3.2-3.5    Y    F       1.0      4.0   \n","24         9.0     10.0  2.0  33-37  3.2-3.5    Y    P       1.0      4.0   \n","...        ...      ...  ...    ...      ...  ...  ...       ...      ...   \n","22393      1.0      2.0  1.0  37-41       <3    Y    F       1.0      4.0   \n","22400      6.0      9.0  2.0    <33       <3    Y    F       1.0      2.0   \n","22401      1.0      1.0  2.0    <33       <3    Y    F       1.0      3.0   \n","22402      1.0      3.0  2.0    <33       <3    Y    F       1.0      2.0   \n","22403      1.0      3.0  2.0    <33       <3    Y    F       1.0      3.0   \n","\n","        age  gender  race1  tier  \n","9     -62.0  female   hisp   1.0  \n","10    -65.0  female  white   2.0  \n","11    -62.0  female  white   4.0  \n","17    -62.0  female  white   5.0  \n","24    -61.0    male  white   3.0  \n","...     ...     ...    ...   ...  \n","22393 -54.0  female  white   3.0  \n","22400 -53.0    male  black   1.0  \n","22401 -46.0    male  other   2.0  \n","22402 -62.0    male  black   1.0  \n","22403 -57.0    male  black   1.0  \n","\n","[4574 rows x 13 columns]"],"text/html":["\n","  <div id=\"df-77c164c1-cc7b-48f5-a5c5-bbf92926cec8\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>decile3</th>\n","      <th>decile1</th>\n","      <th>sex</th>\n","      <th>lsat</th>\n","      <th>ugpa</th>\n","      <th>grad</th>\n","      <th>bar1</th>\n","      <th>fulltime</th>\n","      <th>fam_inc</th>\n","      <th>age</th>\n","      <th>gender</th>\n","      <th>race1</th>\n","      <th>tier</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>9</th>\n","      <td>5.0</td>\n","      <td>7.0</td>\n","      <td>1.0</td>\n","      <td>&lt;33</td>\n","      <td>3.2-3.5</td>\n","      <td>Y</td>\n","      <td>F</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>-62.0</td>\n","      <td>female</td>\n","      <td>hisp</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>1.0</td>\n","      <td>&lt;33</td>\n","      <td>3.2-3.5</td>\n","      <td>Y</td>\n","      <td>F</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>-65.0</td>\n","      <td>female</td>\n","      <td>white</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>5.0</td>\n","      <td>6.0</td>\n","      <td>1.0</td>\n","      <td>33-37</td>\n","      <td>3.2-3.5</td>\n","      <td>Y</td>\n","      <td>P</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>-62.0</td>\n","      <td>female</td>\n","      <td>white</td>\n","      <td>4.0</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>3.0</td>\n","      <td>5.0</td>\n","      <td>1.0</td>\n","      <td>&lt;33</td>\n","      <td>3.2-3.5</td>\n","      <td>Y</td>\n","      <td>F</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>-62.0</td>\n","      <td>female</td>\n","      <td>white</td>\n","      <td>5.0</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>9.0</td>\n","      <td>10.0</td>\n","      <td>2.0</td>\n","      <td>33-37</td>\n","      <td>3.2-3.5</td>\n","      <td>Y</td>\n","      <td>P</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>-61.0</td>\n","      <td>male</td>\n","      <td>white</td>\n","      <td>3.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>22393</th>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>37-41</td>\n","      <td>&lt;3</td>\n","      <td>Y</td>\n","      <td>F</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>-54.0</td>\n","      <td>female</td>\n","      <td>white</td>\n","      <td>3.0</td>\n","    </tr>\n","    <tr>\n","      <th>22400</th>\n","      <td>6.0</td>\n","      <td>9.0</td>\n","      <td>2.0</td>\n","      <td>&lt;33</td>\n","      <td>&lt;3</td>\n","      <td>Y</td>\n","      <td>F</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>-53.0</td>\n","      <td>male</td>\n","      <td>black</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>22401</th>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>&lt;33</td>\n","      <td>&lt;3</td>\n","      <td>Y</td>\n","      <td>F</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>-46.0</td>\n","      <td>male</td>\n","      <td>other</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>22402</th>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>&lt;33</td>\n","      <td>&lt;3</td>\n","      <td>Y</td>\n","      <td>F</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>-62.0</td>\n","      <td>male</td>\n","      <td>black</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>22403</th>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>&lt;33</td>\n","      <td>&lt;3</td>\n","      <td>Y</td>\n","      <td>F</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>-57.0</td>\n","      <td>male</td>\n","      <td>black</td>\n","      <td>1.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>4574 rows Ã— 13 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-77c164c1-cc7b-48f5-a5c5-bbf92926cec8')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-77c164c1-cc7b-48f5-a5c5-bbf92926cec8 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-77c164c1-cc7b-48f5-a5c5-bbf92926cec8');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-2ae54970-e079-4c41-9745-e7e4db5e8d3b\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2ae54970-e079-4c41-9745-e7e4db5e8d3b')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-2ae54970-e079-4c41-9745-e7e4db5e8d3b button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":24}],"source":["# data[\"lsat\"] = pd.cut(data[\"lsat\"], [0,33,37,41,50], labels=[\"<33\", \"33-37\",\"37-41\",\">41\"])\n","# data[\"ugpa\"] = pd.cut(data[\"ugpa\"], [0,3,3.2,3.5,4], labels=[\"<3\", \"3-3.2\",\"3.2-3.5\",\">3.5\"])\n","# data\n","\n"]},{"cell_type":"code","source":["data = data.drop(columns=[\"age\"], axis = 1)\n","data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":443},"id":"xSoD1uxS6TQh","executionInfo":{"status":"ok","timestamp":1702402286881,"user_tz":300,"elapsed":263,"user":{"displayName":"Yin Lin","userId":"15130785411332062006"}},"outputId":"b96f3b5e-be6e-4489-ed09-e7a96e4b5c4e"},"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       decile3  decile1  sex  lsat  ugpa grad bar1  fulltime  fam_inc  gender  \\\n","9          5.0      7.0  1.0  28.0   3.5    Y    F       1.0      2.0  female   \n","10         2.0      3.0  1.0  33.0   3.5    Y    F       1.0      4.0  female   \n","11         5.0      6.0  1.0  36.0   3.5    Y    P       1.0      3.0  female   \n","17         3.0      5.0  1.0  33.0   3.5    Y    F       1.0      4.0  female   \n","28         7.0      7.0  1.0  30.0   3.5    Y    F       1.0      3.0  female   \n","...        ...      ...  ...   ...   ...  ...  ...       ...      ...     ...   \n","22393      1.0      2.0  1.0  39.5   1.7    Y    F       1.0      4.0  female   \n","22400      6.0      9.0  2.0  31.0   1.8    Y    F       1.0      2.0    male   \n","22401      1.0      1.0  2.0  25.0   1.8    Y    F       1.0      3.0    male   \n","22402      1.0      3.0  2.0  26.5   1.8    Y    F       1.0      2.0    male   \n","22403      1.0      3.0  2.0  19.7   1.8    Y    F       1.0      3.0    male   \n","\n","       race1  tier  \n","9       hisp   1.0  \n","10     white   2.0  \n","11     white   4.0  \n","17     white   5.0  \n","28     white   2.0  \n","...      ...   ...  \n","22393  white   3.0  \n","22400  black   1.0  \n","22401  other   2.0  \n","22402  black   1.0  \n","22403  black   1.0  \n","\n","[4574 rows x 12 columns]"],"text/html":["\n","  <div id=\"df-b40c2a6c-c248-464e-b8d8-febe7f501bdd\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>decile3</th>\n","      <th>decile1</th>\n","      <th>sex</th>\n","      <th>lsat</th>\n","      <th>ugpa</th>\n","      <th>grad</th>\n","      <th>bar1</th>\n","      <th>fulltime</th>\n","      <th>fam_inc</th>\n","      <th>gender</th>\n","      <th>race1</th>\n","      <th>tier</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>9</th>\n","      <td>5.0</td>\n","      <td>7.0</td>\n","      <td>1.0</td>\n","      <td>28.0</td>\n","      <td>3.5</td>\n","      <td>Y</td>\n","      <td>F</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>female</td>\n","      <td>hisp</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>1.0</td>\n","      <td>33.0</td>\n","      <td>3.5</td>\n","      <td>Y</td>\n","      <td>F</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>female</td>\n","      <td>white</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>5.0</td>\n","      <td>6.0</td>\n","      <td>1.0</td>\n","      <td>36.0</td>\n","      <td>3.5</td>\n","      <td>Y</td>\n","      <td>P</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>female</td>\n","      <td>white</td>\n","      <td>4.0</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>3.0</td>\n","      <td>5.0</td>\n","      <td>1.0</td>\n","      <td>33.0</td>\n","      <td>3.5</td>\n","      <td>Y</td>\n","      <td>F</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>female</td>\n","      <td>white</td>\n","      <td>5.0</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>7.0</td>\n","      <td>7.0</td>\n","      <td>1.0</td>\n","      <td>30.0</td>\n","      <td>3.5</td>\n","      <td>Y</td>\n","      <td>F</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>female</td>\n","      <td>white</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>22393</th>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>39.5</td>\n","      <td>1.7</td>\n","      <td>Y</td>\n","      <td>F</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>female</td>\n","      <td>white</td>\n","      <td>3.0</td>\n","    </tr>\n","    <tr>\n","      <th>22400</th>\n","      <td>6.0</td>\n","      <td>9.0</td>\n","      <td>2.0</td>\n","      <td>31.0</td>\n","      <td>1.8</td>\n","      <td>Y</td>\n","      <td>F</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>male</td>\n","      <td>black</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>22401</th>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>25.0</td>\n","      <td>1.8</td>\n","      <td>Y</td>\n","      <td>F</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>male</td>\n","      <td>other</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>22402</th>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>26.5</td>\n","      <td>1.8</td>\n","      <td>Y</td>\n","      <td>F</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>male</td>\n","      <td>black</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>22403</th>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>19.7</td>\n","      <td>1.8</td>\n","      <td>Y</td>\n","      <td>F</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>male</td>\n","      <td>black</td>\n","      <td>1.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>4574 rows Ã— 12 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b40c2a6c-c248-464e-b8d8-febe7f501bdd')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-b40c2a6c-c248-464e-b8d8-febe7f501bdd button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-b40c2a6c-c248-464e-b8d8-febe7f501bdd');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-418454bb-cc4b-43a3-872d-f309d6834c60\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-418454bb-cc4b-43a3-872d-f309d6834c60')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-418454bb-cc4b-43a3-872d-f309d6834c60 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["data.to_csv('bar_pass.csv', index = False)"],"metadata":{"id":"sbSRvGWX6UjG","executionInfo":{"status":"ok","timestamp":1702402291213,"user_tz":300,"elapsed":143,"user":{"displayName":"Yin Lin","userId":"15130785411332062006"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["data.dtypes"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FBpS6r5L7ELv","executionInfo":{"status":"ok","timestamp":1702402328249,"user_tz":300,"elapsed":124,"user":{"displayName":"Yin Lin","userId":"15130785411332062006"}},"outputId":"54a185f7-58f3-41bf-df65-8bd2063c1974"},"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["decile3     float64\n","decile1     float64\n","sex         float64\n","lsat        float64\n","ugpa        float64\n","grad         object\n","bar1         object\n","fulltime    float64\n","fam_inc     float64\n","gender       object\n","race1        object\n","tier        float64\n","dtype: object"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":283,"status":"ok","timestamp":1702399345589,"user":{"displayName":"Yin Lin","userId":"15130785411332062006"},"user_tz":300},"id":"E-2JQfWYN3y5"},"outputs":[],"source":["# data['age'] = data['age'].astype(str)\n","# for c in data.columns:\n","#     print(c)\n","#     if type(data[c].unique()[0]) == str:\n","#       print(data[c].unique())\n","#       temp = list(data[c].unique())\n","#       print(temp)\n","#       for t in range(len(temp)):\n","#           print(str(t), temp[t])\n","#           data[c] = data[c].str.replace(temp[t],str(t))\n","#     # if c == 'age':\n","\n","#     #   temp = list(data[c].unique())\n","#     #   for a in range(len(temp)):\n","#     #     data[c].str.replace(temp[a], str(a))\n","\n","# data"]},{"cell_type":"code","source":["# data['age'] = data['age'].astype(int)\n","\n","# age_bins = [0, 10, 20, 30, 40, 50]  # Define your own bin edges\n","# age_labels = [0, 1, 2, 3, 4]\n","# data['age'] = pd.cut(data['age'], bins=age_bins, labels=age_labels, include_lowest=True)\n","# data['age'].value_counts()"],"metadata":{"id":"TrXAh5nfPY5A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# data[\"gender\"] = pd.to_numeric(data[\"gender\"])\n","# data[\"age\"] = pd.to_numeric(data[\"age\"])\n","# data[\"race1\"] = pd.to_numeric(data[\"race1\"])"],"metadata":{"id":"uI-wKg8AKp3b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# data.dtypes"],"metadata":{"id":"KmozpMQ_QasK"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Z4szRg_aYbl"},"outputs":[],"source":["# data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hOPuwyXv2yAu"},"outputs":[],"source":["# data.columns"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":156,"status":"ok","timestamp":1702399261687,"user":{"displayName":"Yin Lin","userId":"15130785411332062006"},"user_tz":300},"id":"eY057S7X5v_H","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e83e7b0d-dcbd-48a0-fcf8-96c4da472c74"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["F    2287\n","P    2287\n","Name: bar1, dtype: int64"]},"metadata":{},"execution_count":7}],"source":["data[\"bar1\"].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QodepRN4aZfL"},"outputs":[],"source":["# data[\"bar1\"] = data[\"bar1\"].astype(int)\n","# data.dtypes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o-gzTGizgPXb"},"outputs":[],"source":["# data.to_csv('bar_pass.csv', index = False)"]},{"cell_type":"markdown","source":["# DATA NEW"],"metadata":{"id":"HMQGaTIQivoI"}},{"cell_type":"code","source":["# Yin added\n","url = \"https://raw.githubusercontent.com/niceIrene/remedy/main/datasets/bar_pass_new.csv\"\n","data = pd.read_csv(url)\n","data = data.drop(columns=[\"gender\"])\n","data"],"metadata":{"id":"1uwjvDygbzQe","executionInfo":{"status":"ok","timestamp":1700329919037,"user_tz":480,"elapsed":13,"user":{"displayName":"Samika Gupta","userId":"09451922472516593519"}},"colab":{"base_uri":"https://localhost:8080/","height":424},"outputId":"038a9c50-4d57-4c7c-e5cd-2838cf2e2ed2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["      decile1b  decile3  decile1  sex  lsat  ugpa  grad  bar1  fulltime  \\\n","0          3.0      2.0      3.0  2.0     0     0     0     0       1.0   \n","1          7.0      5.0      7.0  1.0     1     0     0     1       1.0   \n","2          2.0      2.0      3.0  1.0     1     0     0     1       1.0   \n","3          5.0      3.0      5.0  1.0     1     0     0     1       1.0   \n","4          5.0      5.0      5.0  1.0     2     0     0     0       1.0   \n","...        ...      ...      ...  ...   ...   ...   ...   ...       ...   \n","4500       9.0      6.0      9.0  2.0     1     1     0     1       1.0   \n","4501       1.0      1.0      1.0  2.0     1     1     0     1       1.0   \n","4502       3.0      1.0      3.0  2.0     1     1     0     1       1.0   \n","4503       2.0      1.0      3.0  2.0     1     1     0     1       1.0   \n","4504      10.0     10.0     10.0  2.0     2     1     0     0       2.0   \n","\n","      fam_inc  age  race1  tier  \n","0         1.0    0      0   3.0  \n","1         2.0    0      1   1.0  \n","2         4.0    0      0   2.0  \n","3         4.0    0      0   5.0  \n","4         3.0    0      0   4.0  \n","...       ...  ...    ...   ...  \n","4500      2.0    2      2   1.0  \n","4501      3.0    2      4   2.0  \n","4502      2.0    0      2   1.0  \n","4503      3.0    1      2   1.0  \n","4504      3.0    1      0   3.0  \n","\n","[4505 rows x 13 columns]"],"text/html":["\n","  <div id=\"df-933f0219-3d95-4c2e-b88f-e4bb921dd81e\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>decile1b</th>\n","      <th>decile3</th>\n","      <th>decile1</th>\n","      <th>sex</th>\n","      <th>lsat</th>\n","      <th>ugpa</th>\n","      <th>grad</th>\n","      <th>bar1</th>\n","      <th>fulltime</th>\n","      <th>fam_inc</th>\n","      <th>age</th>\n","      <th>race1</th>\n","      <th>tier</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>7.0</td>\n","      <td>5.0</td>\n","      <td>7.0</td>\n","      <td>1.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>1.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>5.0</td>\n","      <td>3.0</td>\n","      <td>5.0</td>\n","      <td>1.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>5.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5.0</td>\n","      <td>5.0</td>\n","      <td>5.0</td>\n","      <td>1.0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>4500</th>\n","      <td>9.0</td>\n","      <td>6.0</td>\n","      <td>9.0</td>\n","      <td>2.0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>4501</th>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>4502</th>\n","      <td>3.0</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>4503</th>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>4504</th>\n","      <td>10.0</td>\n","      <td>10.0</td>\n","      <td>10.0</td>\n","      <td>2.0</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>3.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>4505 rows Ã— 13 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-933f0219-3d95-4c2e-b88f-e4bb921dd81e')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-933f0219-3d95-4c2e-b88f-e4bb921dd81e button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-933f0219-3d95-4c2e-b88f-e4bb921dd81e');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-c649de3f-08b3-4c6d-9ae1-872668dccf10\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c649de3f-08b3-4c6d-9ae1-872668dccf10')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-c649de3f-08b3-4c6d-9ae1-872668dccf10 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":52}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NNSifBfwflg8"},"outputs":[],"source":["# get training and testing set\n","\n","# columns_compas = ['stay', 'age', 'charge', 'sex', '#prior', 'race']\n","\n","\n","compas_y = 'bar1'\n","columns_all =['decile1b', 'decile3', 'decile1', 'sex', 'lsat', 'ugpa', 'grad',\n","       'fulltime', 'fam_inc', 'race1', 'tier', 'age']\n","# columns_compas =['fam_inc', 'race1', 'lsat', 'ugpa', 'sex', 'fulltime']\n","columns_compas = [\"sex\", \"age\", \"race1\", \"fam_inc\"]\n","# columns_compas = ['decile1b', 'decile3', 'decile1', 'sex', 'lsat', 'ugpa', 'grad',\n","#        'fulltime', 'fam_inc', 'gender', 'race1', 'tier']\n","def split_train_test(data,test_ratio):\n","    np.random.seed(42)\n","    shuffled_indices = np.random.permutation(len(data))\n","    test_set_size = int(len(data) * test_ratio)\n","    test_indices = shuffled_indices[:test_set_size]\n","    train_indices = shuffled_indices[test_set_size:]\n","    return data.iloc[train_indices],data.iloc[test_indices]\n","\n","def get_train_test(data, split, list_cols, y_label):\n","  train_set,test_set = split_train_test(data,split)\n","  print(len(train_set), \"train +\", len(test_set), \"test\")\n","  train_x = pd.DataFrame(train_set, columns = list_cols)\n","  train_label = train_set[y_label]\n","  test_x = pd.DataFrame(test_set, columns = list_cols)\n","  test_label = test_set[y_label]\n","  return train_x, test_x, train_label, test_label, train_set, test_set"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1700329919038,"user":{"displayName":"Samika Gupta","userId":"09451922472516593519"},"user_tz":480},"id":"FxVT3d69jWhR","outputId":"b1bfb992-6696-4dd6-c4bd-3744e07d1060"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['sex', 'age', 'race1', 'fam_inc']"]},"metadata":{},"execution_count":54}],"source":["columns_compas\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":174,"status":"ok","timestamp":1700329919205,"user":{"displayName":"Samika Gupta","userId":"09451922472516593519"},"user_tz":480},"id":"WJQsq95bfpEH","outputId":"20b7775a-835d-422a-be24-c8a0c7cf4b8b"},"outputs":[{"output_type":"stream","name":"stdout","text":["3154 train + 1351 test\n","[1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1]\n","      decile1b  decile3  decile1  sex  lsat  ugpa  grad  bar1  fulltime  \\\n","2258       5.0      5.0      6.0  2.0     3     1     0     0       1.0   \n","2955       1.0      1.0      1.0  1.0     1     1     0     1       2.0   \n","2775       1.0      1.0      1.0  2.0     1     3     0     1       1.0   \n","1056       4.0      6.0      5.0  1.0     1     0     0     1       1.0   \n","2208       3.0      3.0      4.0  2.0     1     1     0     1       2.0   \n","...        ...      ...      ...  ...   ...   ...   ...   ...       ...   \n","2810       9.0     10.0      9.0  2.0     2     3     0     0       1.0   \n","3252       7.0      8.0      8.0  2.0     1     2     0     1       1.0   \n","4212       1.0      3.0      1.0  1.0     3     3     0     0       1.0   \n","578        1.0      1.0      1.0  1.0     1     0     0     1       1.0   \n","4072       7.0      7.0      7.0  1.0     2     3     0     0       1.0   \n","\n","      fam_inc  age  race1  tier  predicted  \n","2258      3.0    1      0   3.0          0  \n","2955      3.0    3      0   3.0          1  \n","2775      1.0    0      2   4.0          1  \n","1056      4.0    0      0   2.0          1  \n","2208      3.0    2      0   2.0          1  \n","...       ...  ...    ...   ...        ...  \n","2810      4.0    1      0   5.0          0  \n","3252      4.0    0      0   2.0          0  \n","4212      4.0    0      0   4.0          0  \n","578       2.0    0      0   3.0          1  \n","4072      3.0    0      0   4.0          0  \n","\n","[1351 rows x 14 columns]\n","1    2287\n","0    2218\n","Name: bar1, dtype: int64\n","0    2339\n","1    2166\n","Name: predicted, dtype: int64\n","1    680\n","0    671\n","Name: bar1, dtype: int64\n","[0 1 1 ... 0 1 0]\n"]}],"source":["train_x, test_x, train_label, test_label, train_set, test_set  = get_train_test(data, 0.3, columns_all, compas_y)\n","\n","###################\n","\n","### about decision tree\n","from sklearn import tree\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import GridSearchCV\n","\n","param = {'criterion': ['gini', 'entropy'], 'max_depth': [10, 20, 30, 40, 50, 100], 'random_state':[17]}\n","\n","grid = DecisionTreeClassifier(criterion=\"entropy\",max_depth=6, random_state=17)\n","grid.fit(train_x, train_label)\n","\n","print(list(train_label))\n","# print(\"best\", grid.best_score_)\n","# test_predict = grid.predict(test_x)\n","data_all = pd.concat([train_x,test_x])\n","data_predict = grid.predict(data_all)\n","test_predict = grid.predict(test_x)\n","data['predicted'] = data_predict\n","\n","test_set['predicted'] = test_predict\n","print(test_set)\n","print(data[compas_y].value_counts())\n","print(data['predicted'].value_counts())\n","print(test_label.value_counts())\n","print(test_predict)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12858,"status":"ok","timestamp":1700329932060,"user":{"displayName":"Samika Gupta","userId":"09451922472516593519"},"user_tz":480},"id":"GVt4YIpUgKlo","outputId":"6879c2ed-58dc-4c12-ee65-ee3b3b5e702e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: divexplorer in /usr/local/lib/python3.10/dist-packages (0.2.1)\n","Requirement already satisfied: matplotlib>=3.1.1 in /usr/local/lib/python3.10/dist-packages (from divexplorer) (3.7.1)\n","Requirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.10/dist-packages (from divexplorer) (1.23.5)\n","Requirement already satisfied: mlxtend>=0.17.1 in /usr/local/lib/python3.10/dist-packages (from divexplorer) (0.22.0)\n","Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.10/dist-packages (from divexplorer) (1.5.3)\n","Requirement already satisfied: plotly>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from divexplorer) (5.15.0)\n","Requirement already satisfied: python-igraph>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from divexplorer) (0.11.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.1->divexplorer) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.1->divexplorer) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.1->divexplorer) (4.44.3)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.1->divexplorer) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.1->divexplorer) (23.2)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.1->divexplorer) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.1->divexplorer) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.1->divexplorer) (2.8.2)\n","Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.10/dist-packages (from mlxtend>=0.17.1->divexplorer) (1.11.3)\n","Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from mlxtend>=0.17.1->divexplorer) (1.2.2)\n","Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from mlxtend>=0.17.1->divexplorer) (1.3.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from mlxtend>=0.17.1->divexplorer) (67.7.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.2->divexplorer) (2023.3.post1)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=4.5.0->divexplorer) (8.2.3)\n","Requirement already satisfied: igraph==0.11.2 in /usr/local/lib/python3.10/dist-packages (from python-igraph>=0.8.3->divexplorer) (0.11.2)\n","Requirement already satisfied: texttable>=1.6.2 in /usr/local/lib/python3.10/dist-packages (from igraph==0.11.2->python-igraph>=0.8.3->divexplorer) (1.7.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.1.1->divexplorer) (1.16.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->mlxtend>=0.17.1->divexplorer) (3.2.0)\n"]}],"source":["pip install divexplorer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2QMjHrFI6YGQ"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f-rnfp9XfuKM"},"outputs":[],"source":["def fpr_onegroup(true, predict):\n","    fp = 0\n","    tn = 0\n","    for i in range(len(true)):\n","        if (true[i] == 0 and predict[i] == 1):\n","            fp += 1\n","        if(true[i] == 0 and predict[i] == 0):\n","            tn += 1\n","    print(fp, tn)\n","    return fp/(fp+tn)\n","\n","\n","def fnr_onegroup(true, predict):\n","    fn = 0\n","    tp = 0\n","    for i in range(len(true)):\n","        if (true[i] == 1 and predict[i] == 0):\n","            fn += 1\n","        if(true[i] == 1 and predict[i] == 1):\n","            tp += 1\n","    print(fn, tp)\n","    return fn/(fn+tp)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1700329932060,"user":{"displayName":"Samika Gupta","userId":"09451922472516593519"},"user_tz":480},"id":"axb85-QngAPX","outputId":"396b3879-0387-46c3-9f1f-5137540bef21"},"outputs":[{"output_type":"stream","name":"stdout","text":["148 523\n","fpr is  0.22056631892697467\n","193 487\n","fnr is  0.2838235294117647\n"]}],"source":["fpr = fpr_onegroup(list(test_label), test_predict)\n","print(\"fpr is \" , fpr)\n","\n","fnr = fnr_onegroup(list(test_label), test_predict)\n","print(\"fnr is \" , fnr)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1700329932060,"user":{"displayName":"Samika Gupta","userId":"09451922472516593519"},"user_tz":480},"id":"7qzANv0KigX3","outputId":"0e929159-1ec3-40a1-87cd-67775e8fe019"},"outputs":[{"output_type":"stream","name":"stdout","text":["accuracy is  0.7475943745373798\n"]}],"source":["# print(fpr_onegroup(l1, l3))\n","accuracy = accuracy_score(test_label, test_predict)\n","print(\"accuracy is \" , accuracy)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tb3zgDbUgXcV"},"outputs":[],"source":["def fairness_score_computation(d, metrics):\n","    sum_of_score = 0\n","    for idx, row in d.iterrows():\n","      sum_of_score += row['support'] * row[metrics]\n","    return sum_of_score\n","\n","\n","# print(fairness_score_computation(d, 'd_fpr'))\n","# print(fairness_score_computation(d2, 'd_fnr'))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3CcWMslQ-jZC"},"outputs":[],"source":["filter_count = 30"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1nr-LivCqD4t"},"outputs":[],"source":["from sklearn.model_selection import GridSearchCV\n","from sklearn.svm import SVC\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score, make_scorer\n","filter_count = 1\n","\n","# ORIGINAL\n","\n","scoring = make_scorer(accuracy_score)\n","param_griddt = {\n","    'max_depth': [2, 5, 10, 20],\n","    'min_samples_split': [2, 5, 10],\n","    'min_samples_leaf': [1, 2, 4]\n","}\n","dt = DecisionTreeClassifier(random_state=42)\n","griddt = GridSearchCV(dt, param_grid=param_griddt, scoring=scoring, cv=5)\n","svc = SVC(probability=True, kernel=\"poly\", degree = 2)\n","gridrf = GridSearchCV(RandomForestClassifier(), param_grid = param, cv = 6)\n","paraml = {'penalty': ['l2'],'C':[0.1, 1, 10, 100, 1000], 'solver':['saga'], 'multi_class':['ovr']}\n","gridl = GridSearchCV(LogisticRegression(), param_grid = paraml, cv = 6)"]},{"cell_type":"code","source":["# type(test_x[\"age\"].iloc[0])"],"metadata":{"id":"d9pIQt3juxgf"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2803,"status":"ok","timestamp":1700329935100,"user":{"displayName":"Samika Gupta","userId":"09451922472516593519"},"user_tz":480},"id":"OVys_xBluGK4","outputId":"cc4d1374-3be4-4a19-e4e4-ca1abaa2d129"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n"]},{"output_type":"execute_result","data":{"text/plain":["0.7809030347890451"]},"metadata":{},"execution_count":64}],"source":["# ORIGINAL HERE\n","gridl.fit(train_x, train_label)\n","test_predict = gridl.predict(test_x)\n","test_set['predicted'] = test_predict\n","accuracy_score(test_label, test_set['predicted'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":393},"executionInfo":{"elapsed":20,"status":"error","timestamp":1700329935334,"user":{"displayName":"Samika Gupta","userId":"09451922472516593519"},"user_tz":480},"id":"V8TWTKxGuUZj","outputId":"871f3368-53fd-45a5-807e-c143add3c6d9"},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-65-31d769df0e53>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclass_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'N'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'P'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdivexplorer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFP_DivergenceExplorer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFP_DivergenceExplorer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcolumns_compas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcompas_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"predicted\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumns_compas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'divexplorer.FP_DivergenceExplorer'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["class_map={'N': 0, 'P': 1}\n","from divexplorer.FP_DivergenceExplorer import FP_DivergenceExplorer\n","columns_compas.extend([compas_y, \"predicted\"])\n","\n","df = pd.DataFrame(test_set, columns = columns_compas)\n","# print(test_set.columns)\n","# print(columns_compas)\n","# print(df.columns)\n","# print(columns_compas)\n","\n","columns_compas.remove(compas_y)\n","columns_compas.remove('predicted')\n","\n","######\n","\n","min_sup=0.1\n","# min_sup = 0.05\n","# min_sup = 0.01\n","\n","fp_diver=FP_DivergenceExplorer(df,compas_y, \"predicted\", class_map=class_map)\n","FP_fm=fp_diver.getFrequentPatternDivergence(min_support=min_sup, metrics=[\"d_fpr\", \"d_fnr\", \"d_accuracy\"])\n","from divexplorer.FP_Divergence import FP_Divergence\n","fp_divergence_fpr=FP_Divergence(FP_fm, \"d_fpr\")\n","fp_divergence_fnr=FP_Divergence(FP_fm, \"d_fnr\")\n","fp_divergence_acc=FP_Divergence(FP_fm, \"d_accuracy\")\n","  # fp_divergence_fpr=FP_Divergence(FP_fm, \"d_fnr\")\n","INFO_VIZ=[\"support\", \"itemsets\",  fp_divergence_fpr.metric, fp_divergence_fpr.t_value_col]\n","INFO_VIZ2=[\"support\", \"itemsets\",  fp_divergence_fnr.metric, fp_divergence_fnr.t_value_col]\n","INFO_VIZ3=[\"support\", \"itemsets\",  fp_divergence_acc.metric, fp_divergence_acc.t_value_col]\n","eps=0.01\n","K=1000\n","d = fp_divergence_fpr.getDivergence(th_redundancy=eps)[INFO_VIZ]\n","d2 = fp_divergence_fnr.getDivergence(th_redundancy=eps)[INFO_VIZ2]\n","d3 = fp_divergence_acc.getDivergence(th_redundancy=eps)[INFO_VIZ3]\n","\n","\n","\n","pd.options.display.max_rows = 200\n","d = fp_divergence_fpr.getDivergence(th_redundancy=0)[INFO_VIZ].head(K)\n","# summerization\n","\n","d = fp_divergence_fpr.getDivergence(th_redundancy=eps)[INFO_VIZ]\n","d= d[d['d_fpr'] > 0]\n","d2= d2[d2['d_fnr'] > 0]\n","d3= d3[d3['d_accuracy'] > 0]\n","\n","dfpr = fairness_score_computation(d, 'd_fpr')\n","dfnr = fairness_score_computation(d2, 'd_fnr')\n","dacc = fairness_score_computation(d3, 'd_accuracy')\n","\"lg\", dfpr, dfnr, dacc"]},{"cell_type":"code","source":["d"],"metadata":{"id":"UQvKdCNAcopO"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MvIJDWIdt1IL"},"outputs":[],"source":["griddt.fit(train_x, train_label)\n","test_predict = griddt.predict(test_x)\n","test_set['predicted'] = test_predict\n","accuracy_score(test_label, test_set['predicted'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8UZfaqywuTiz"},"outputs":[],"source":["class_map={'N': 0, 'P': 1}\n","from divexplorer.FP_DivergenceExplorer import FP_DivergenceExplorer\n","columns_compas.extend([compas_y, \"predicted\"])\n","\n","df = pd.DataFrame(test_set, columns = columns_compas)\n","# print(test_set.columns)\n","# print(columns_compas)\n","# print(df.columns)\n","# print(columns_compas)\n","\n","columns_compas.remove(compas_y)\n","columns_compas.remove('predicted')\n","\n","######\n","\n","min_sup=0.1\n","# min_sup = 0.05\n","# min_sup = 0.01\n","\n","fp_diver=FP_DivergenceExplorer(df,compas_y, \"predicted\", class_map=class_map)\n","FP_fm=fp_diver.getFrequentPatternDivergence(min_support=min_sup, metrics=[\"d_fpr\", \"d_fnr\", \"d_accuracy\"])\n","from divexplorer.FP_Divergence import FP_Divergence\n","fp_divergence_fpr=FP_Divergence(FP_fm, \"d_fpr\")\n","fp_divergence_fnr=FP_Divergence(FP_fm, \"d_fnr\")\n","fp_divergence_acc=FP_Divergence(FP_fm, \"d_accuracy\")\n","  # fp_divergence_fpr=FP_Divergence(FP_fm, \"d_fnr\")\n","INFO_VIZ=[\"support\", \"itemsets\",  fp_divergence_fpr.metric, fp_divergence_fpr.t_value_col]\n","INFO_VIZ2=[\"support\", \"itemsets\",  fp_divergence_fnr.metric, fp_divergence_fnr.t_value_col]\n","INFO_VIZ3=[\"support\", \"itemsets\",  fp_divergence_acc.metric, fp_divergence_acc.t_value_col]\n","eps=0.01\n","K=1000\n","d = fp_divergence_fpr.getDivergence(th_redundancy=eps)[INFO_VIZ]\n","d2 = fp_divergence_fnr.getDivergence(th_redundancy=eps)[INFO_VIZ2]\n","d3 = fp_divergence_acc.getDivergence(th_redundancy=eps)[INFO_VIZ3]\n","\n","\n","\n","pd.options.display.max_rows = 200\n","d = fp_divergence_fpr.getDivergence(th_redundancy=0)[INFO_VIZ].head(K)\n","# summerization\n","\n","d = fp_divergence_fpr.getDivergence(th_redundancy=eps)[INFO_VIZ]\n","d= d[d['d_fpr'] > 0]\n","d2= d2[d2['d_fnr'] > 0]\n","d3= d3[d3['d_accuracy'] > 0]\n","\n","dfpr = fairness_score_computation(d, 'd_fpr')\n","dfnr = fairness_score_computation(d2, 'd_fnr')\n","dacc = fairness_score_computation(d3, 'd_accuracy')\n","\"dt\", dfpr, dfnr, dacc"]},{"cell_type":"code","source":["d"],"metadata":{"id":"bqyD2mPdc1Tw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"FklZKGXEc1Vb"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_08PrnM4tp24"},"outputs":[],"source":["gridrf.fit(train_x, train_label)\n","test_predict = gridrf.predict(test_x)\n","test_set['predicted'] = test_predict\n","accuracy_score(test_label, test_set['predicted'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UAWhq4Kttslt"},"outputs":[],"source":["class_map={'N': 0, 'P': 1}\n","from divexplorer.FP_DivergenceExplorer import FP_DivergenceExplorer\n","columns_compas.extend([compas_y, \"predicted\"])\n","\n","df = pd.DataFrame(test_set, columns = columns_compas)\n","# print(test_set.columns)\n","# print(columns_compas)\n","# print(df.columns)\n","# print(columns_compas)\n","\n","columns_compas.remove(compas_y)\n","columns_compas.remove('predicted')\n","\n","######\n","\n","min_sup=0.1\n","# min_sup = 0.05\n","# min_sup = 0.01\n","\n","fp_diver=FP_DivergenceExplorer(df,compas_y, \"predicted\", class_map=class_map)\n","FP_fm=fp_diver.getFrequentPatternDivergence(min_support=min_sup, metrics=[\"d_fpr\", \"d_fnr\", \"d_accuracy\"])\n","from divexplorer.FP_Divergence import FP_Divergence\n","fp_divergence_fpr=FP_Divergence(FP_fm, \"d_fpr\")\n","fp_divergence_fnr=FP_Divergence(FP_fm, \"d_fnr\")\n","fp_divergence_acc=FP_Divergence(FP_fm, \"d_accuracy\")\n","  # fp_divergence_fpr=FP_Divergence(FP_fm, \"d_fnr\")\n","INFO_VIZ=[\"support\", \"itemsets\",  fp_divergence_fpr.metric, fp_divergence_fpr.t_value_col]\n","INFO_VIZ2=[\"support\", \"itemsets\",  fp_divergence_fnr.metric, fp_divergence_fnr.t_value_col]\n","INFO_VIZ3=[\"support\", \"itemsets\",  fp_divergence_acc.metric, fp_divergence_acc.t_value_col]\n","eps=0.01\n","K=1000\n","d = fp_divergence_fpr.getDivergence(th_redundancy=eps)[INFO_VIZ]\n","d2 = fp_divergence_fnr.getDivergence(th_redundancy=eps)[INFO_VIZ2]\n","d3 = fp_divergence_acc.getDivergence(th_redundancy=eps)[INFO_VIZ3]\n","\n","\n","\n","pd.options.display.max_rows = 200\n","d = fp_divergence_fpr.getDivergence(th_redundancy=0)[INFO_VIZ].head(K)\n","# summerization\n","\n","d = fp_divergence_fpr.getDivergence(th_redundancy=eps)[INFO_VIZ]\n","d= d[d['d_fpr'] > 0]\n","d2= d2[d2['d_fnr'] > 0]\n","d3= d3[d3['d_accuracy'] > 0]\n","\n","dfpr = fairness_score_computation(d, 'd_fpr')\n","dfnr = fairness_score_computation(d2, 'd_fnr')\n","dacc = fairness_score_computation(d3, 'd_accuracy')\n","\"rf\", dfpr, dfnr, dacc"]},{"cell_type":"code","source":["d"],"metadata":{"id":"oVZuSqQYdPkd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"bqMVJz9FdPmD"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8D4Hs_UKspVO"},"outputs":[],"source":["svc.fit(train_x, train_label)\n","test_predict = svc.predict(test_x)\n","test_set['predicted'] = test_predict\n","accuracy_score(test_label, test_set['predicted'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3TYAF4eAmfhA"},"outputs":[],"source":["class_map={'N': 0, 'P': 1}\n","from divexplorer.FP_DivergenceExplorer import FP_DivergenceExplorer\n","columns_compas.extend([compas_y, \"predicted\"])\n","\n","df = pd.DataFrame(test_set, columns = columns_compas)\n","# print(test_set.columns)\n","# print(columns_compas)\n","# print(df.columns)\n","# print(columns_compas)\n","\n","columns_compas.remove(compas_y)\n","columns_compas.remove('predicted')\n","\n","######\n","\n","min_sup=0.1\n","# min_sup = 0.05\n","# min_sup = 0.01\n","\n","fp_diver=FP_DivergenceExplorer(df,compas_y, \"predicted\", class_map=class_map)\n","FP_fm=fp_diver.getFrequentPatternDivergence(min_support=min_sup, metrics=[\"d_fpr\", \"d_fnr\", \"d_accuracy\"])\n","from divexplorer.FP_Divergence import FP_Divergence\n","fp_divergence_fpr=FP_Divergence(FP_fm, \"d_fpr\")\n","fp_divergence_fnr=FP_Divergence(FP_fm, \"d_fnr\")\n","fp_divergence_acc=FP_Divergence(FP_fm, \"d_accuracy\")\n","  # fp_divergence_fpr=FP_Divergence(FP_fm, \"d_fnr\")\n","INFO_VIZ=[\"support\", \"itemsets\",  fp_divergence_fpr.metric, fp_divergence_fpr.t_value_col]\n","INFO_VIZ2=[\"support\", \"itemsets\",  fp_divergence_fnr.metric, fp_divergence_fnr.t_value_col]\n","INFO_VIZ3=[\"support\", \"itemsets\",  fp_divergence_acc.metric, fp_divergence_acc.t_value_col]\n","eps=0.01\n","K=1000\n","d = fp_divergence_fpr.getDivergence(th_redundancy=eps)[INFO_VIZ]\n","d2 = fp_divergence_fnr.getDivergence(th_redundancy=eps)[INFO_VIZ2]\n","d3 = fp_divergence_acc.getDivergence(th_redundancy=eps)[INFO_VIZ3]\n","\n","\n","\n","pd.options.display.max_rows = 200\n","d = fp_divergence_fpr.getDivergence(th_redundancy=0)[INFO_VIZ].head(K)\n","# summerization\n","\n","d = fp_divergence_fpr.getDivergence(th_redundancy=eps)[INFO_VIZ]\n","d= d[d['d_fpr'] > 0]\n","d2= d2[d2['d_fnr'] > 0]\n","d3= d3[d3['d_accuracy'] > 0]\n","\n","dfpr = fairness_score_computation(d, 'd_fpr')\n","dfnr = fairness_score_computation(d2, 'd_fnr')\n","dacc = fairness_score_computation(d3, 'd_accuracy')\n","\"svc\", dfpr, dfnr, dacc"]},{"cell_type":"code","source":["d"],"metadata":{"id":"VPmwnS4udmpY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vmhcs6wcgGiZ"},"source":["# Divexplorer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oT6Kmu35gRYe"},"outputs":[],"source":["# run divexplorer to find unfair groups\n","class_map={'N': 0, 'P': 1}\n","from divexplorer.FP_DivergenceExplorer import FP_DivergenceExplorer\n","\n","\n","######\n","# Yin: by consideri the unfair subgroups, we only consider the subgroups given by the protected attributes.\n","\n","columns_compas.extend([compas_y, \"predicted\"])\n","\n","df = pd.DataFrame(test_set, columns = columns_compas)\n","# print(test_set.columns)\n","print(columns_compas)\n","print(df.columns)\n","print(columns_compas)\n","\n","columns_compas.remove(compas_y)\n","columns_compas.remove('predicted')\n","\n","######\n","\n","min_sup=0.1\n","# min_sup = 0.05\n","# min_sup = 0.01\n","\n","fp_diver=FP_DivergenceExplorer(df,compas_y, \"predicted\", class_map=class_map)\n","FP_fm=fp_diver.getFrequentPatternDivergence(min_support=min_sup, metrics=[\"d_fpr\", \"d_fnr\", \"d_accuracy\"])\n","from divexplorer.FP_Divergence import FP_Divergence\n","fp_divergence_fpr=FP_Divergence(FP_fm, \"d_fpr\")\n","fp_divergence_fnr=FP_Divergence(FP_fm, \"d_fnr\")\n","fp_divergence_acc=FP_Divergence(FP_fm, \"d_accuracy\")\n","  # fp_divergence_fpr=FP_Divergence(FP_fm, \"d_fnr\")\n","INFO_VIZ=[\"support\", \"itemsets\",  fp_divergence_fpr.metric, fp_divergence_fpr.t_value_col]\n","INFO_VIZ2=[\"support\", \"itemsets\",  fp_divergence_fnr.metric, fp_divergence_fnr.t_value_col]\n","INFO_VIZ3=[\"support\", \"itemsets\",  fp_divergence_acc.metric, fp_divergence_acc.t_value_col]\n","eps=0.01\n","K=1000\n","d = fp_divergence_fpr.getDivergence(th_redundancy=eps)[INFO_VIZ].head(K)\n","d2 = fp_divergence_fnr.getDivergence(th_redundancy=eps)[INFO_VIZ2].head(K)\n","d3 = fp_divergence_acc.getDivergence(th_redundancy=eps)[INFO_VIZ3].head(K)\n","\n","# dfpr = fairness_score_computation(d, 'd_fpr')\n","# dfnr = fairness_score_computation(d2, 'd_fnr')\n","# dacc = fairness_score_computation(d3, 'd_accuracy')\n","\n","pd.options.display.max_rows = 200\n","d = fp_divergence_fpr.getDivergence(th_redundancy=0)[INFO_VIZ].head(K)\n","# summerization\n","\n","d = fp_divergence_fpr.getDivergence(th_redundancy=eps)[INFO_VIZ].head(K)\n","d= d[d['d_fpr'] > 0]\n","d, list(d[\"itemsets\"].iloc[0])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TKxJHipgsYoG"},"outputs":[],"source":["\n","d2 = fp_divergence_fnr.getDivergence(th_redundancy=eps)[INFO_VIZ2].head(K)\n","d2= d2[d2['d_fnr'] > 0]\n","d2, list(d2[\"itemsets\"].iloc[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"poCBx60phFm5"},"outputs":[],"source":["# with open('GC_results.csv', 'w', newline='') as file:\n","#   writer = csv.writer(file)\n","#   writer.writerow([\"Dataset\",\"Remedy\", \"Algorithm\",\"d_fpr\",\"d_fnr\", \"d_acc\", \"model_acc\"])\n","#   writer.writerow([\"German Credit\", \"Original\", \"None\", dfpr, dfnr, dacc, \"None\"])"]},{"cell_type":"markdown","metadata":{"id":"Kf1ffb1bggPp"},"source":["# For entire dataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aU9MirNCglkA"},"outputs":[],"source":["import itertools\n","def get_unfair_group(list_parse, entire = 1):\n","  unfair_group = []\n","  unfair_dict = {}\n","  names = []\n","  for col in columns_compas:\n","    found = False\n","    for item in list_parse:\n","      attr_given = item.split(\"=\")[0]\n","      if col == attr_given:\n","        unfair_group.append(int(item.split(\"=\")[1]))\n","        names.append(attr_given)\n","        unfair_dict[attr_given] = int(item.split(\"=\")[1])\n","        found = True\n","  # if use the entire dataset\n","  if entire:\n","    return unfair_group, names, columns_compas, unfair_dict\n","        # break\n","    # if found == False:\n","    #   unfair_group.append(-1)\n","  return unfair_group, names, list(set(columns_compas).symmetric_difference(set(names))), unfair_dict\n","def candidate_groups(skew_candidates, unfair_dict, ordering, names):\n","  candidate_combos = []\n","  candidate_ind = {}\n","  num = 0\n","  for i in range(len(skew_candidates)+1):\n","    temp_candidate = list(itertools.combinations(skew_candidates, i))\n","    for tc in temp_candidate:\n","      candidate_ind[num] = list(tc)\n","      num += 1\n","  return candidate_ind\n","\n","def name_val_dict(train_set,names):\n","  names_values = {}\n","  for n in names:\n","    names_values[n] = list(train_set[n].unique())\n","  return names_values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZgdhgqYZjTci"},"outputs":[],"source":["def get_temp(train_set, names, y_label):\n","  names2 = copy.deepcopy(names)\n","  names2.append(y_label)\n","  temp = train_set[names2]\n","  temp['cnt'] = 0\n","  temp2 = temp.groupby(names2)['cnt'].count().reset_index()\n","  temp2['cnt'].sum()\n","  return temp2, names\n","temp2, names = get_temp(train_set, columns_compas, compas_y)\n","temp2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uuEIYpHwjg_H"},"outputs":[],"source":["def get_temp_g(train_set, names, y_label):\n","  names2 = copy.deepcopy(names)\n","  names2.append(y_label)\n","  temp = train_set[names2]\n","  temp['cnt'] = 0\n","  temp_g = temp.groupby(names)['cnt'].count().reset_index()\n","  return temp, temp_g"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7k-CLD9ygjS9"},"outputs":[],"source":["unfair_group, unfair_names, skew_candidates, unfair_dict = get_unfair_group([])\n","print(unfair_group, unfair_names, skew_candidates, unfair_dict)\n","all_names = candidate_groups(skew_candidates, unfair_dict, columns_compas, unfair_names)\n","names_values = name_val_dict(train_set, names)\n","print(all_names[len(all_names)-1])\n","print(names_values)\n","all_names_lst = list(all_names.keys())[1:] # CHANGED HERE\n","all_names_lst.reverse()\n","print(all_names_lst)\n","print(all_names)"]},{"cell_type":"markdown","metadata":{"id":"TttueBgvg53C"},"source":["# Helper Functions"]},{"cell_type":"markdown","metadata":{"id":"g4TwHPK4hd0A"},"source":["## General Helper Functions\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wvC9RSgZkD80"},"outputs":[],"source":["def get_one_degree_neighbors(temp2, names, group_lst):\n","    result = []\n","    for i in range(len(group_lst)):\n","        d = copy.copy(temp2)\n","        for k in range(len(group_lst)):\n","            if k != i:\n","                d = d[d[names[k]] == group_lst[k]]\n","            else:\n","                d = d[d[names[k]] != group_lst[k]]\n","        # print(d)\n","        result.append(d)\n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sGQalez0kLkE"},"outputs":[],"source":["# compute the pos/neg ration of this neighbor\n","def compute_neighbors(group_lst, result):\n","    # compute the ratio of positive and negative records\n","    start2 = time.time()\n","    pos = 0\n","    neg = 0\n","    for r in result:\n","        total  = r['cnt'].sum()\n","        r = r[r[compas_y] == 1]\n","        pos += r['cnt'].sum()\n","        neg += total - r['cnt'].sum()\n","    if(neg == 0):\n","        return (pos, neg, -1)\n","    end2 = time.time()\n","    # print(\"The time to compute the neighbor counts for \" +  str(group_lst) +\" is \" + str(end2-start2))\n","    return(pos, neg, pos/neg)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ijabVWcBjtCh"},"outputs":[],"source":["def compute_diff_add_and_remove(group_lst, temp2, need_positive_or_negative, label, names):\n","    d = copy.copy(temp2)\n","    for i in range(len(group_lst)):\n","        d = d[d[names[i]] == group_lst[i]]\n","        # print(len(d))\n","    total =  d['cnt'].sum()\n","    # Total here was 0: here, errors when this is commented out\n","    if total == 0:\n","      return -1\n","    d = d[d[label] == 1]\n","    pos = d['cnt'].sum()\n","    # print(d, group_lst)\n","    neg = total - pos\n","    result = get_one_degree_neighbors(temp2,names, group_lst)\n","    neighbors = compute_neighbors(group_lst, result)\n","    if(need_positive_or_negative == 1):\n","        # need pos\n","        x = Symbol('x')\n","        # print(pos, neg , neighbors[2])\n","        try:\n","          diff = solve((pos + x)/ (neg - x) - neighbors[2])[0]\n","        except:\n","          return -1\n","        # print(solve((pos + x)/ (neg - x) - neighbors[2]))\n","    else:\n","        #need negative\n","        x = Symbol('x')\n","        try:\n","          diff = solve((pos - x)/ (neg + x) - neighbors[2])[0]\n","        except:\n","          return -1\n","    print(neighbors[2],pos, neg, diff)\n","    return diff"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TH7YGCuh69D7"},"outputs":[],"source":["def compute_diff_add(group_lst, temp2, names, label_y, need_positive_or_negative):\n","    # print(\"compute diff\")\n","    d = copy.copy(temp2)\n","    # print(\"here\", d, names)\n","    for i in range(len(group_lst)):\n","        # print(d, group_lst[i])\n","        d = d[d[names[i]] == group_lst[i]]\n","    total =  d['cnt'].sum()\n","    d = d[d[label_y] == 1]\n","    pos = d['cnt'].sum()\n","    neg = total - pos\n","    result = get_one_degree_neighbors(temp2, names, group_lst)\n","    neighbors = compute_neighbors(group_lst, result)\n","    if(need_positive_or_negative == 1):\n","        # need pos\n","        # if neg == 0:\n","        #     # to avoid zero neg\n","        #     neg = 1\n","        x = Symbol('x')\n","        diff = solve((pos + x)/ neg -  neighbors[2])[0]\n","        # enough = neighbors[2] * neg\n","        # diff = round_int(enough - pos)\n","        print(neighbors[2], pos, neg, diff)\n","    else:\n","        #need negative\n","        # if(pos == 0):\n","        #     diff  = 0\n","        #     enough = 0\n","        # else:\n","        # enough = pos / (neighbors[2] )\n","        x = Symbol('x')\n","        diff = solve(pos/ (neg + x) -  neighbors[2])[0]\n","    print(neighbors[2], pos, neg, diff)\n","    return diff\n","\n","def compute_diff_remove(group_lst, temp2, names, label_y, need_positive_or_negative):\n","    d = copy.copy(temp2)\n","    for i in range(len(group_lst)):\n","        # print(d, group_lst[i])\n","        d = d[d[names[i]] == group_lst[i]]\n","    total =  d['cnt'].sum()\n","    d = d[d[label_y] == 1]\n","    pos = d['cnt'].sum()\n","    neg = total - pos\n","    result = get_one_degree_neighbors(temp2, names, group_lst)\n","    neighbors = compute_neighbors(group_lst, result)\n","    if(need_positive_or_negative == 1):\n","        # need pos, remove some neg\n","        x = Symbol('x')\n","        try:\n","          diff = solve( pos/ (neg - x) -  neighbors[2])[0]\n","        except:\n","          return -1\n","        # enough = neighbors[2] * neg\n","        # diff = round_int(enough - pos)\n","        print(neighbors[2], pos, neg, diff)\n","    else:\n","        #need negative\n","        x = Symbol('x')\n","        # diff = solve((pos -x )/ neg -  neighbors[2])[0]\n","        try:\n","          diff = solve((pos -x )/ neg -  neighbors[2])[0]\n","        except:\n","          return -1\n","        print(neighbors[2], pos, neg, diff)\n","    return diff\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3J0EcLGqrH0-"},"outputs":[],"source":["from divexplorer.FP_DivergenceExplorer import FP_DivergenceExplorer\n","\n","def div_results(db, remedy, algo):\n","  columns_compas.extend([compas_y, \"predicted\"])\n","\n","  df = pd.DataFrame(test_set, columns = columns_compas)\n","  # print(test_set.columns)\n","  print(columns_compas)\n","  print(df.columns)\n","  print(columns_compas)\n","\n","  columns_compas.remove(compas_y)\n","  columns_compas.remove('predicted')\n","  class_map={'N': 0, 'P': 1}\n","\n","  min_sup=0.1\n","\n","  # min_sup = 0.05\n","\n","  fp_diver=FP_DivergenceExplorer(df,compas_y, \"predicted\", class_map=class_map)\n","  FP_fm=fp_diver.getFrequentPatternDivergence(min_support=min_sup, metrics=[\"d_fpr\", \"d_fnr\", \"d_accuracy\"])\n","  from divexplorer.FP_Divergence import FP_Divergence\n","  fp_divergence_fpr=FP_Divergence(FP_fm, \"d_fpr\")\n","  fp_divergence_fnr=FP_Divergence(FP_fm, \"d_fnr\")\n","  fp_divergence_acc=FP_Divergence(FP_fm, \"d_accuracy\")\n","  # fp_divergence_fpr=FP_Divergence(FP_fm, \"d_fnr\")\n","  INFO_VIZ=[\"support\", \"itemsets\",  fp_divergence_fpr.metric, fp_divergence_fpr.t_value_col]\n","  INFO_VIZ2=[\"support\", \"itemsets\",  fp_divergence_fnr.metric, fp_divergence_fnr.t_value_col]\n","  INFO_VIZ3=[\"support\", \"itemsets\",  fp_divergence_acc.metric, fp_divergence_acc.t_value_col]\n","\n","  K=10\n","  pd.options.display.max_rows = 200\n","  d = fp_divergence_fpr.getDivergence(th_redundancy=0)[INFO_VIZ].head(K)\n","  # summerization\n","  eps=0.01\n","\n","  # d = fp_divergence_fpr.getDivergence(th_redundancy=eps)[INFO_VIZ].head(K)\n","  # d2 = fp_divergence_fnr.getDivergence(th_redundancy=eps)[INFO_VIZ2].head(K)\n","  # d3 = fp_divergence_acc.getDivergence(th_redundancy=eps)[INFO_VIZ3].head(K)\n","\n","  d = fp_divergence_fpr.getDivergence(th_redundancy=eps)[INFO_VIZ]\n","  d2 = fp_divergence_fnr.getDivergence(th_redundancy=eps)[INFO_VIZ2]\n","  d3 = fp_divergence_acc.getDivergence(th_redundancy=eps)[INFO_VIZ3]\n","\n","  d= d[d['d_fpr'] > 0]\n","  d2= d2[d2['d_fnr'] > 0]\n","\n","  dfpr = fairness_score_computation(d, 'd_fpr')\n","  dfnr = fairness_score_computation(d2, 'd_fnr')\n","  dacc = fairness_score_computation(d3, 'd_accuracy')\n","  print(dfpr)\n","  print(dfnr)\n","  print(dacc)\n","  accuracy = accuracy_score(test_label, test_predict)\n","  print(\"accuracy is \" , accuracy)\n","  writelist = [db,remedy,algo, dfpr, dfnr, dacc, accuracy]\n","  with open('GC_results.csv', 'a', newline='') as file:\n","    writer = csv.writer(file)\n","    writer.writerow(writelist)\n","  print(writelist)\n","  return d,d2,d3"]},{"cell_type":"markdown","metadata":{"id":"4iEQ83nxkQr4"},"source":["## Optimized Helper Function\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vuxfZHSlhXNR"},"outputs":[],"source":["# helper function for optimized\n","def compute_neighbors_opt(group_lst,lst_of_counts, pos, neg):\n","    #start2 = time.time()\n","    times = len(group_lst)\n","    pos_cnt = 0\n","    neg_cnt = 0\n","    for i in range(times):\n","        df_groupby = lst_of_counts[i]\n","        temp_group_lst_pos = copy.copy(group_lst)\n","        temp_group_lst_neg = copy.copy(group_lst)\n","        del temp_group_lst_pos[i]\n","        del temp_group_lst_neg[i]\n","        # count positive\n","        temp_group_lst_pos.append(1)\n","        group_tuple_pos = tuple(temp_group_lst_pos)\n","        if group_tuple_pos in df_groupby.keys():\n","            pos_cnt += df_groupby[group_tuple_pos]\n","        else:\n","            pos_cnt += 0\n","        # count negative\n","        temp_group_lst_neg.append(0)\n","        group_tuple_neg = tuple(temp_group_lst_neg)\n","        if group_tuple_neg in df_groupby.keys():\n","            neg_cnt += df_groupby[group_tuple_neg]\n","        else:\n","            neg_cnt += 0\n","    pos_val = pos_cnt - times* pos\n","    neg_val = neg_cnt - times* neg\n","    #end2 = time.time()\n","    #print(\"The time to compute the neighbor counts for \" +  str(group_lst) +\" is \" + str(end2-start2))\n","    if neg_val == -1 or (neg_val == 0 and pos_val == 0):\n","        return (pos_val, neg_val, -1)\n","    if pos_val == 0 or neg_val == 0:\n","        return (pos_val, neg_val, 0)\n","    return (pos_val, neg_val, pos_val/neg_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J8Iny4kJhm4V"},"outputs":[],"source":["# get the list of neighbors\n","def get_one_degree_neighbors_opt(group_lst):\n","    result = []\n","    for i in range(len(group_lst)):\n","        d = copy.copy(group_lst)\n","        d[i] = 'x'\n","        result.append(d)\n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1MmwA0X5hofE"},"outputs":[],"source":["def determine_problematic_opt(group_lst, names, temp2, lst_of_counts, label, threshold= 0.3):\n","    #0: ok group, 1: need negative records, 2: need positive records\n","    d = copy.copy(temp2)\n","    for i in range(len(group_lst)):\n","        d = d[d[names[i]] == group_lst[i]]\n","    total =  d['cnt'].sum()\n","    d = d[d[label] == 1]\n","    pos = d['cnt'].sum()\n","    neg = total - pos\n","    neighbors = compute_neighbors_opt(group_lst,lst_of_counts, pos, neg)\n","    if(neighbors[2] == -1):\n","        # there is no neighbors\n","        return 0\n","    if(total > filter_count):\n","        # need to be large enough, need to adjust with different datasets.\n","        if neg == 0:\n","            if (pos > neighbors[2]):\n","                return 1\n","            if(pos <= neighbors[2]):\n","                return 0\n","        if (pos/(neg) - neighbors[2] > threshold):\n","            # too many positive records\n","            return 1\n","        if (neighbors[2] - pos/(neg) > threshold):\n","            return 2\n","    return 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A76VlG-bhqqS"},"outputs":[],"source":["def compute_problematic_opt(temp2, temp_g, names, label, lst_of_counts):\n","    need_pos = []\n","    need_neg = []\n","    for index, row in temp_g.iterrows():\n","        group_lst = []\n","        for n in names:\n","            group_lst.append(row[n])\n","        problematic = determine_problematic_opt(group_lst, names, temp2, lst_of_counts,label)\n","#         #print(problematic)\n","        if(problematic == 1):\n","            if group_lst not in need_neg:\n","                need_neg.append(group_lst)\n","        if(problematic == 2):\n","            if group_lst not in need_pos:\n","                need_pos.append(group_lst)\n","    return need_pos, need_neg"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tZd2c6PAhtTH"},"outputs":[],"source":["# build the list of X00\n","def compute_lst_of_counts(temp, names, label):\n","    # get the list of group-by attributes\n","    lst_of_counts = []\n","    for i in range(len(names)):\n","        grp_names = copy.copy(names)\n","        del grp_names[i]\n","        grp_names.append(label)\n","        temp_df = temp.groupby(grp_names)['cnt'].count()\n","        lst_of_counts.append(temp_df)\n","    return lst_of_counts\n","\n","def get_tuple(group_lst):\n","    return tuple(group_lst)\n","\n","\n","def get_temp_g(train_set, names, y_label):\n","  names2 = copy.deepcopy(names)\n","  names2.append(y_label)\n","  temp = train_set[names2]\n","  temp['cnt'] = 0\n","  temp_g = temp.groupby(names)['cnt'].count().reset_index()\n","  return temp, temp_g"]},{"cell_type":"markdown","metadata":{"id":"qnQ09ffAkalO"},"source":["# Preferential Sampling"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2L4D0kJ7h2lS"},"outputs":[],"source":["from sklearn.naive_bayes import MultinomialNB\n","def pref_sampling_opt(train_set, cols_given, label, need_pos, need_neg):\n","    if len(need_pos)+ len(need_neg) > 0:\n","        temp_train_x = pd.DataFrame(train_set, columns = columns_all)\n","        temp_train_label = pd.DataFrame(train_set, columns = [label])\n","        temp_train_label = temp_train_label[label]\n","        temp_train_label = temp_train_label.astype('int')\n","        mnb = MultinomialNB()\n","        mnb = mnb.fit(temp_train_x, temp_train_label)\n","        probs = mnb.predict_proba(temp_train_x)[:,0]\n","        train_set[\"prob\"] = abs(probs - 0.5)\n","        # get the set of\n","    new_train_set = pd.DataFrame(columns = list(train_set.columns))\n","    updated_pos = 0\n","    for i in need_pos:\n","        # needs to updated more positive records\n","        # print(i)\n","        temp_df = copy.deepcopy(train_set)\n","        for n in range(len(i)):\n","          temp_df = temp_df[temp_df[cols_given[n]] == i[n]]\n","        # update the skew and diff\n","        idx = list(temp_df.index)\n","        train_set.loc[idx, 'skewed'] = 1\n","        idx_pos = list(temp_df[(getattr(temp_df, label) == 1)].index)\n","        if(len(idx_pos) == 0):\n","          # if there is no positive\n","          idx_neg = list(temp_df[(getattr(temp_df, label) == 0)].index)\n","          neg_ranked = train_set.loc[idx_neg].sort_values(by=\"prob\", ascending=True)\n","          new_train_set = pd.concat([new_train_set, neg_ranked], ignore_index=True)\n","          continue\n","        idx_neg = list(temp_df[(getattr(temp_df, label) == 0)].index)\n","        pos_ranked = train_set.loc[idx_pos].sort_values(by=\"prob\", ascending=True)\n","        neg_ranked = train_set.loc[idx_neg].sort_values(by=\"prob\", ascending=True)\n","        diff = compute_diff_add_and_remove(i, temp2,  1, compas_y, names)\n","        if diff == -1:\n","          new_train_set = pd.concat([new_train_set, pos_ranked], ignore_index=True)\n","          new_train_set = pd.concat([new_train_set, neg_ranked], ignore_index=True)\n","          continue\n","        train_set.loc[idx, 'diff'] = int(diff)\n","        cnt = int(train_set.loc[idx_pos[0]][\"diff\"])\n","        updated_pos += cnt * 2\n","        # add more records when there are not enough available records\n","        new_train_set = pd.concat([new_train_set, pos_ranked], ignore_index=True)\n","        temp_cnt = cnt\n","        if len(pos_ranked) >= temp_cnt:\n","            new_train_set = pd.concat([new_train_set,pos_ranked[0:cnt]], ignore_index=True)\n","        else:\n","            while(temp_cnt > 0 ):\n","                new_train_set = pd.concat([new_train_set,pos_ranked[0:temp_cnt]], ignore_index=True)\n","            # duplicate the dataframe\n","                temp_cnt = temp_cnt - len(pos_ranked)\n","        # duplicate the top cnt records from the pos\n","        # remove the top cnt records from the neg\n","        if cnt == 0:\n","          new_train_set = pd.concat([new_train_set, neg_ranked], ignore_index=True)\n","          # print(\"+++++++++++++++++++++++++++++++++++++++++++++\")\n","          # print(i)\n","          # print(len(pos_ranked)+cnt)\n","          # print(len(neg_ranked))\n","        else:\n","          new_train_set = pd.concat([new_train_set, neg_ranked[cnt-1:-1]], ignore_index=True)\n","          # print(\"+++++++++++++++++++++++++++++++++++++++++++++\")\n","          # print(i)\n","          # print(len(pos_ranked)+cnt)\n","          # print(len(neg_ranked[cnt-1:-1]))\n","    print(\"updated {} positive records\".format(str(updated_pos)))\n","    updated_neg = 0\n","    # adding more records to the need_neg set\n","    for i in need_neg:\n","        # print(i)\n","        # list of idx belongs to this group\n","        temp_df = copy.deepcopy(train_set)\n","        for n in range(len(i)):\n","          temp_df = temp_df[temp_df[cols_given[n]] == i[n]]\n","        # update the skew and diff\n","        idx = list(temp_df.index)\n","        train_set.loc[idx, 'skewed'] = 1\n","        idx_pos = list(temp_df[(getattr(temp_df, label) == 1)].index)\n","        idx_neg = list(temp_df[(getattr(temp_df, label) == 0)].index)\n","        if(len(idx_neg) == 0):\n","          pos_ranked = train_set.loc[idx_pos].sort_values(by=\"prob\", ascending=True)\n","          new_train_set = pd.concat([new_train_set, pos_ranked], ignore_index=True)\n","          continue\n","        pos_ranked = train_set.loc[idx_pos].sort_values(by=\"prob\", ascending=True)\n","        neg_ranked = train_set.loc[idx_neg].sort_values(by=\"prob\", ascending=True)\n","        diff = compute_diff_add_and_remove(i, temp2, 0, compas_y, names)\n","        if diff == -1:\n","          new_train_set = pd.concat([new_train_set, neg_ranked], ignore_index=True)\n","          new_train_set = pd.concat([new_train_set, pos_ranked], ignore_index=True)\n","          continue\n","        train_set.loc[idx, 'diff'] = int(diff)\n","        cnt = int(train_set.loc[idx_pos[0]][\"diff\"])\n","        updated_neg += cnt * 2\n","        # add more records when there are not enough available records\n","        new_train_set = pd.concat([new_train_set, neg_ranked], ignore_index=True)\n","        temp_cnt = cnt\n","        if len(neg_ranked) >= temp_cnt:\n","            new_train_set = pd.concat([new_train_set,neg_ranked[0:cnt]], ignore_index=True)\n","        else:\n","            while(temp_cnt > 0 ):\n","                new_train_set = pd.concat([new_train_set,neg_ranked[0:temp_cnt]], ignore_index=True)\n","            # duplicate the dataframe\n","                temp_cnt = temp_cnt - len(neg_ranked)\n","        # duplicate the top cnt records from the pos\n","        # remove the top cnt records from the neg\n","        if cnt ==0:\n","          new_train_set = pd.concat([new_train_set, pos_ranked], ignore_index=True)\n","          # print(\"+++++++++++++++++++++++++++++++++++++++++++++\")\n","          # print(i)\n","          # print(len(neg_ranked)+cnt)\n","          # print(len(pos_ranked))\n","        else:\n","          new_train_set = pd.concat([new_train_set, pos_ranked[cnt-1:-1]], ignore_index=True)\n","          # print(\"+++++++++++++++++++++++++++++++++++++++++++++\")\n","          # print(i)\n","          # print(len(neg_ranked)+cnt)\n","          # print(len(pos_ranked[cnt-1:-1]))\n","        #print(len(new_train_set[new_train_set['income'] == 1]), len(new_train_set[new_train_set['income'] == 0]))\n","        # print(train_set.loc[idx_neg])\n","    print(\"updated {} negative records\".format(str(updated_neg)))\n","    # add the other irrelavant items:\n","    idx_irr = list(train_set[train_set['skewed'] == 0].index)\n","    irr_df = train_set.loc[idx_irr]\n","    new_train_set = pd.concat([new_train_set, irr_df], ignore_index=True)\n","    print(\"The new dataset contains {} rows.\".format(str(len(new_train_set))))\n","    new_train_set.reset_index()\n","    return new_train_set\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vMrjlEciiv--"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"paqO9W2jeTMX"},"outputs":[],"source":["# unfair_group, unfair_names, skew_candidates, unfair_dict = get_unfair_group([])\n","# print(unfair_group, unfair_names, skew_candidates, unfair_dict)\n","# combos_dict, all_names = candidate_groups(skew_candidates, unfair_dict, columns_compas, unfair_names)\n","# print(combos_dict, all_names)\n","\n","# # the\n","# print(list(all_names.keys())[len(columns_compas)+1:])\n","# all_names_lst = list(all_names.keys())[len(columns_compas)+1:]\n","# all_names_lst.reverse()\n","# print(all_names_lst)\n","\n","def find_top(all_names):\n","  all_names_lst_top = []\n","  for all in range(len(all_names)):\n","    if len(all_names[all]) == 1: # CHANGED HERE\n","      all_names_lst_top.append(all)\n","  return all_names_lst_top"]},{"cell_type":"markdown","metadata":{"id":"SGdoCnWLlkc4"},"source":["## Run Algorithm Lattice\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QfMMRp-S4vTL"},"outputs":[],"source":["\n","#get all of the candidate groups possible with the combos and names\n","\n","new_train_data = copy.deepcopy(train_set)\n","\n","#iterate over all the names to get the temp2 df for each name\n","for a in all_names_lst:\n","# for a in [all_names_lst[0]]: # leaf\n","# for a in [all_names_lst[-1:x]]: # top\n","  print(\"/////////////\")\n","  print(a)\n","  temp2, names = get_temp(new_train_data, all_names[a], compas_y)\n","  temp, temp_g = get_temp_g(new_train_data, names, compas_y)\n","  temp_g = temp_g[temp_g['cnt'] > filter_count]\n","  lst_of_counts = compute_lst_of_counts(temp, names, compas_y)\n","  start = time.time()\n","  need_pos, need_neg = compute_problematic_opt(temp2, temp_g, names, compas_y, lst_of_counts)\n","  end = time.time()\n","  excute_time = end - start\n","  # print(\"The time to compute unfair group is {}\".format(str(excute_time)))\n","  print(\"The sets of need pos and neg are\")\n","  print(need_pos)\n","  print(need_neg)\n","  new_train_data['skewed'] = 0\n","  new_train_data[\"diff\"] = 0\n","  print(\"started pref sampling\")\n","  print(names)\n","  new_train_data = pref_sampling_opt(new_train_data, names, compas_y, need_pos, need_neg)\n","  # print(new_train_data)\n","  print(new_train_data[compas_y].value_counts())\n","new_train_x = pd.DataFrame(new_train_data, columns = columns_all)\n","new_train_label = pd.DataFrame(new_train_data, columns = [compas_y])\n","new_train_label = new_train_label[compas_y]\n","new_train_label = new_train_label.astype('int')\n","grid_new = GridSearchCV(tree.DecisionTreeClassifier(), param_grid = param, cv = 6)\n","grid_new.fit(new_train_x, new_train_label)\n","print(\"model accuracy\")\n","print(\"best\", grid_new.best_score_)\n","test_predict = grid_new.predict(test_x)\n","print(\"fpr and fnr\")\n","# print(fpr_onegroup(new_labels.tolist(), predict))\n","# print(fnr_onegroup(new_labels.tolist(), predict))\n","print(fpr_onegroup(list(test_label), test_predict))\n","print(fnr_onegroup(list(test_label), test_predict))\n","# print(pref_sampling_mod(train_x, train_set, train_label, names, compas_y, need_pos, need_neg))"]},{"cell_type":"markdown","metadata":{"id":"euVPjcb_lqe2"},"source":["### Results Lattice"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bv6m5nM7lujt"},"outputs":[],"source":["print(\"dt\")\n","griddt.fit(new_train_x, new_train_label)\n","print(\"best\", griddt.best_score_)\n","test_predict = griddt.predict(test_x)\n","test_set['predicted'] = test_predict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jWyPWk3RuP59"},"outputs":[],"source":["# run divexplorer to find unfair groups\n","# run divexplorer to find unfair groups\n","class_map={'N': 0, 'P': 1}\n","from divexplorer.FP_DivergenceExplorer import FP_DivergenceExplorer\n","\n","\n","######\n","# Yin: by consideri the unfair subgroups, we only consider the subgroups given by the protected attributes.\n","\n","columns_compas.extend([compas_y, \"predicted\"])\n","\n","df = pd.DataFrame(test_set, columns = columns_compas)\n","# print(test_set.columns)\n","print(columns_compas)\n","print(df.columns)\n","print(columns_compas)\n","\n","columns_compas.remove(compas_y)\n","columns_compas.remove('predicted')\n","\n","######\n","\n","min_sup=0.1\n","# min_sup = 0.05\n","# min_sup = 0.01\n","\n","fp_diver=FP_DivergenceExplorer(df,compas_y, \"predicted\", class_map=class_map)\n","FP_fm=fp_diver.getFrequentPatternDivergence(min_support=min_sup, metrics=[\"d_fpr\", \"d_fnr\", \"d_accuracy\"])\n","from divexplorer.FP_Divergence import FP_Divergence\n","fp_divergence_fpr=FP_Divergence(FP_fm, \"d_fpr\")\n","fp_divergence_fnr=FP_Divergence(FP_fm, \"d_fnr\")\n","fp_divergence_acc=FP_Divergence(FP_fm, \"d_accuracy\")\n","  # fp_divergence_fpr=FP_Divergence(FP_fm, \"d_fnr\")\n","INFO_VIZ=[\"support\", \"itemsets\",  fp_divergence_fpr.metric, fp_divergence_fpr.t_value_col]\n","INFO_VIZ2=[\"support\", \"itemsets\",  fp_divergence_fnr.metric, fp_divergence_fnr.t_value_col]\n","INFO_VIZ3=[\"support\", \"itemsets\",  fp_divergence_acc.metric, fp_divergence_acc.t_value_col]\n","eps=0.01\n","K=1000\n","d = fp_divergence_fpr.getDivergence(th_redundancy=eps)[INFO_VIZ]\n","d2 = fp_divergence_fnr.getDivergence(th_redundancy=eps)[INFO_VIZ2]\n","d3 = fp_divergence_acc.getDivergence(th_redundancy=eps)[INFO_VIZ3]\n","\n","# dfpr = fairness_score_computation(d, 'd_fpr')\n","# dfnr = fairness_score_computation(d2, 'd_fnr')\n","# dacc = fairness_score_computation(d3, 'd_accuracy')\n","\n","pd.options.display.max_rows = 200\n","d = fp_divergence_fpr.getDivergence(th_redundancy=0)[INFO_VIZ]\n","# summerization\n","\n","d = fp_divergence_fpr.getDivergence(th_redundancy=eps)[INFO_VIZ]\n","d= d[d['d_fpr'] > 0]\n","# d, list(d[\"itemsets\"].iloc[0])\n","\n","# d2 = fp_divergence_fnr.getDivergence(th_redundancy=eps)[INFO_VIZ2].head(K)\n","# d2= d2[d2['d_fnr'] > 0]\n","# d2, list(d2[\"itemsets\"].iloc[0])"]},{"cell_type":"code","source":["d"],"metadata":{"id":"clr8VLFpexgX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dfpr = fairness_score_computation(d, 'd_fpr')\n","dfpr"],"metadata":{"id":"VDG6k4U0exh9"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MHfpY_YLsMmd"},"outputs":[],"source":["d2 = fp_divergence_fnr.getDivergence(th_redundancy=eps)[INFO_VIZ2]\n","d2= d2[d2['d_fnr'] > 0]\n","d2, list(d2[\"itemsets\"].iloc[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fn8KEHI3ERym"},"outputs":[],"source":["print(fairness_score_computation(d, 'd_fpr'))\n","print(fairness_score_computation(d2, 'd_fnr'))\n","print(fairness_score_computation(d3, 'd_accuracy'))\n","d,d1,d2 = div_results(\"German Credit\", \"Preferential Sampling-Lattice\",\"DT\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GDvLtC3BlDpa"},"outputs":[],"source":["# d,d1,d2 = div_results(\"German Credit\", \"Preferential Sampling-Lattice\",\"Decision Tree\")\n","print()\n","print(\"rf\")\n","gridrf.fit(new_train_x, new_train_label)\n","print(\"best\", gridrf.best_score_)\n","test_predict = gridrf.predict(test_x)\n","print(\"fpr and fnr\")\n","# print(fpr_onegroup(new_labels.tolist(), predict))\n","# print(fnr_onegroup(new_labels.tolist(), predict))\n","print(fpr_onegroup(list(test_label), test_predict))\n","print(fnr_onegroup(list(test_label), test_predict))\n","test_set['predicted'] = test_predict\n","r,r2,r3 = div_results(\"German Credit\", \"Preferential Sampling-Lattice\",\"RF\")\n","\n","print()\n","print(\"logistic\")\n","gridl.fit(new_train_x, new_train_label)\n","print(\"best\", gridl.best_score_)\n","test_predict = gridl.predict(test_x)\n","print(\"fpr and fnr\")\n","# print(fpr_onegroup(new_labels.tolist(), predict))\n","# print(fnr_onegroup(new_labels.tolist(), predict))\n","print(fpr_onegroup(list(test_label), test_predict))\n","print(fnr_onegroup(list(test_label), test_predict))\n","test_set['predicted'] = test_predict\n","r,r2,r3 = div_results(\"German Credit\", \"Preferential Sampling-Lattice\",\"L\")\n","\n","print()\n","print(\"svm\")\n","svc.fit(new_train_x, new_train_label)\n","# print(\"best\", svc.best_score_)\n","test_predict = svc.predict(test_x)\n","print(\"fpr and fnr\")\n","# print(fpr_onegroup(new_labels.tolist(), predict))\n","# print(fnr_onegroup(new_labels.tolist(), predict))\n","print(fpr_onegroup(list(test_label), test_predict))\n","print(fnr_onegroup(list(test_label), test_predict))\n","test_set['predicted'] = test_predict\n","s,s2,s3 = div_results(\"German Credit\", \"Preferential Sampling-Lattice\",\"SVM\")"]},{"cell_type":"markdown","metadata":{"id":"FaHdC0EaqzZz"},"source":["## Run Algorithm Leaf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x-kS0DqRq2_S"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nh523URuq7AD"},"outputs":[],"source":["\n","#get all of the candidate groups possible with the combos and names\n","\n","new_train_data = copy.deepcopy(train_set)\n","\n","#iterate over all the names to get the temp2 df for each name\n","for a in [all_names_lst[0]]:\n","# for a in [all_names_lst[0]]: # leaf\n","# for a in [all_names_lst[-1:x]]: # top\n","  print(\"/////////////\")\n","  print(a)\n","  temp2, names = get_temp(new_train_data, all_names[a], compas_y)\n","  temp, temp_g = get_temp_g(new_train_data, names, compas_y)\n","  temp_g = temp_g[temp_g['cnt'] > filter_count]\n","  lst_of_counts = compute_lst_of_counts(temp, names, compas_y)\n","  start = time.time()\n","  need_pos, need_neg = compute_problematic_opt(temp2, temp_g, names, compas_y, lst_of_counts)\n","  end = time.time()\n","  excute_time = end - start\n","  # print(\"The time to compute unfair group is {}\".format(str(excute_time)))\n","  print(\"The sets of need pos and neg are\")\n","  print(need_pos)\n","  print(need_neg)\n","  new_train_data['skewed'] = 0\n","  new_train_data[\"diff\"] = 0\n","  print(\"started pref sampling\")\n","  new_train_data = pref_sampling_opt(new_train_data, names, compas_y, need_pos, need_neg)\n","  # print(new_train_data)\n","  print(new_train_data[compas_y].value_counts())\n","new_train_x = pd.DataFrame(new_train_data, columns = columns_all)\n","new_train_label = pd.DataFrame(new_train_data, columns = [compas_y])\n","new_train_label = new_train_label[compas_y]\n","new_train_label = new_train_label.astype('int')\n","grid_new = GridSearchCV(tree.DecisionTreeClassifier(), param_grid = param, cv = 6)\n","grid_new.fit(new_train_x, new_train_label)\n","print(\"model accuracy\")\n","print(\"best\", grid_new.best_score_)\n","test_predict = grid_new.predict(test_x)\n","print(\"fpr and fnr\")\n","# print(fpr_onegroup(new_labels.tolist(), predict))\n","# print(fnr_onegroup(new_labels.tolist(), predict))\n","print(fpr_onegroup(list(test_label), test_predict))\n","print(fnr_onegroup(list(test_label), test_predict))\n","# print(pref_sampling_mod(train_x, train_set, train_label, names, compas_y, need_pos, need_neg))"]},{"cell_type":"markdown","metadata":{"id":"27rcEGB9rhrf"},"source":["### Results Leaf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F1iW-8AYrlzn"},"outputs":[],"source":["test_set['predicted'] = test_predict\n","d, d2, d3 = div_results(\"German Credit\", \"Preferential Sampling-Leaf\",\"DT\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4-1zYQIWsDdn"},"outputs":[],"source":["# d,d1,d2 = div_results(\"German Credit\", \"Preferential Sampling-Leaf\",\"RF\")\n","print()\n","print(\"rf\")\n","gridrf.fit(new_train_x, new_train_label)\n","print(\"best\", gridrf.best_score_)\n","test_predict = gridrf.predict(test_x)\n","print(\"fpr and fnr\")\n","# print(fpr_onegroup(new_labels.tolist(), predict))\n","# print(fnr_onegroup(new_labels.tolist(), predict))\n","print(fpr_onegroup(list(test_label), test_predict))\n","print(fnr_onegroup(list(test_label), test_predict))\n","test_set['predicted'] = test_predict\n","r,r2,r3 = div_results(\"German Credit\", \"Preferential Sampling-Leaf\",\"RF\")\n","\n","print()\n","print(\"logistic\")\n","gridl.fit(new_train_x, new_train_label)\n","print(\"best\", gridl.best_score_)\n","test_predict = gridl.predict(test_x)\n","print(\"fpr and fnr\")\n","# print(fpr_onegroup(new_labels.tolist(), predict))\n","# print(fnr_onegroup(new_labels.tolist(), predict))\n","print(fpr_onegroup(list(test_label), test_predict))\n","print(fnr_onegroup(list(test_label), test_predict))\n","test_set['predicted'] = test_predict\n","r,r2,r3 = div_results(\"German Credit\", \"Preferential Sampling-Leaf\",\"L\")\n","\n","print()\n","print(\"svm\")\n","svc.fit(new_train_x, new_train_label)\n","# print(\"best\", svc.best_score_)\n","test_predict = svc.predict(test_x)\n","print(\"fpr and fnr\")\n","# print(fpr_onegroup(new_labels.tolist(), predict))\n","# print(fnr_onegroup(new_labels.tolist(), predict))\n","print(fpr_onegroup(list(test_label), test_predict))\n","print(fnr_onegroup(list(test_label), test_predict))\n","test_set['predicted'] = test_predict\n","s,s2,s3 = div_results(\"German Credit\", \"Preferential Sampling-Leaf\",\"SVM\")"]},{"cell_type":"markdown","metadata":{"id":"vM1zX7q-ruBj"},"source":["## Run Algorithm Top"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mzp7XkuksXyD"},"outputs":[],"source":["\n","#get all of the candidate groups possible with the combos and names\n","\n","new_train_data = copy.deepcopy(train_set)\n","all_names_lst_top = find_top(all_names)\n","#iterate over all the names to get the temp2 df for each name\n","for a in all_names_lst_top:\n","# for a in [all_names_lst[0]]: # leaf\n","# for a in [all_names_lst[-1:x]]: # top\n","  print(\"/////////////\")\n","  print(a)\n","  temp2, names = get_temp(new_train_data, all_names[a], compas_y)\n","  temp, temp_g = get_temp_g(new_train_data, names, compas_y)\n","  temp_g = temp_g[temp_g['cnt'] > filter_count]\n","  lst_of_counts = compute_lst_of_counts(temp, names, compas_y)\n","  start = time.time()\n","  need_pos, need_neg = compute_problematic_opt(temp2, temp_g, names, compas_y, lst_of_counts)\n","  end = time.time()\n","  excute_time = end - start\n","  # print(\"The time to compute unfair group is {}\".format(str(excute_time)))\n","  print(\"The sets of need pos and neg are\")\n","  print(need_pos)\n","  print(need_neg)\n","  new_train_data['skewed'] = 0\n","  new_train_data[\"diff\"] = 0\n","  print(\"started pref sampling\")\n","  new_train_data = pref_sampling_opt(new_train_data, names, compas_y, need_pos, need_neg)\n","  # print(new_train_data)\n","  print(new_train_data[compas_y].value_counts())\n","new_train_x = pd.DataFrame(new_train_data, columns = columns_all)\n","new_train_label = pd.DataFrame(new_train_data, columns = [compas_y])\n","new_train_label = new_train_label[compas_y]\n","new_train_label = new_train_label.astype('int')\n","grid_new = GridSearchCV(tree.DecisionTreeClassifier(), param_grid = param, cv = 6)\n","grid_new.fit(new_train_x, new_train_label)\n","print(\"model accuracy\")\n","print(\"best\", grid_new.best_score_)\n","test_predict = grid_new.predict(test_x)\n","print(\"fpr and fnr\")\n","# print(fpr_onegroup(new_labels.tolist(), predict))\n","# print(fnr_onegroup(new_labels.tolist(), predict))\n","print(fpr_onegroup(list(test_label), test_predict))\n","print(fnr_onegroup(list(test_label), test_predict))\n","# print(pref_sampling_mod(train_x, train_set, train_label, names, compas_y, need_pos, need_neg))"]},{"cell_type":"markdown","metadata":{"id":"kVRTFfXmr2NG"},"source":["### Results Top"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cwVjX5PNsj8s"},"outputs":[],"source":["test_set['predicted'] = test_predict\n","d, d2, d3 = div_results(\"German Credit\", \"Preferential Sampling-Top\",\"DT\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7cjoGWE-sFf9"},"outputs":[],"source":["# d,d1,d2 = div_results()\n","print()\n","print(\"rf\")\n","gridrf.fit(new_train_x, new_train_label)\n","print(\"best\", gridrf.best_score_)\n","test_predict = gridrf.predict(test_x)\n","print(\"fpr and fnr\")\n","# print(fpr_onegroup(new_labels.tolist(), predict))\n","# print(fnr_onegroup(new_labels.tolist(), predict))\n","print(fpr_onegroup(list(test_label), test_predict))\n","print(fnr_onegroup(list(test_label), test_predict))\n","test_set['predicted'] = test_predict\n","r,r2,r3 = div_results(\"German Credit\", \"Preferential Sampling-Top\",\"RF\")\n","\n","print()\n","print(\"logistic\")\n","gridl.fit(new_train_x, new_train_label)\n","print(\"best\", gridl.best_score_)\n","test_predict = gridl.predict(test_x)\n","print(\"fpr and fnr\")\n","# print(fpr_onegroup(new_labels.tolist(), predict))\n","# print(fnr_onegroup(new_labels.tolist(), predict))\n","print(fpr_onegroup(list(test_label), test_predict))\n","print(fnr_onegroup(list(test_label), test_predict))\n","test_set['predicted'] = test_predict\n","r,r2,r3 = div_results(\"German Credit\", \"Preferential Sampling-Top\",\"L\")\n","\n","print()\n","print(\"svm\")\n","svc.fit(new_train_x, new_train_label)\n","# print(\"best\", svc.best_score_)\n","test_predict = svc.predict(test_x)\n","print(\"fpr and fnr\")\n","# print(fpr_onegroup(new_labels.tolist(), predict))\n","# print(fnr_onegroup(new_labels.tolist(), predict))\n","print(fpr_onegroup(list(test_label), test_predict))\n","print(fnr_onegroup(list(test_label), test_predict))\n","test_set['predicted'] = test_predict\n","s,s2,s3 = div_results(\"German Credit\", \"Preferential Sampling-Top\",\"SVM\")"]},{"cell_type":"markdown","metadata":{"id":"JeNaCEQ26n7V"},"source":["# Duplication"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F-x05Dlbuc62"},"outputs":[],"source":["def round_int(x):\n","    if x in [float(\"-inf\"),float(\"inf\")]: return 0\n","    return int(round(x))\n","\n","\n","def make_duplicate(d, group_lst, diff, label_y, names, need_positive_or_negative):\n","    #print(\"make samples\")\n","    selected = copy.deepcopy(d)\n","    print(\"names \", names, group_lst)\n","    for i in range(len(group_lst)):\n","        att_name = names[i]\n","        selected = selected[(selected[att_name] == group_lst[i])]\n","    selected = selected[(selected[label_y] == need_positive_or_negative)]\n","    # print(\"=============\")\n","    # print(group_lst, names)\n","    # print(selected)\n","    if len(selected) == 0:\n","        return pd.DataFrame()\n","    # print(len(temp))\n","    # randomly generated diff samples:\n","    # print(len(selected), diff)\n","    while(len(selected) < diff):\n","        # duplicate the dataframe\n","        select_copy = selected.copy(deep=True)\n","        selected = pd.concat([selected, select_copy])\n","        # print(len(selected))\n","        #generated = temp\n","        # the number needed is more than the not needed numbers.\n","    #print(len(temp))\n","    generated = selected.sample(n = diff, replace = False, axis = 0)\n","    # print(generated)\n","    return generated\n","\n","\n","def naive_duplicate(d, temp2, names, need_pos, need_neg, label_y):\n","    # add more records for all groups\n","    # The smote algorithm to boost the coverage\n","    for r in need_pos:\n","        # print(\"adding more positive\")\n","    # add more positive records\n","        # determine how many points to add\n","        print(\"pos_vals\", r)\n","        diff = compute_diff_add(r, temp2, names, label_y, 1)\n","        diff = round_int(diff)\n","        # add more records\n","        print(\"Adding \" + str(diff) +\" positive records\")\n","        samples_to_add = make_duplicate(d, r, diff, label_y, names, need_positive_or_negative = 1)\n","        d = pd.concat([d, samples_to_add], ignore_index=True)\n","    for k in need_neg:\n","        print(\"neg_vals\", k)\n","        # print(\"adding more negative\")\n","        diff = compute_diff_add(k, temp2, names, label_y, need_positive_or_negative = 0)\n","        diff = round_int(diff)\n","        print(\"Adding \" + str(diff) +\" negative records\")\n","        samples_to_add = make_duplicate(d, k, diff, label_y, names, need_positive_or_negative = 0)\n","        d = pd.concat([d, samples_to_add], ignore_index=True)\n","    return d"]},{"cell_type":"markdown","metadata":{"id":"H_HqjDkKtclY"},"source":["## Run Algorithm Lattice"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ptgbqFIO8PDF"},"outputs":[],"source":["#get all of the candidate groups possible with the combos and names\n","\n","new_train_data = copy.deepcopy(train_set)\n","\n","#iterate over all the names to get the temp2 df for each name\n","for a in all_names_lst:\n","  print(\"?????/////\")\n","  print(a)\n","  temp2, names = get_temp(new_train_data, all_names[a], compas_y)\n","  temp, temp_g = get_temp_g(new_train_data, names, compas_y)\n","  temp_g = temp_g[temp_g['cnt'] > filter_count]\n","  lst_of_counts = compute_lst_of_counts(temp, names, compas_y)\n","  # start = time.time()\n","  need_pos, need_neg = compute_problematic_opt(temp2, temp_g, names, compas_y, lst_of_counts)\n","  # end = time.time()\n","  # excute_time = end - start\n","  # print(\"The time to compute unfair group is {}\".format(str(excute_time)))\n","  print(\"The sets of need pos and neg are\")\n","  print(need_pos)\n","  print(need_neg)\n","  new_train_data['skewed'] = 0\n","  new_train_data[\"diff\"] = 0\n","  print(\"started duplication\")\n","  new_train_data = naive_duplicate(new_train_data, temp2, names, need_pos, need_neg, compas_y)\n","  # new_train_x = pd.DataFrame(new_train_data, columns = columns_compas)\n","  # new_train_label = pd.DataFrame(new_train_data, columns = [compas_y])\n","  # new_train_label = new_train_label[compas_y]\n","  # new_train_label = new_train_label.astype('int')\n","  # grid_new = DecisionTreeClassifier(criterion=\"entropy\",max_depth=6, random_state=17)\n","  # grid_new.fit(new_train_x, new_train_label)\n","  # test_predict = grid_new.predict(test_x)\n","  # print(\"new trainset length\", len(new_train_x))\n","  # print(\"fpr and fnr\")\n","  # print(fpr_onegroup(list(test_label), test_predict))\n","  # print(fnr_onegroup(list(test_label), test_predict))\n","  print(compas_y, new_train_data[compas_y].value_counts())\n","new_train_x = pd.DataFrame(new_train_data, columns = columns_all)\n","new_train_label = pd.DataFrame(new_train_data, columns = [compas_y])\n","new_train_label = new_train_label[compas_y]\n","new_train_label = new_train_label.astype('int')\n","grid_new = GridSearchCV(tree.DecisionTreeClassifier(), param_grid = param, cv = 6)\n","grid_new.fit(new_train_x, new_train_label)\n","print(\"model accuracy\")\n","print(\"best\", grid_new.best_score_)\n","test_predict = grid_new.predict(test_x)\n","print(\"fpr and fnr\")\n","# print(fpr_onegroup(new_labels.tolist(), predict))\n","# print(fnr_onegroup(new_labels.tolist(), predict))\n","print(fpr_onegroup(list(test_label), test_predict))\n","print(fnr_onegroup(list(test_label), test_predict))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wid4ViElCw6i"},"outputs":[],"source":["test_set['predicted'] = test_predict"]},{"cell_type":"markdown","metadata":{"id":"QuQ3lc7aU9NG"},"source":["### Results Lattice\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uVcvdRuMQu26"},"outputs":[],"source":["# # run divexplorer to find unfair groups\n","# class_map={'N': 0, 'P': 1}\n","# from divexplorer.FP_DivergenceExplorer import FP_DivergenceExplorer\n","\n","# #min_sup=0.1\n","\n","# min_sup = 0.05\n","\n","# fp_diver=FP_DivergenceExplorer(test_set,\"class\", \"predicted\", class_map=class_map)\n","# FP_fm=fp_diver.getFrequentPatternDivergence(min_support=min_sup, metrics=[\"d_fpr\", \"d_fnr\", \"d_accuracy\"])\n","# from divexplorer.FP_Divergence import FP_Divergence\n","# fp_divergence_fpr=FP_Divergence(FP_fm, \"d_fpr\")\n","# # fp_divergence_fpr=FP_Divergence(FP_fm, \"d_fnr\")\n","# INFO_VIZ=[\"support\", \"itemsets\",  fp_divergence_fpr.metric, fp_divergence_fpr.t_value_col]\n","\n","# K=10\n","# pd.options.display.max_rows = 200\n","# d = fp_divergence_fpr.getDivergence(th_redundancy=0)[INFO_VIZ].head(K)\n","# # summerization\n","# eps=0.01\n","\n","# d = fp_divergence_fpr.getDivergence(th_redundancy=eps)[INFO_VIZ].head(K)\n","\n","# d\n","d, d2, d3 = div_results(\"Bar\", \"Duplication-Lattice\",\"DT\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VTqA6ic3sHXO"},"outputs":[],"source":["# d,d1,d2 = div_results()\n","print()\n","print(\"rf\")\n","gridrf.fit(new_train_x, new_train_label)\n","print(\"best\", gridrf.best_score_)\n","test_predict = gridrf.predict(test_x)\n","print(\"fpr and fnr\")\n","# print(fpr_onegroup(new_labels.tolist(), predict))\n","# print(fnr_onegroup(new_labels.tolist(), predict))\n","print(fpr_onegroup(list(test_label), test_predict))\n","print(fnr_onegroup(list(test_label), test_predict))\n","test_set['predicted'] = test_predict\n","r,r2,r3 = div_results(\"Bar\", \"Duplication-Lattice\",\"RF\")\n","\n","print()\n","print(\"logistic\")\n","gridl.fit(new_train_x, new_train_label)\n","print(\"best\", gridl.best_score_)\n","test_predict = gridl.predict(test_x)\n","print(\"fpr and fnr\")\n","# print(fpr_onegroup(new_labels.tolist(), predict))\n","# print(fnr_onegroup(new_labels.tolist(), predict))\n","print(fpr_onegroup(list(test_label), test_predict))\n","print(fnr_onegroup(list(test_label), test_predict))\n","test_set['predicted'] = test_predict\n","r,r2,r3 = div_results(\"Bar\", \"Duplication-Lattice\",\"L\")\n","\n","print()\n","print(\"svm\")\n","svc.fit(new_train_x, new_train_label)\n","# print(\"best\", svc.best_score_)\n","test_predict = svc.predict(test_x)\n","print(\"fpr and fnr\")\n","# print(fpr_onegroup(new_labels.tolist(), predict))\n","# print(fnr_onegroup(new_labels.tolist(), predict))\n","print(fpr_onegroup(list(test_label), test_predict))\n","print(fnr_onegroup(list(test_label), test_predict))\n","test_set['predicted'] = test_predict\n","s,s2,s3 = div_results(\"Bar\", \"Duplication-Lattice\",\"SVM\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7whA1nLnvev0"},"outputs":[],"source":["def fairness_score_computation(d, metrics):\n","    sum_of_score = 0\n","    for idx, row in d.iterrows():\n","      sum_of_score += row['support'] * row[metrics]\n","    return sum_of_score\n","\n","print(fairness_score_computation(d, 'd_fpr'))\n"]},{"cell_type":"markdown","metadata":{"id":"cJ4Kag3MuFuf"},"source":["## Run Algorithm Leaf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XWAzykiouMXt"},"outputs":[],"source":["#get all of the candidate groups possible with the combos and names\n","\n","new_train_data = copy.deepcopy(train_set)\n","\n","#iterate over all the names to get the temp2 df for each name\n","for a in [all_names_lst[0]]:\n","  print(\"?????/////\")\n","  print(a)\n","  temp2, names = get_temp(new_train_data, all_names[a], compas_y)\n","  temp, temp_g = get_temp_g(new_train_data, names, compas_y)\n","  temp_g = temp_g[temp_g['cnt'] > filter_count]\n","  lst_of_counts = compute_lst_of_counts(temp, names, compas_y)\n","  # start = time.time()\n","  need_pos, need_neg = compute_problematic_opt(temp2, temp_g, names, compas_y, lst_of_counts)\n","  # end = time.time()\n","  # excute_time = end - start\n","  # print(\"The time to compute unfair group is {}\".format(str(excute_time)))\n","  print(\"The sets of need pos and neg are\")\n","  print(need_pos)\n","  print(need_neg)\n","  new_train_data['skewed'] = 0\n","  new_train_data[\"diff\"] = 0\n","  print(\"started duplication\")\n","  new_train_data = naive_duplicate(new_train_data, temp2, names, need_pos, need_neg, compas_y)\n","  # new_train_x = pd.DataFrame(new_train_data, columns = columns_compas)\n","  # new_train_label = pd.DataFrame(new_train_data, columns = [compas_y])\n","  # new_train_label = new_train_label[compas_y]\n","  # new_train_label = new_train_label.astype('int')\n","  # grid_new = DecisionTreeClassifier(criterion=\"entropy\",max_depth=6, random_state=17)\n","  # grid_new.fit(new_train_x, new_train_label)\n","  # test_predict = grid_new.predict(test_x)\n","  # print(\"new trainset length\", len(new_train_x))\n","  # print(\"fpr and fnr\")\n","  # print(fpr_onegroup(list(test_label), test_predict))\n","  # print(fnr_onegroup(list(test_label), test_predict))\n","  print(compas_y, new_train_data[compas_y].value_counts())\n","new_train_x = pd.DataFrame(new_train_data, columns = columns_all)\n","new_train_label = pd.DataFrame(new_train_data, columns = [compas_y])\n","new_train_label = new_train_label[compas_y]\n","new_train_label = new_train_label.astype('int')\n","grid_new = GridSearchCV(tree.DecisionTreeClassifier(), param_grid = param, cv = 6)\n","grid_new.fit(new_train_x, new_train_label)\n","print(\"model accuracy\")\n","print(\"best\", grid_new.best_score_)\n","test_predict = grid_new.predict(test_x)\n","print(\"fpr and fnr\")\n","# print(fpr_onegroup(new_labels.tolist(), predict))\n","# print(fnr_onegroup(new_labels.tolist(), predict))\n","print(fpr_onegroup(list(test_label), test_predict))\n","print(fnr_onegroup(list(test_label), test_predict))"]},{"cell_type":"markdown","metadata":{"id":"MyiaC-BPuT-N"},"source":["### Results Leaf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tIpq68mRuWVd"},"outputs":[],"source":["test_set['predicted'] = test_predict\n","d,d2,d3 = div_results(\"Bar\", \"Duplication-Leaf\",\"DT\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FVCnBjPxsJgg"},"outputs":[],"source":["# d,d1,d2 = div_results()\n","print()\n","print(\"rf\")\n","gridrf.fit(new_train_x, new_train_label)\n","print(\"best\", gridrf.best_score_)\n","test_predict = gridrf.predict(test_x)\n","print(\"fpr and fnr\")\n","# print(fpr_onegroup(new_labels.tolist(), predict))\n","# print(fnr_onegroup(new_labels.tolist(), predict))\n","print(fpr_onegroup(list(test_label), test_predict))\n","print(fnr_onegroup(list(test_label), test_predict))\n","test_set['predicted'] = test_predict\n","r,r2,r3 = div_results(\"Bar\", \"Duplication-Leaf\",\"RF\")\n","\n","print()\n","print(\"logistic\")\n","gridl.fit(new_train_x, new_train_label)\n","print(\"best\", gridl.best_score_)\n","test_predict = gridl.predict(test_x)\n","print(\"fpr and fnr\")\n","# print(fpr_onegroup(new_labels.tolist(), predict))\n","# print(fnr_onegroup(new_labels.tolist(), predict))\n","print(fpr_onegroup(list(test_label), test_predict))\n","print(fnr_onegroup(list(test_label), test_predict))\n","test_set['predicted'] = test_predict\n","r,r2,r3 = div_results(\"Bar\", \"Duplication-Leaf\",\"L\")\n","\n","print()\n","print(\"svm\")\n","svc.fit(new_train_x, new_train_label)\n","# print(\"best\", svc.best_score_)\n","test_predict = svc.predict(test_x)\n","print(\"fpr and fnr\")\n","# print(fpr_onegroup(new_labels.tolist(), predict))\n","# print(fnr_onegroup(new_labels.tolist(), predict))\n","print(fpr_onegroup(list(test_label), test_predict))\n","print(fnr_onegroup(list(test_label), test_predict))\n","test_set['predicted'] = test_predict\n","s,s2,s3 = div_results(\"Bar\", \"Duplication-Leaf\",\"SVM\")"]},{"cell_type":"markdown","metadata":{"id":"7Ud6LiPvubLz"},"source":["## Run Algorithm Top"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"itgAs3JCuj6B"},"outputs":[],"source":["#get all of the candidate groups possible with the combos and names\n","\n","new_train_data = copy.deepcopy(train_set)\n","\n","#iterate over all the names to get the temp2 df for each name\n","all_names_lst_top = find_top(all_names)\n","for a in all_names_lst_top:\n","  print(\"?????/////\")\n","  print(a)\n","  temp2, names = get_temp(new_train_data, all_names[a], compas_y)\n","  temp, temp_g = get_temp_g(new_train_data, names, compas_y)\n","  temp_g = temp_g[temp_g['cnt'] > filter_count]\n","  lst_of_counts = compute_lst_of_counts(temp, names, compas_y)\n","  # start = time.time()\n","  need_pos, need_neg = compute_problematic_opt(temp2, temp_g, names, compas_y, lst_of_counts)\n","  # end = time.time()\n","  # excute_time = end - start\n","  # print(\"The time to compute unfair group is {}\".format(str(excute_time)))\n","  print(\"The sets of need pos and neg are\")\n","  print(need_pos)\n","  print(need_neg)\n","  new_train_data['skewed'] = 0\n","  new_train_data[\"diff\"] = 0\n","  print(\"started duplication\")\n","  new_train_data = naive_duplicate(new_train_data, temp2, names, need_pos, need_neg, compas_y)\n","  # new_train_x = pd.DataFrame(new_train_data, columns = columns_compas)\n","  # new_train_label = pd.DataFrame(new_train_data, columns = [compas_y])\n","  # new_train_label = new_train_label[compas_y]\n","  # new_train_label = new_train_label.astype('int')\n","  # grid_new = DecisionTreeClassifier(criterion=\"entropy\",max_depth=6, random_state=17)\n","  # grid_new.fit(new_train_x, new_train_label)\n","  # test_predict = grid_new.predict(test_x)\n","  # print(\"new trainset length\", len(new_train_x))\n","  # print(\"fpr and fnr\")\n","  # print(fpr_onegroup(list(test_label), test_predict))\n","  # print(fnr_onegroup(list(test_label), test_predict))\n","  print(compas_y, new_train_data[compas_y].value_counts())\n","new_train_x = pd.DataFrame(new_train_data, columns = columns_all)\n","new_train_label = pd.DataFrame(new_train_data, columns = [compas_y])\n","new_train_label = new_train_label[compas_y]\n","new_train_label = new_train_label.astype('int')\n","grid_new = GridSearchCV(tree.DecisionTreeClassifier(), param_grid = param, cv = 6)\n","grid_new.fit(new_train_x, new_train_label)\n","print(\"model accuracy\")\n","print(\"best\", grid_new.best_score_)\n","test_predict = grid_new.predict(test_x)\n","print(\"fpr and fnr\")\n","# print(fpr_onegroup(new_labels.tolist(), predict))\n","# print(fnr_onegroup(new_labels.tolist(), predict))\n","print(fpr_onegroup(list(test_label), test_predict))\n","print(fnr_onegroup(list(test_label), test_predict))"]},{"cell_type":"markdown","metadata":{"id":"WtQcykniuqUC"},"source":["### Results Top"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rXyGMq6Fuunb"},"outputs":[],"source":["test_set['predicted'] = test_predict\n","d,d2,d3 = div_results(\"Bar\", \"Duplication-Top\",\"DT\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-PXEjrEDsOEl"},"outputs":[],"source":["# d,d1,d2 = div_results()\n","print()\n","print(\"rf\")\n","gridrf.fit(new_train_x, new_train_label)\n","print(\"best\", gridrf.best_score_)\n","test_predict = gridrf.predict(test_x)\n","print(\"fpr and fnr\")\n","# print(fpr_onegroup(new_labels.tolist(), predict))\n","# print(fnr_onegroup(new_labels.tolist(), predict))\n","print(fpr_onegroup(list(test_label), test_predict))\n","print(fnr_onegroup(list(test_label), test_predict))\n","test_set['predicted'] = test_predict\n","r,r2,r3 = div_results(\"Bar\", \"Duplication-Top\",\"RF\")\n","\n","print()\n","print(\"logistic\")\n","gridl.fit(new_train_x, new_train_label)\n","print(\"best\", gridl.best_score_)\n","test_predict = gridl.predict(test_x)\n","print(\"fpr and fnr\")\n","# print(fpr_onegroup(new_labels.tolist(), predict))\n","# print(fnr_onegroup(new_labels.tolist(), predict))\n","print(fpr_onegroup(list(test_label), test_predict))\n","print(fnr_onegroup(list(test_label), test_predict))\n","test_set['predicted'] = test_predict\n","r,r2,r3 = div_results(\"Bar\", \"Duplication-Top\",\"L\")\n","\n","print()\n","print(\"svm\")\n","svc.fit(new_train_x, new_train_label)\n","# print(\"best\", svc.best_score_)\n","test_predict = svc.predict(test_x)\n","print(\"fpr and fnr\")\n","# print(fpr_onegroup(new_labels.tolist(), predict))\n","# print(fnr_onegroup(new_labels.tolist(), predict))\n","print(fpr_onegroup(list(test_label), test_predict))\n","print(fnr_onegroup(list(test_label), test_predict))\n","test_set['predicted'] = test_predict\n","s,s2,s3 = div_results(\"Bar\", \"Duplication-Top\",\"SVM\")"]},{"cell_type":"markdown","metadata":{"id":"2SfzogxQ-Q5d"},"source":["#Down-sampling"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h9m0e5GbVL-d"},"outputs":[],"source":["def round_int(x):\n","    if x in [float(\"-inf\"),float(\"inf\")]: return 0\n","    return int(round(x))\n","\n","\n","def make_remove(d, group_lst, diff, names, label_y, need_positive_or_negative):\n","    #print(\"make samples\")\n","    temp = copy.deepcopy(d)\n","    for i in range(len(group_lst)):\n","        att_name = names[i]\n","        temp = temp[(temp[att_name] == group_lst[i])]\n","    temp = temp[(temp[label_y] == need_positive_or_negative)]\n","    # randomly generated diff samples\n","        #generated = temp\n","        # the number needed is more than the not needed numbers.\n","    # print(len(temp))\n","    # print(diff)\n","    if(diff>len(temp)):\n","        diff = len(temp)\n","    generated = temp.sample(n = diff, replace = False, axis = 0)\n","    #print(generated.index)\n","    return generated.index\n","\n","\n","def naive_downsampling(d, temp2, names, need_pos, need_neg, label_y):\n","    # add more records for all groups\n","    # The smote algorithm to boost the coverage\n","    for r in need_pos:\n","        print(\"removing more negative\")\n","    # add more positive records\n","        # determine how many points to add\n","        print(r)\n","        diff = compute_diff_remove(r, temp2, names, label_y, need_positive_or_negative = 1)\n","        diff = round_int(diff)\n","        if diff == -1:\n","          continue\n","        # add more records\n","        print(\"Removed \" + str(diff) +\" negative records\")\n","        samples_to_remove = make_remove(d, r, diff, names, label_y, need_positive_or_negative = 0)\n","        d.drop(index  = samples_to_remove, inplace = True)\n","        # print(len(d))\n","    for k in need_neg:\n","        print(k)\n","        diff = compute_diff_remove(k, temp2, names, label_y, need_positive_or_negative = 0)\n","        diff = round_int(diff)\n","        if diff == -1:\n","          continue\n","        print(\"Removed \" + str(diff) +\" positive records\")\n","        samples_to_remove = make_remove(d, k, diff, names, label_y, need_positive_or_negative = 1)\n","        d.drop(index  = samples_to_remove, inplace = True)\n","    return d"]},{"cell_type":"markdown","metadata":{"id":"DcUYj8wJwrHK"},"source":["## Run Algorithm Lattice"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QTfYbyt4pcRq"},"outputs":[],"source":["#get all of the candidate groups possible with the combos and names\n","\n","new_train_data = copy.deepcopy(train_set)\n","\n","#iterate over all the names to get the temp2 df for each name\n","for a in all_names_lst:\n","  print(\"?????/////\")\n","  print(a)\n","  temp2, names = get_temp(new_train_data, all_names[a], compas_y)\n","  temp, temp_g = get_temp_g(new_train_data, names, compas_y)\n","  temp_g = temp_g[temp_g['cnt'] > filter_count]\n","  # print(temp_g)\n","  # print(temp2, names)\n","  # print(temp)\n","  lst_of_counts = compute_lst_of_counts(temp, names, compas_y)\n","  start = time.time()\n","  need_pos, need_neg = compute_problematic_opt(temp2, temp_g, names, compas_y, lst_of_counts)\n","  end = time.time()\n","  excute_time = end - start\n","  print(\"The time to compute unfair group is {}\".format(str(excute_time)))\n","  print(\"The sets of need pos and neg are\")\n","  print(need_pos)\n","  print(need_neg)\n","  new_train_data['skewed'] = 0\n","  new_train_data[\"diff\"] = 0\n","  # print(\"started duplication\")\n","  new_train_data = naive_downsampling(new_train_data, temp2, names, need_pos, need_neg, compas_y)\n","  print(new_train_data[compas_y].value_counts())\n","new_train_x = pd.DataFrame(new_train_data, columns = columns_all)\n","new_train_label = pd.DataFrame(new_train_data, columns = [compas_y])\n","new_train_label = new_train_label[compas_y]\n","new_train_label = new_train_label.astype('int')\n","grid_new = GridSearchCV(tree.DecisionTreeClassifier(), param_grid = param, cv = 6)\n","grid_new.fit(new_train_x, new_train_label)\n","print(\"model accuracy\")\n","print(\"best\", grid_new.best_score_)\n","test_predict = grid_new.predict(test_x)\n","print(\"fpr and fnr\")\n","# print(fpr_onegroup(new_labels.tolist(), predict))\n","# print(fnr_onegroup(new_labels.tolist(), predict))\n","print(fpr_onegroup(list(test_label), test_predict))\n","print(fnr_onegroup(list(test_label), test_predict))\n","# new_train_x = pd.DataFrame(new_train_data, columns = columns_compas)\n","# new_train_label = pd.DataFrame(new_train_data, columns = [compas_y])\n","# new_train_label = new_train_label[compas_y]\n","# new_train_label = new_train_label.astype('int')\n","# grid_new = DecisionTreeClassifier(criterion=\"entropy\",max_depth=6, random_state=17)\n","# grid_new.fit(new_train_x, new_train_label)\n","# test_predict = grid_new.predict(test_x)\n","# print(\"new trainset length\", len(new_train_x))\n","# print(\"fpr\", fpr_onegroup(list(test_label), test_predict))\n","# print(\"fnr\", fnr_onegroup(list(test_label), test_predict))"]},{"cell_type":"markdown","metadata":{"id":"xhS42vuGsI2q"},"source":["### Results Lattice"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"El4mFbYwr-4v"},"outputs":[],"source":["test_set['predicted'] = test_predict\n","d,d2,d3 = div_results(\"Bar\", \"Downsampling-Lattice\",\"DT\")\n","\n","# # run divexplorer to find unfair groups\n","# class_map={'N': 0, 'P': 1}\n","# from divexplorer.FP_DivergenceExplorer import FP_DivergenceExplorer\n","\n","# #min_sup=0.1\n","\n","# min_sup = 0.05\n","\n","# fp_diver=FP_DivergenceExplorer(test_set,\"class\", \"predicted\", class_map=class_map)\n","# FP_fm=fp_diver.getFrequentPatternDivergence(min_support=min_sup, metrics=[\"d_fpr\", \"d_fnr\", \"d_accuracy\"])\n","# from divexplorer.FP_Divergence import FP_Divergence\n","# fp_divergence_fpr=FP_Divergence(FP_fm, \"d_fpr\")\n","# # fp_divergence_fpr=FP_Divergence(FP_fm, \"d_fnr\")\n","# INFO_VIZ=[\"support\", \"itemsets\",  fp_divergence_fpr.metric, fp_divergence_fpr.t_value_col]\n","\n","# K=10\n","# pd.options.display.max_rows = 200\n","# d = fp_divergence_fpr.getDivergence(th_redundancy=0)[INFO_VIZ].head(K)\n","# # summerization\n","# eps=0.01\n","\n","# d = fp_divergence_fpr.getDivergence(th_redundancy=eps)[INFO_VIZ].head(K)\n","\n","# d\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dbzCpafMsVko"},"outputs":[],"source":["# d,d1,d2 = div_results()\n","print()\n","print(\"rf\")\n","gridrf.fit(new_train_x, new_train_label)\n","print(\"best\", gridrf.best_score_)\n","test_predict = gridrf.predict(test_x)\n","print(\"fpr and fnr\")\n","# print(fpr_onegroup(new_labels.tolist(), predict))\n","# print(fnr_onegroup(new_labels.tolist(), predict))\n","print(fpr_onegroup(list(test_label), test_predict))\n","print(fnr_onegroup(list(test_label), test_predict))\n","test_set['predicted'] = test_predict\n","r,r2,r3 = div_results(\"Bar\", \"Downsampling-Lattice\",\"RF\")\n","\n","print()\n","print(\"logistic\")\n","gridl.fit(new_train_x, new_train_label)\n","print(\"best\", gridl.best_score_)\n","test_predict = gridl.predict(test_x)\n","print(\"fpr and fnr\")\n","# print(fpr_onegroup(new_labels.tolist(), predict))\n","# print(fnr_onegroup(new_labels.tolist(), predict))\n","print(fpr_onegroup(list(test_label), test_predict))\n","print(fnr_onegroup(list(test_label), test_predict))\n","test_set['predicted'] = test_predict\n","r,r2,r3 = div_results(\"Bar\", \"Downsampling-Lattice\",\"L\")\n","\n","print()\n","print(\"svm\")\n","svc.fit(new_train_x, new_train_label)\n","# print(\"best\", svc.best_score_)\n","test_predict = svc.predict(test_x)\n","print(\"fpr and fnr\")\n","# print(fpr_onegroup(new_labels.tolist(), predict))\n","# print(fnr_onegroup(new_labels.tolist(), predict))\n","print(fpr_onegroup(list(test_label), test_predict))\n","print(fnr_onegroup(list(test_label), test_predict))\n","test_set['predicted'] = test_predict\n","s,s2,s3 = div_results(\"Bar\", \"Downsampling-Lattice\",\"SVM\")"]},{"cell_type":"markdown","metadata":{"id":"YqfnIjDrwk9i"},"source":["## Run Algorithm Leaf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IvmlKdUqwl_1"},"outputs":[],"source":["#get all of the candidate groups possible with the combos and names\n","\n","new_train_data = copy.deepcopy(train_set)\n","\n","#iterate over all the names to get the temp2 df for each name\n","for a in [all_names_lst[0]]:\n","  print(\"?????/////\")\n","  print(a)\n","  temp2, names = get_temp(new_train_data, all_names[a], compas_y)\n","  temp, temp_g = get_temp_g(new_train_data, names, compas_y)\n","  temp_g = temp_g[temp_g['cnt'] > filter_count]\n","  # print(temp_g)\n","  # print(temp2, names)\n","  # print(temp)\n","  lst_of_counts = compute_lst_of_counts(temp, names, compas_y)\n","  start = time.time()\n","  need_pos, need_neg = compute_problematic_opt(temp2, temp_g, names, compas_y, lst_of_counts)\n","  end = time.time()\n","  excute_time = end - start\n","  print(\"The time to compute unfair group is {}\".format(str(excute_time)))\n","  print(\"The sets of need pos and neg are\")\n","  print(need_pos)\n","  print(need_neg)\n","  new_train_data['skewed'] = 0\n","  new_train_data[\"diff\"] = 0\n","  # print(\"started duplication\")\n","  new_train_data = naive_downsampling(new_train_data, temp2, names, need_pos, need_neg, compas_y)\n","  print(new_train_data[compas_y].value_counts())\n","new_train_x = pd.DataFrame(new_train_data, columns = columns_all)\n","new_train_label = pd.DataFrame(new_train_data, columns = [compas_y])\n","new_train_label = new_train_label[compas_y]\n","new_train_label = new_train_label.astype('int')\n","grid_new = GridSearchCV(tree.DecisionTreeClassifier(), param_grid = param, cv = 6)\n","grid_new.fit(new_train_x, new_train_label)\n","print(\"model accuracy\")\n","print(\"best\", grid_new.best_score_)\n","test_predict = grid_new.predict(test_x)\n","print(\"fpr and fnr\")\n","# print(fpr_onegroup(new_labels.tolist(), predict))\n","# print(fnr_onegroup(new_labels.tolist(), predict))\n","print(fpr_onegroup(list(test_label), test_predict))\n","print(fnr_onegroup(list(test_label), test_predict))\n","# new_train_x = pd.DataFrame(new_train_data, columns = columns_compas)\n","# new_train_label = pd.DataFrame(new_train_data, columns = [compas_y])\n","# new_train_label = new_train_label[compas_y]\n","# new_train_label = new_train_label.astype('int')\n","# grid_new = DecisionTreeClassifier(criterion=\"entropy\",max_depth=6, random_state=17)\n","# grid_new.fit(new_train_x, new_train_label)\n","# test_predict = grid_new.predict(test_x)\n","# print(\"new trainset length\", len(new_train_x))\n","# print(\"fpr\", fpr_onegroup(list(test_label), test_predict))\n","# print(\"fnr\", fnr_onegroup(list(test_label), test_predict))"]},{"cell_type":"markdown","metadata":{"id":"5-O26rF9xYc-"},"source":["### Results Leaf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fsPaBOa7xYMK"},"outputs":[],"source":["test_set['predicted'] = test_predict\n","d,d2,d3 = div_results(\"Bar\", \"Downsampling-Leaf\",\"DT\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7HWsJpa-sgzZ"},"outputs":[],"source":["# d,d1,d2 = div_results()\n","print()\n","print(\"rf\")\n","gridrf.fit(new_train_x, new_train_label)\n","print(\"best\", gridrf.best_score_)\n","test_predict = gridrf.predict(test_x)\n","print(\"fpr and fnr\")\n","# print(fpr_onegroup(new_labels.tolist(), predict))\n","# print(fnr_onegroup(new_labels.tolist(), predict))\n","print(fpr_onegroup(list(test_label), test_predict))\n","print(fnr_onegroup(list(test_label), test_predict))\n","test_set['predicted'] = test_predict\n","r,r2,r3 = div_results(\"Bar\", \"Downsampling-Leaf\",\"RF\")\n","\n","print()\n","print(\"logistic\")\n","gridl.fit(new_train_x, new_train_label)\n","print(\"best\", gridl.best_score_)\n","test_predict = gridl.predict(test_x)\n","print(\"fpr and fnr\")\n","# print(fpr_onegroup(new_labels.tolist(), predict))\n","# print(fnr_onegroup(new_labels.tolist(), predict))\n","print(fpr_onegroup(list(test_label), test_predict))\n","print(fnr_onegroup(list(test_label), test_predict))\n","test_set['predicted'] = test_predict\n","r,r2,r3 = div_results(\"Bar\", \"Downsampling-Leaf\",\"L\")\n","\n","print()\n","print(\"svm\")\n","svc.fit(new_train_x, new_train_label)\n","# print(\"best\", svc.best_score_)\n","test_predict = svc.predict(test_x)\n","print(\"fpr and fnr\")\n","# print(fpr_onegroup(new_labels.tolist(), predict))\n","# print(fnr_onegroup(new_labels.tolist(), predict))\n","print(fpr_onegroup(list(test_label), test_predict))\n","print(fnr_onegroup(list(test_label), test_predict))\n","test_set['predicted'] = test_predict\n","s,s2,s3 = div_results(\"Bar\", \"Downsampling-Leaf\",\"SVM\")"]},{"cell_type":"markdown","metadata":{"id":"lEjc2orXxeJU"},"source":["## Run Algorithm Top"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g1ttSPisxj-R"},"outputs":[],"source":["#get all of the candidate groups possible with the combos and names\n","\n","new_train_data = copy.deepcopy(train_set)\n","\n","#iterate over all the names to get the temp2 df for each name\n","all_names_lst_top = find_top(all_names)\n","for a in all_names_lst_top:\n","  print(\"?????/////\")\n","  print(a)\n","  temp2, names = get_temp(new_train_data, all_names[a], compas_y)\n","  temp, temp_g = get_temp_g(new_train_data, names, compas_y)\n","  temp_g = temp_g[temp_g['cnt'] > filter_count]\n","  # print(temp_g)\n","  # print(temp2, names)\n","  # print(temp)\n","  lst_of_counts = compute_lst_of_counts(temp, names, compas_y)\n","  start = time.time()\n","  need_pos, need_neg = compute_problematic_opt(temp2, temp_g, names, compas_y, lst_of_counts)\n","  end = time.time()\n","  excute_time = end - start\n","  print(\"The time to compute unfair group is {}\".format(str(excute_time)))\n","  print(\"The sets of need pos and neg are\")\n","  print(need_pos)\n","  print(need_neg)\n","  new_train_data['skewed'] = 0\n","  new_train_data[\"diff\"] = 0\n","  # print(\"started duplication\")\n","  new_train_data = naive_downsampling(new_train_data, temp2, names, need_pos, need_neg, compas_y)\n","  print(new_train_data[compas_y].value_counts())\n","new_train_x = pd.DataFrame(new_train_data, columns = columns_all)\n","new_train_label = pd.DataFrame(new_train_data, columns = [compas_y])\n","new_train_label = new_train_label[compas_y]\n","new_train_label = new_train_label.astype('int')\n","grid_new = GridSearchCV(tree.DecisionTreeClassifier(), param_grid = param, cv = 6)\n","grid_new.fit(new_train_x, new_train_label)\n","print(\"model accuracy\")\n","print(\"best\", grid_new.best_score_)\n","test_predict = grid_new.predict(test_x)\n","print(\"fpr and fnr\")\n","# print(fpr_onegroup(new_labels.tolist(), predict))\n","# print(fnr_onegroup(new_labels.tolist(), predict))\n","print(fpr_onegroup(list(test_label), test_predict))\n","print(fnr_onegroup(list(test_label), test_predict))\n","# new_train_x = pd.DataFrame(new_train_data, columns = columns_compas)\n","# new_train_label = pd.DataFrame(new_train_data, columns = [compas_y])\n","# new_train_label = new_train_label[compas_y]\n","# new_train_label = new_train_label.astype('int')\n","# grid_new = DecisionTreeClassifier(criterion=\"entropy\",max_depth=6, random_state=17)\n","# grid_new.fit(new_train_x, new_train_label)\n","# test_predict = grid_new.predict(test_x)\n","# print(\"new trainset length\", len(new_train_x))\n","# print(\"fpr\", fpr_onegroup(list(test_label), test_predict))\n","# print(\"fnr\", fnr_onegroup(list(test_label), test_predict))"]},{"cell_type":"markdown","metadata":{"id":"YQOAJrGyxrsr"},"source":["### Results Top"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6mK6lI8exuxW"},"outputs":[],"source":["test_set['predicted'] = test_predict\n","d,d2,d3 = div_results(\"Bar\", \"Downsampling-Top\",\"DT\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WyrmV-7cshtA"},"outputs":[],"source":["# d,d1,d2 = div_results()\n","print()\n","print(\"rf\")\n","gridrf.fit(new_train_x, new_train_label)\n","print(\"best\", gridrf.best_score_)\n","test_predict = gridrf.predict(test_x)\n","print(\"fpr and fnr\")\n","# print(fpr_onegroup(new_labels.tolist(), predict))\n","# print(fnr_onegroup(new_labels.tolist(), predict))\n","print(fpr_onegroup(list(test_label), test_predict))\n","print(fnr_onegroup(list(test_label), test_predict))\n","test_set['predicted'] = test_predict\n","r,r2,r3 = div_results(\"Bar\", \"Downsampling-Top\",\"RF\")\n","\n","print()\n","print(\"logistic\")\n","gridl.fit(new_train_x, new_train_label)\n","print(\"best\", gridl.best_score_)\n","test_predict = gridl.predict(test_x)\n","print(\"fpr and fnr\")\n","# print(fpr_onegroup(new_labels.tolist(), predict))\n","# print(fnr_onegroup(new_labels.tolist(), predict))\n","print(fpr_onegroup(list(test_label), test_predict))\n","print(fnr_onegroup(list(test_label), test_predict))\n","test_set['predicted'] = test_predict\n","r,r2,r3 = div_results(\"Bar\", \"Downsampling-Top\",\"L\")\n","\n","print()\n","print(\"svm\")\n","svc.fit(new_train_x, new_train_label)\n","# print(\"best\", svc.best_score_)\n","test_predict = svc.predict(test_x)\n","print(\"fpr and fnr\")\n","# print(fpr_onegroup(new_labels.tolist(), predict))\n","# print(fnr_onegroup(new_labels.tolist(), predict))\n","print(fpr_onegroup(list(test_label), test_predict))\n","print(fnr_onegroup(list(test_label), test_predict))\n","test_set['predicted'] = test_predict\n","s,s2,s3 = div_results(\"Bar\", \"Downsampling-Top\",\"SVM\")"]},{"cell_type":"markdown","metadata":{"id":"YWbCzMkrnT-0"},"source":["# Massaging"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tINoeYnSnWn4"},"outputs":[],"source":["from sklearn.naive_bayes import MultinomialNB\n","def round_int(x):\n","    if x in [float(\"-inf\"),float(\"inf\")]: return 0\n","    return int(round(x))\n","\n","def get_depromotion(d, diff, group_lst, names, label_y, flag_depro):\n","    # double check\n","    print(\"start depromotion\")\n","    # print(len(d))\n","    print(names)\n","    # input_test = d.drop(columns = [label_y])\n","    # COMPAS COLUMNS = ]]\n","    # input_test = pd.DataFrame(train_set, columns = names)\n","    input_test = pd.DataFrame(d, columns = columns_compas)\n","    # should it be d instead?\n","    # print(group_lst, names)\n","    clf = MultinomialNB()\n","    # temp_train_label = pd.DataFrame(train_set, columns = [label_y])\n","    temp_train_label = pd.DataFrame(d, columns = [label_y])\n","    temp_train_label = temp_train_label[label_y]\n","    temp_train_label = temp_train_label.astype('int')\n","    clf = clf.fit(input_test, temp_train_label)\n","    prob  = clf.predict_proba(input_test)[:,0]\n","    select = copy.deepcopy(d)\n","    select['prob'] = prob # the higher the probablity is, the more likely for it to be 0\n","    # filter out those belongs to this group\n","    for i in range(len(group_lst)):\n","        att_name = names[i]\n","        select = select[(select[att_name] == group_lst[i])]\n","    select = select[(select[label_y] == flag_depro)]\n","    # rank them according to the probability\n","    # filp the records and remove the records from d\n","    if (flag_depro == 0):\n","        select.sort_values(by=\"prob\", ascending=True, inplace=True)\n","        select[label_y] = 1\n","    else:\n","        select.sort_values(by=\"prob\", ascending=False, inplace=True)\n","        select[label_y] = 0\n","    head = select.head(diff)\n","    index_list = []\n","    index_list = list(head.index)\n","    d.drop(index_list,inplace = True)\n","    head.drop(columns = ['prob'],inplace = True)\n","    #print(head.head())\n","    #print(d.head())\n","    return head\n","\n","\n","\n","def naive_massaging(d, temp2, names, need_pos, need_neg,label_y):\n","    # add more records for all groups\n","    # The smote algorithm to boost the coverage\n","    for r in need_pos:\n","        print(\"adding more positive\")\n","    # add more positive records\n","        # determine how many points to add\n","        print(r)\n","        diff = compute_diff_add_and_remove(r, temp2, 1, label_y, names)\n","        diff =  round_int(diff)\n","        # add more records\n","        #0 for promotion\n","        samples_to_add = get_depromotion(d, diff, r, names, label_y, flag_depro = 0)\n","        print(\"Changed \" + str(len(samples_to_add)) +\" records\")\n","        d = pd.concat([d, samples_to_add])\n","        print(len(d))\n","    for k in need_neg:\n","        print(k)\n","        print(\"adding more negative\")\n","        diff = compute_diff_add_and_remove(k, temp2, 0, label_y, names)\n","        diff =  round_int(diff)\n","        #1 for demotion\n","        samples_to_add = get_depromotion(d, diff, k, names, label_y, flag_depro = 1)\n","        print(\"Changed \" + str(len(samples_to_add)) +\" records\")\n","        d = pd.concat([d, samples_to_add])\n","        print(len(d))\n","    return d"]},{"cell_type":"markdown","metadata":{"id":"B6JgxVuTx8cj"},"source":["## Run Algorithm Lattice"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8CvRKxY6ne5o"},"outputs":[],"source":["#get all of the candidate groups possible with the combos and names\n","\n","new_train_data = copy.deepcopy(train_set)\n","\n","#iterate over all the names to get the temp2 df for each name\n","for a in all_names_lst:\n","  print(\"?????/////\")\n","  print(a)\n","  temp2, names = get_temp(new_train_data, all_names[a], compas_y)\n","  temp, temp_g = get_temp_g(new_train_data, names, compas_y)\n","  temp_g = temp_g[temp_g['cnt'] > filter_count]\n","  # print(temp_g)\n","  # print(temp2, names)\n","  # print(temp)\n","  lst_of_counts = compute_lst_of_counts(temp, names, compas_y)\n","  start = time.time()\n","  need_pos, need_neg = compute_problematic_opt(temp2, temp_g, names, compas_y, lst_of_counts)\n","  end = time.time()\n","  excute_time = end - start\n","  print(\"The time to compute unfair group is {}\".format(str(excute_time)))\n","  print(\"The sets of need pos and neg are\")\n","  print(need_pos)\n","  print(need_neg)\n","  new_train_data['skewed'] = 0\n","  new_train_data[\"diff\"] = 0\n","  # print(\"started duplication\")\n","  new_train_data = naive_massaging(new_train_data, temp2, names, need_pos, need_neg, compas_y)\n","  # print(new_train_data)\n","  # new_train_x = pd.DataFrame(new_train_data, columns = columns_compas)\n","  # new_train_label = pd.DataFrame(new_train_data, columns = [compas_y])\n","  # new_train_label = new_train_label[compas_y]\n","  # new_train_label = new_train_label.astype('int')\n","  # grid_new = DecisionTreeClassifier(criterion=\"entropy\",max_depth=6, random_state=17)\n","  # grid_new.fit(new_train_x, new_train_label)\n","  # test_predict = grid_new.predict(test_x)\n","  # print(\"new trainset length\", len(new_train_x))\n","  # print(\"fpr\", fpr_onegroup(list(test_label), test_predict))\n","  # print(\"fnr\", fnr_onegroup(list(test_label), test_predict))\n","  print(new_train_data[compas_y].value_counts())\n","new_train_x = pd.DataFrame(new_train_data, columns = columns_all)\n","new_train_label = pd.DataFrame(new_train_data, columns = [compas_y])\n","new_train_label = new_train_label[compas_y]\n","new_train_label = new_train_label.astype('int')\n","grid_new = GridSearchCV(tree.DecisionTreeClassifier(), param_grid = param, cv = 6)\n","grid_new.fit(new_train_x, new_train_label)\n","print(\"model accuracy\")\n","print(\"best\", grid_new.best_score_)\n","test_predict = grid_new.predict(test_x)\n","print(\"fpr and fnr\")\n","print(fpr_onegroup(list(test_label), test_predict))\n","print(fnr_onegroup(list(test_label), test_predict))"]},{"cell_type":"markdown","metadata":{"id":"T6hqSHlurjGO"},"source":["### Results Lattice"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pFlN6k7mqLRn"},"outputs":[],"source":["test_set['predicted'] = test_predict\n","d,d2,d3 = div_results(\"Bar\", \"Massaging-Lattice\",\"DT\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OvXIDnsDsiz4"},"outputs":[],"source":["# d,d1,d2 = div_results()\n","print()\n","print(\"rf\")\n","gridrf.fit(new_train_x, new_train_label)\n","print(\"best\", gridrf.best_score_)\n","test_predict = gridrf.predict(test_x)\n","print(\"fpr and fnr\")\n","# print(fpr_onegroup(new_labels.tolist(), predict))\n","# print(fnr_onegroup(new_labels.tolist(), predict))\n","print(fpr_onegroup(list(test_label), test_predict))\n","print(fnr_onegroup(list(test_label), test_predict))\n","test_set['predicted'] = test_predict\n","r,r2,r3 = div_results(\"Bar\", \"Massaging-Lattice\",\"RF\")\n","\n","print()\n","print(\"logistic\")\n","gridl.fit(new_train_x, new_train_label)\n","print(\"best\", gridl.best_score_)\n","test_predict = gridl.predict(test_x)\n","print(\"fpr and fnr\")\n","# print(fpr_onegroup(new_labels.tolist(), predict))\n","# print(fnr_onegroup(new_labels.tolist(), predict))\n","print(fpr_onegroup(list(test_label), test_predict))\n","print(fnr_onegroup(list(test_label), test_predict))\n","test_set['predicted'] = test_predict\n","r,r2,r3 = div_results(\"Bar\", \"Massaging-Lattice\",\"L\")\n","\n","print()\n","print(\"svm\")\n","svc.fit(new_train_x, new_train_label)\n","# print(\"best\", svc.best_score_)\n","test_predict = svc.predict(test_x)\n","print(\"fpr and fnr\")\n","# print(fpr_onegroup(new_labels.tolist(), predict))\n","# print(fnr_onegroup(new_labels.tolist(), predict))\n","print(fpr_onegroup(list(test_label), test_predict))\n","print(fnr_onegroup(list(test_label), test_predict))\n","test_set['predicted'] = test_predict\n","s,s2,s3 = div_results(\"Bar\", \"Massaging-Lattice\",\"SVM\")"]},{"cell_type":"markdown","metadata":{"id":"zfBfO1hhytVS"},"source":["## Run Algorithm Leaf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R6oMxp2hzVXD"},"outputs":[],"source":["#get all of the candidate groups possible with the combos and names\n","\n","new_train_data = copy.deepcopy(train_set)\n","\n","#iterate over all the names to get the temp2 df for each name\n","for a in [all_names_lst[0]]:\n","  print(\"?????/////\")\n","  print(a)\n","  temp2, names = get_temp(new_train_data, all_names[a], compas_y)\n","  temp, temp_g = get_temp_g(new_train_data, names, compas_y)\n","  temp_g = temp_g[temp_g['cnt'] > filter_count]\n","  # print(temp_g)\n","  # print(temp2, names)\n","  # print(temp)\n","  lst_of_counts = compute_lst_of_counts(temp, names, compas_y)\n","  start = time.time()\n","  need_pos, need_neg = compute_problematic_opt(temp2, temp_g, names, compas_y, lst_of_counts)\n","  end = time.time()\n","  excute_time = end - start\n","  print(\"The time to compute unfair group is {}\".format(str(excute_time)))\n","  print(\"The sets of need pos and neg are\")\n","  print(need_pos)\n","  print(need_neg)\n","  new_train_data['skewed'] = 0\n","  new_train_data[\"diff\"] = 0\n","  # print(\"started duplication\")\n","  new_train_data = naive_massaging(new_train_data, temp2, names, need_pos, need_neg, compas_y)\n","  # print(new_train_data)\n","  # new_train_x = pd.DataFrame(new_train_data, columns = columns_compas)\n","  # new_train_label = pd.DataFrame(new_train_data, columns = [compas_y])\n","  # new_train_label = new_train_label[compas_y]\n","  # new_train_label = new_train_label.astype('int')\n","  # grid_new = DecisionTreeClassifier(criterion=\"entropy\",max_depth=6, random_state=17)\n","  # grid_new.fit(new_train_x, new_train_label)\n","  # test_predict = grid_new.predict(test_x)\n","  # print(\"new trainset length\", len(new_train_x))\n","  # print(\"fpr\", fpr_onegroup(list(test_label), test_predict))\n","  # print(\"fnr\", fnr_onegroup(list(test_label), test_predict))\n","  print(new_train_data[compas_y].value_counts())\n","new_train_x = pd.DataFrame(new_train_data, columns = columns_all)\n","new_train_label = pd.DataFrame(new_train_data, columns = [compas_y])\n","new_train_label = new_train_label[compas_y]\n","new_train_label = new_train_label.astype('int')\n","grid_new = GridSearchCV(tree.DecisionTreeClassifier(), param_grid = param, cv = 6)\n","grid_new.fit(new_train_x, new_train_label)\n","print(\"model accuracy\")\n","print(\"best\", grid_new.best_score_)\n","test_predict = grid_new.predict(test_x)\n","print(\"fpr and fnr\")\n","print(fpr_onegroup(list(test_label), test_predict))\n","print(fnr_onegroup(list(test_label), test_predict))"]},{"cell_type":"markdown","metadata":{"id":"CM-vpguwyt2H"},"source":["### Results Leaf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JirQHr31ziUA"},"outputs":[],"source":["test_set['predicted'] = test_predict\n","d,d2,d3 = div_results(\"Bar\", \"Massaging-Leaf\",\"DT\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g0N6uIH9slwL"},"outputs":[],"source":["# d,d1,d2 = div_results()\n","print()\n","print(\"rf\")\n","gridrf.fit(new_train_x, new_train_label)\n","print(\"best\", gridrf.best_score_)\n","test_predict = gridrf.predict(test_x)\n","print(\"fpr and fnr\")\n","# print(fpr_onegroup(new_labels.tolist(), predict))\n","# print(fnr_onegroup(new_labels.tolist(), predict))\n","print(fpr_onegroup(list(test_label), test_predict))\n","print(fnr_onegroup(list(test_label), test_predict))\n","test_set['predicted'] = test_predict\n","r,r2,r3 = div_results(\"Bar\", \"Massaging-Leaf\",\"RF\")\n","\n","print()\n","print(\"logistic\")\n","gridl.fit(new_train_x, new_train_label)\n","print(\"best\", gridl.best_score_)\n","test_predict = gridl.predict(test_x)\n","print(\"fpr and fnr\")\n","# print(fpr_onegroup(new_labels.tolist(), predict))\n","# print(fnr_onegroup(new_labels.tolist(), predict))\n","print(fpr_onegroup(list(test_label), test_predict))\n","print(fnr_onegroup(list(test_label), test_predict))\n","test_set['predicted'] = test_predict\n","r,r2,r3 = div_results(\"Bar\", \"Massaging-Leaf\",\"L\")\n","\n","print()\n","print(\"svm\")\n","svc.fit(new_train_x, new_train_label)\n","# print(\"best\", svc.best_score_)\n","test_predict = svc.predict(test_x)\n","print(\"fpr and fnr\")\n","# print(fpr_onegroup(new_labels.tolist(), predict))\n","# print(fnr_onegroup(new_labels.tolist(), predict))\n","print(fpr_onegroup(list(test_label), test_predict))\n","print(fnr_onegroup(list(test_label), test_predict))\n","test_set['predicted'] = test_predict\n","s,s2,s3 = div_results(\"Bar\", \"Massaging-Leaf\",\"SVM\")"]},{"cell_type":"markdown","metadata":{"id":"i6oj1yzvyuNv"},"source":["## Run Algorithm Top"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lXWlcnvmzaef"},"outputs":[],"source":["#get all of the candidate groups possible with the combos and names\n","\n","new_train_data = copy.deepcopy(train_set)\n","\n","#iterate over all the names to get the temp2 df for each name\n","all_names_lst_top = find_top(all_names)\n","for a in all_names_lst_top:\n","  print(\"?????/////\")\n","  print(a)\n","  temp2, names = get_temp(new_train_data, all_names[a], compas_y)\n","  temp, temp_g = get_temp_g(new_train_data, names, compas_y)\n","  temp_g = temp_g[temp_g['cnt'] > filter_count]\n","  # print(temp_g)\n","  # print(temp2, names)\n","  # print(temp)\n","  lst_of_counts = compute_lst_of_counts(temp, names, compas_y)\n","  start = time.time()\n","  need_pos, need_neg = compute_problematic_opt(temp2, temp_g, names, compas_y, lst_of_counts)\n","  end = time.time()\n","  excute_time = end - start\n","  print(\"The time to compute unfair group is {}\".format(str(excute_time)))\n","  print(\"The sets of need pos and neg are\")\n","  print(need_pos)\n","  print(need_neg)\n","  new_train_data['skewed'] = 0\n","  new_train_data[\"diff\"] = 0\n","  # print(\"started duplication\")\n","  new_train_data = naive_massaging(new_train_data, temp2, names, need_pos, need_neg, compas_y)\n","  # print(new_train_data)\n","  # new_train_x = pd.DataFrame(new_train_data, columns = columns_compas)\n","  # new_train_label = pd.DataFrame(new_train_data, columns = [compas_y])\n","  # new_train_label = new_train_label[compas_y]\n","  # new_train_label = new_train_label.astype('int')\n","  # grid_new = DecisionTreeClassifier(criterion=\"entropy\",max_depth=6, random_state=17)\n","  # grid_new.fit(new_train_x, new_train_label)\n","  # test_predict = grid_new.predict(test_x)\n","  # print(\"new trainset length\", len(new_train_x))\n","  # print(\"fpr\", fpr_onegroup(list(test_label), test_predict))\n","  # print(\"fnr\", fnr_onegroup(list(test_label), test_predict))\n","  print(new_train_data[compas_y].value_counts())\n","new_train_x = pd.DataFrame(new_train_data, columns = columns_all)\n","new_train_label = pd.DataFrame(new_train_data, columns = [compas_y])\n","new_train_label = new_train_label[compas_y]\n","new_train_label = new_train_label.astype('int')\n","grid_new = GridSearchCV(tree.DecisionTreeClassifier(), param_grid = param, cv = 6)\n","grid_new.fit(new_train_x, new_train_label)\n","print(\"model accuracy\")\n","print(\"best\", grid_new.best_score_)\n","test_predict = grid_new.predict(test_x)\n","print(\"fpr and fnr\")\n","print(fpr_onegroup(list(test_label), test_predict))\n","print(fnr_onegroup(list(test_label), test_predict))"]},{"cell_type":"markdown","metadata":{"id":"YylnNCsvyuuU"},"source":["### Results Top"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b-5OxZ1MzjoN"},"outputs":[],"source":["test_set['predicted'] = test_predict\n","d,d2,d3 = div_results(\"Bar\", \"Massaging-Top\",\"DT\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NWUjzEK5smxi"},"outputs":[],"source":["# d,d1,d2 = div_results()\n","print()\n","print(\"rf\")\n","gridrf.fit(new_train_x, new_train_label)\n","print(\"best\", gridrf.best_score_)\n","test_predict = gridrf.predict(test_x)\n","print(\"fpr and fnr\")\n","# print(fpr_onegroup(new_labels.tolist(), predict))\n","# print(fnr_onegroup(new_labels.tolist(), predict))\n","print(fpr_onegroup(list(test_label), test_predict))\n","print(fnr_onegroup(list(test_label), test_predict))\n","test_set['predicted'] = test_predict\n","r,r2,r3 = div_results(\"Bar\", \"Massaging-Top\",\"RF\")\n","\n","print()\n","print(\"logistic\")\n","gridl.fit(new_train_x, new_train_label)\n","print(\"best\", gridl.best_score_)\n","test_predict = gridl.predict(test_x)\n","print(\"fpr and fnr\")\n","# print(fpr_onegroup(new_labels.tolist(), predict))\n","# print(fnr_onegroup(new_labels.tolist(), predict))\n","print(fpr_onegroup(list(test_label), test_predict))\n","print(fnr_onegroup(list(test_label), test_predict))\n","test_set['predicted'] = test_predict\n","r,r2,r3 = div_results(\"Bar\", \"Massaging-Top\",\"L\")\n","\n","print()\n","print(\"svm\")\n","svc.fit(new_train_x, new_train_label)\n","# print(\"best\", svc.best_score_)\n","test_predict = svc.predict(test_x)\n","print(\"fpr and fnr\")\n","# print(fpr_onegroup(new_labels.tolist(), predict))\n","# print(fnr_onegroup(new_labels.tolist(), predict))\n","print(fpr_onegroup(list(test_label), test_predict))\n","print(fnr_onegroup(list(test_label), test_predict))\n","test_set['predicted'] = test_predict\n","s,s2,s3 = div_results(\"Bar\", \"Massaging-Top\",\"SVM\")"]},{"cell_type":"code","source":[],"metadata":{"id":"vEPOZIydnJeh"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
