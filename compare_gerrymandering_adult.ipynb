{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oMgFj8Ve2Oq"
      },
      "source": [
        "# DecisionTreeClassifer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "772S9aKIfv-9"
      },
      "source": [
        "#Imports and Dataset processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Evnn9BbOfEme"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import metrics\n",
        "import copy\n",
        "from sympy import Symbol\n",
        "from sympy.solvers import solve\n",
        "pd.options.mode.chained_assignment = None\n",
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "drive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnHqZnCJTcxE",
        "outputId": "a0386ae6-adb8-4f9e-ba5b-acdac47e7b85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'google.colab.drive' from '/usr/local/lib/python3.10/dist-packages/google/colab/drive.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "_vP0FN3vfgjA",
        "outputId": "68ba0956-540f-4d31-9bf1-4f8dedd6419c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       age  workclass  fnlwgt  education  educational-num  marital-status  \\\n",
              "0        0          2  226802          1                7               4   \n",
              "1        1          2   89814         11                9               2   \n",
              "2        0          1  336951          7               12               2   \n",
              "3        1          2  160323         15               10               2   \n",
              "4        1          2  198693          0                6               4   \n",
              "...    ...        ...     ...        ...              ...             ...   \n",
              "45217    0          2  257302          7               12               2   \n",
              "45218    1          2  154374         11                9               2   \n",
              "45219    2          2  151910         11                9               6   \n",
              "45220    0          2  201490         11                9               4   \n",
              "45221    2          3  287927         11                9               2   \n",
              "\n",
              "       occupation  relationship  race  gender  capital-gain  capital-loss  \\\n",
              "0               6             3     2       1             0             0   \n",
              "1               4             0     4       1             0             0   \n",
              "2              10             0     4       1             0             0   \n",
              "3               6             0     2       1             1             0   \n",
              "4               7             1     4       1             0             0   \n",
              "...           ...           ...   ...     ...           ...           ...   \n",
              "45217          12             5     4       0             0             0   \n",
              "45218           6             0     4       1             0             0   \n",
              "45219           0             4     4       0             0             0   \n",
              "45220           0             3     4       1             0             0   \n",
              "45221           3             5     4       0             1             0   \n",
              "\n",
              "       hours-per-week  native-country  income  \n",
              "0                   0              38       0  \n",
              "1                   1              38       0  \n",
              "2                   0              38       1  \n",
              "3                   0              38       1  \n",
              "4                   0              38       0  \n",
              "...               ...             ...     ...  \n",
              "45217               0              38       0  \n",
              "45218               0              38       1  \n",
              "45219               0              38       0  \n",
              "45220               0              38       0  \n",
              "45221               0              38       1  \n",
              "\n",
              "[45222 rows x 15 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f0d200b9-2b7f-4218-8fa6-ce7b411efdc9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>workclass</th>\n",
              "      <th>fnlwgt</th>\n",
              "      <th>education</th>\n",
              "      <th>educational-num</th>\n",
              "      <th>marital-status</th>\n",
              "      <th>occupation</th>\n",
              "      <th>relationship</th>\n",
              "      <th>race</th>\n",
              "      <th>gender</th>\n",
              "      <th>capital-gain</th>\n",
              "      <th>capital-loss</th>\n",
              "      <th>hours-per-week</th>\n",
              "      <th>native-country</th>\n",
              "      <th>income</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>226802</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>89814</td>\n",
              "      <td>11</td>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>336951</td>\n",
              "      <td>7</td>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>38</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>160323</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>38</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>198693</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45217</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>257302</td>\n",
              "      <td>7</td>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45218</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>154374</td>\n",
              "      <td>11</td>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>38</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45219</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>151910</td>\n",
              "      <td>11</td>\n",
              "      <td>9</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45220</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>201490</td>\n",
              "      <td>11</td>\n",
              "      <td>9</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45221</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>287927</td>\n",
              "      <td>11</td>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>38</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>45222 rows Ã— 15 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f0d200b9-2b7f-4218-8fa6-ce7b411efdc9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f0d200b9-2b7f-4218-8fa6-ce7b411efdc9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f0d200b9-2b7f-4218-8fa6-ce7b411efdc9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6629e121-3789-4846-a150-11c77c022ba8\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6629e121-3789-4846-a150-11c77c022ba8')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6629e121-3789-4846-a150-11c77c022ba8 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "url = \"/content/drive/Shareddrives/Data Remedy/models/CleanAdult_numerical_cat.csv\"\n",
        "data = pd.read_csv(url)\n",
        "# data.dtypes\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['income'].value_counts()"
      ],
      "metadata": {
        "id": "Ei5tLJ_rGb9I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d03102d-5416-458f-c8a2-409a44241442"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    34014\n",
              "1    11208\n",
              "Name: income, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNSifBfwflg8"
      },
      "outputs": [],
      "source": [
        "# get training and testing set\n",
        "\n",
        "# columns_compas = ['stay', 'age', 'charge', 'sex', '#prior', 'race']\n",
        "\n",
        "# columns_compas = ['age',  'education', 'marital-status',\n",
        "#                                             'occupation', 'relationship', 'race','gender',\n",
        "#                                           'native-country']\n",
        "columns_compas = ['age', 'race', 'gender']\n",
        "\n",
        "\n",
        "columns_all = ['age', 'workclass', 'education', 'educational-num', 'marital-status',\n",
        "                                            'occupation', 'relationship', 'race','gender', 'capital-gain', 'capital-loss',\n",
        "                                            'hours-per-week', 'native-country']\n",
        "# compas_y = 'class'\n",
        "compas_y = 'income'\n",
        "def split_train_test(data,test_ratio):\n",
        "    np.random.seed(42)\n",
        "    shuffled_indices = np.random.permutation(len(data))\n",
        "    test_set_size = int(len(data) * test_ratio)\n",
        "    test_indices = shuffled_indices[:test_set_size]\n",
        "    train_indices = shuffled_indices[test_set_size:]\n",
        "    return data.iloc[train_indices],data.iloc[test_indices]\n",
        "\n",
        "def get_train_test(data, split, list_cols, y_label):\n",
        "  all_list = copy.deepcopy(list_cols)\n",
        "  all_list.append(y_label)\n",
        "  data = pd.DataFrame(data, columns = all_list)\n",
        "  train_set,test_set = split_train_test(data,split)\n",
        "  print(len(train_set), \"train +\", len(test_set), \"test\")\n",
        "  train_x = pd.DataFrame(train_set, columns = list_cols)\n",
        "  train_label = train_set[y_label]\n",
        "  test_x = pd.DataFrame(test_set, columns = list_cols)\n",
        "  test_label = test_set[y_label]\n",
        "  return train_x, test_x, train_label, test_label, train_set, test_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJQsq95bfpEH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90af8131-32b3-4294-eb28-dc4190f98be3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "31656 train + 13566 test\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "25 fits failed out of a total of 50.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "25 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.80976744        nan 0.81103103        nan 0.81106263\n",
            "        nan 0.81103104        nan 0.81099945]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best 0.8110626270894963\n"
          ]
        }
      ],
      "source": [
        "train_x, test_x, train_label, test_label, train_set, test_set  = get_train_test(data, 0.3, columns_all, compas_y)\n",
        "\n",
        "###################\n",
        "\n",
        "### about decision tree\n",
        "# from sklearn import tree\n",
        "# import matplotlib.pyplot as plt\n",
        "# from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "gridlg.fit(train_x, train_label)\n",
        "\n",
        "print(\"best\", gridlg.best_score_)\n",
        "test_predict = gridlg.predict(test_x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "test_set['predicted'] = test_predict\n",
        "# print(test_set)\n",
        "# print(test_x)\n",
        "# print(data[compas_y].value_counts())\n",
        "# print(data['predicted'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-rnfp9XfuKM"
      },
      "outputs": [],
      "source": [
        "def fpr_onegroup(true, predict):\n",
        "    fp = 0\n",
        "    tn = 0\n",
        "    for i in range(len(true)):\n",
        "        if (true[i] == 0 and predict[i] == 1):\n",
        "            fp += 1\n",
        "        if(true[i] == 0 and predict[i] == 0):\n",
        "            tn += 1\n",
        "    return fp/(fp+tn)\n",
        "\n",
        "\n",
        "def fnr_onegroup(true, predict):\n",
        "    fn = 0\n",
        "    tp = 0\n",
        "    for i in range(len(true)):\n",
        "        if (true[i] == 1 and predict[i] == 0):\n",
        "            fn += 1\n",
        "        if(true[i] == 1 and predict[i] == 1):\n",
        "            tp += 1\n",
        "    return fn/(fn+tp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axb85-QngAPX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "da66cfe8-9dce-4888-fd4e-f680218f1446"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-8a2013b330c9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfpr_onegroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fpr is \"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mfpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfnr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfnr_onegroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fnr is \"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mfnr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_predict' is not defined"
          ]
        }
      ],
      "source": [
        "fpr = fpr_onegroup(list(test_label), test_predict)\n",
        "print(\"fpr is \" , fpr)\n",
        "\n",
        "fnr = fnr_onegroup(list(test_label), test_predict)\n",
        "print(\"fnr is \" , fnr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qzANv0KigX3"
      },
      "outputs": [],
      "source": [
        "# # print(fpr_onegroup(l1, l3))\n",
        "# accuracy = accuracy_score(test_label, test_predict)\n",
        "# print(\"accuracy is \" , accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, make_scorer\n",
        "\n",
        "\n",
        "filter_count = 30\n",
        "\n",
        "\n",
        "\n",
        "scoring = make_scorer(accuracy_score)\n",
        "# gridsvc = SVC(kernel='rbf', C=6.0, gamma = 100, random_state =42)\n",
        "# gridrf = GridSearchCV(RandomForestClassifier(), param_grid = param, cv = 6)\n",
        "# paraml = {'penalty': ['l2'],'C':[0.1, 1, 10, 100, 1000], 'solver':['saga'], 'multi_class':['ovr']}\n",
        "# gridl = GridSearchCV(LogisticRegression(), param_grid = paraml, cv = 6)\n",
        "# param_gridsvc = {\n",
        "#     'C': [0.1, 1, 10],\n",
        "#     'gamma': [0.01, 0.1, 1]\n",
        "# }\n",
        "# scoring = make_scorer(accuracy_score)\n",
        "# svc = SVC(kernel='rbf', random_state=42)\n",
        "# gridsvc = GridSearchCV(svc, param_grid=param_gridsvc, scoring=scoring, cv=5)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# #####################\n",
        "param_gridlg = {\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'C': [0.01, 0.1, 1, 10, 100]\n",
        "}\n",
        "logreg = LogisticRegression(random_state=42, max_iter=1000)\n",
        "gridlg = GridSearchCV(logreg, param_grid=param_gridlg, scoring=scoring, cv=5)\n",
        "# #####################\n",
        "param_griddt = {\n",
        "    'max_depth': [2, 5, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "griddt = GridSearchCV(dt, param_grid=param_griddt, scoring=scoring, cv=5)\n",
        "\n",
        "# #####################\n",
        "param_gridrf = {'criterion': ['gini', 'entropy'], 'max_depth': [10, 20, 30, 40, 50, 100], 'random_state':[17]}\n",
        "# param_gridrf = {\n",
        "#     'max_depth': [10, 20, 30],\n",
        "#     'min_samples_split': [2, 5, 10],\n",
        "#     'min_samples_leaf': [1, 2, 4],\n",
        "# }\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "gridrf = GridSearchCV(rf, param_grid=param_gridrf, scoring=scoring, cv=5)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "asgU_pEhAOlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "djQgbNZ9L60H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmhcs6wcgGiZ"
      },
      "source": [
        "# Divexplorer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVt4YIpUgKlo",
        "outputId": "e37319e1-f496-489d-e498-2418926c1c07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: DivExplorer==0.1.1 in /usr/local/lib/python3.10/dist-packages (0.1.1)\n",
            "Requirement already satisfied: ipywidgets>=7.2.1 in /usr/local/lib/python3.10/dist-packages (from DivExplorer==0.1.1) (7.7.1)\n",
            "Requirement already satisfied: matplotlib>=3.1.1 in /usr/local/lib/python3.10/dist-packages (from DivExplorer==0.1.1) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.10/dist-packages (from DivExplorer==0.1.1) (1.23.5)\n",
            "Requirement already satisfied: mlxtend>=0.17.1 in /usr/local/lib/python3.10/dist-packages (from DivExplorer==0.1.1) (0.22.0)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.10/dist-packages (from DivExplorer==0.1.1) (1.5.3)\n",
            "Requirement already satisfied: plotly>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from DivExplorer==0.1.1) (5.15.0)\n",
            "Requirement already satisfied: python-igraph>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from DivExplorer==0.1.1) (0.11.3)\n",
            "Requirement already satisfied: scikit-learn>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from DivExplorer==0.1.1) (1.2.2)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.2.1->DivExplorer==0.1.1) (5.5.6)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.2.1->DivExplorer==0.1.1) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.2.1->DivExplorer==0.1.1) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.2.1->DivExplorer==0.1.1) (3.6.6)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.2.1->DivExplorer==0.1.1) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.2.1->DivExplorer==0.1.1) (3.0.9)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.1->DivExplorer==0.1.1) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.1->DivExplorer==0.1.1) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.1->DivExplorer==0.1.1) (4.44.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.1->DivExplorer==0.1.1) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.1->DivExplorer==0.1.1) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.1->DivExplorer==0.1.1) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.1->DivExplorer==0.1.1) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.1->DivExplorer==0.1.1) (2.8.2)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.10/dist-packages (from mlxtend>=0.17.1->DivExplorer==0.1.1) (1.11.3)\n",
            "Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from mlxtend>=0.17.1->DivExplorer==0.1.1) (1.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from mlxtend>=0.17.1->DivExplorer==0.1.1) (67.7.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.2->DivExplorer==0.1.1) (2023.3.post1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=4.5.0->DivExplorer==0.1.1) (8.2.3)\n",
            "Requirement already satisfied: igraph==0.11.3 in /usr/local/lib/python3.10/dist-packages (from python-igraph>=0.8.3->DivExplorer==0.1.1) (0.11.3)\n",
            "Requirement already satisfied: texttable>=1.6.2 in /usr/local/lib/python3.10/dist-packages (from igraph==0.11.3->python-igraph>=0.8.3->DivExplorer==0.1.1) (1.7.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.23.2->DivExplorer==0.1.1) (3.2.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.2.1->DivExplorer==0.1.1) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.2.1->DivExplorer==0.1.1) (6.3.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.2.1->DivExplorer==0.1.1) (0.19.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.2.1->DivExplorer==0.1.1) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.2.1->DivExplorer==0.1.1) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.2.1->DivExplorer==0.1.1) (3.0.41)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.2.1->DivExplorer==0.1.1) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.2.1->DivExplorer==0.1.1) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.2.1->DivExplorer==0.1.1) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.2.1->DivExplorer==0.1.1) (4.8.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.1.1->DivExplorer==0.1.1) (1.16.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets>=7.2.1->DivExplorer==0.1.1) (6.5.5)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets>=7.2.1->DivExplorer==0.1.1) (0.8.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.2.1->DivExplorer==0.1.1) (3.1.2)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.2.1->DivExplorer==0.1.1) (23.2.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.2.1->DivExplorer==0.1.1) (23.1.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.2.1->DivExplorer==0.1.1) (5.5.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.2.1->DivExplorer==0.1.1) (5.9.2)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.2.1->DivExplorer==0.1.1) (6.5.4)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.2.1->DivExplorer==0.1.1) (1.5.8)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.2.1->DivExplorer==0.1.1) (1.8.2)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.2.1->DivExplorer==0.1.1) (0.18.0)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.2.1->DivExplorer==0.1.1) (0.18.0)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.2.1->DivExplorer==0.1.1) (1.0.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets>=7.2.1->DivExplorer==0.1.1) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets>=7.2.1->DivExplorer==0.1.1) (0.2.10)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.2.1->DivExplorer==0.1.1) (4.0.0)\n",
            "Requirement already satisfied: jupyter-server>=1.8 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.2.1->DivExplorer==0.1.1) (1.24.0)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.2.1->DivExplorer==0.1.1) (0.2.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.2.1->DivExplorer==0.1.1) (4.9.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.2.1->DivExplorer==0.1.1) (4.11.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.2.1->DivExplorer==0.1.1) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.2.1->DivExplorer==0.1.1) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.2.1->DivExplorer==0.1.1) (0.4)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.2.1->DivExplorer==0.1.1) (0.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.2.1->DivExplorer==0.1.1) (2.1.3)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.2.1->DivExplorer==0.1.1) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.2.1->DivExplorer==0.1.1) (0.9.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.2.1->DivExplorer==0.1.1) (1.5.0)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.2.1->DivExplorer==0.1.1) (1.2.1)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.2.1->DivExplorer==0.1.1) (2.19.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.2.1->DivExplorer==0.1.1) (4.19.2)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.2.1->DivExplorer==0.1.1) (21.2.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.2.1->DivExplorer==0.1.1) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.2.1->DivExplorer==0.1.1) (2023.11.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.2.1->DivExplorer==0.1.1) (0.31.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.2.1->DivExplorer==0.1.1) (0.12.0)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.2.1->DivExplorer==0.1.1) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.2.1->DivExplorer==0.1.1) (1.6.4)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.2.1->DivExplorer==0.1.1) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.2.1->DivExplorer==0.1.1) (2.5)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.2.1->DivExplorer==0.1.1) (0.5.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.2.1->DivExplorer==0.1.1) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.2.1->DivExplorer==0.1.1) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.2.1->DivExplorer==0.1.1) (1.1.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.2.1->DivExplorer==0.1.1) (2.21)\n"
          ]
        }
      ],
      "source": [
        "pip install DivExplorer==0.1.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tb3zgDbUgXcV"
      },
      "outputs": [],
      "source": [
        "def fairness_score_computation(d, metrics):\n",
        "    sum_of_score = 0\n",
        "    for idx, row in d.iterrows():\n",
        "      sum_of_score += row['support'] * row[metrics]\n",
        "    return sum_of_score\n",
        "\n",
        "# print(fairness_score_computation(d, 'd_fpr'))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_test_predict(gridalg, train_x, alg_name):\n",
        "  gridalg.fit(train_x, train_label)\n",
        "  print(\"best {}\".format(alg_name), gridalg.best_score_)\n",
        "  test_predict = gridalg.predict(test_x)\n",
        "  data_all = pd.concat([train_x,test_x])\n",
        "  data_predict = gridalg.predict(data_all)\n",
        "  test_predict = gridalg.predict(test_x)\n",
        "  data['predicted'] = data_predict\n",
        "  test_set['predicted'] = test_predict"
      ],
      "metadata": {
        "id": "wq83r2kCMpr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get_test_predict(griddt, train_x, \"dt\")"
      ],
      "metadata": {
        "id": "yBfBWDzuMh3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SgtAJze6sv2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(test_label, test_set['predicted'])\n",
        "print(\"accuracy is \" , accuracy)\n"
      ],
      "metadata": {
        "id": "-dmbWEw9NqRn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "073683ba-bdb8-48e8-e06d-61698292de04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy is  0.813578062804069\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# run divexplorer to find unfair groups\n",
        "class_map={'N': 0, 'P': 1}\n",
        "from divexplorer.FP_DivergenceExplorer import FP_DivergenceExplorer\n",
        "\n",
        "\n",
        "######\n",
        "# Yin: by consideri the unfair subgroups, we only consider the subgroups given by the protected attributes.\n",
        "\n",
        "columns_compas.extend([compas_y, \"predicted\"])\n",
        "\n",
        "df = pd.DataFrame(test_set, columns = columns_compas)\n",
        "# print(test_set.columns)\n",
        "# print(columns_compas)\n",
        "# print(df.columns)\n",
        "# print(columns_compas)\n",
        "\n",
        "columns_compas.remove(compas_y)\n",
        "columns_compas.remove('predicted')\n",
        "\n",
        "######\n",
        "\n",
        "min_sup=0.1\n",
        "# min_sup = 0.05\n",
        "# min_sup = 0.01\n",
        "\n",
        "fp_diver=FP_DivergenceExplorer(df,compas_y, \"predicted\", class_map=class_map)\n",
        "FP_fm=fp_diver.getFrequentPatternDivergence(min_support=min_sup, metrics=[\"d_fpr\", \"d_fnr\", \"d_accuracy\"])\n",
        "from divexplorer.FP_Divergence import FP_Divergence\n",
        "fp_divergence_fpr=FP_Divergence(FP_fm, \"d_fpr\")\n",
        "fp_divergence_fnr=FP_Divergence(FP_fm, \"d_fnr\")\n",
        "fp_divergence_acc=FP_Divergence(FP_fm, \"d_accuracy\")\n",
        "  # fp_divergence_fpr=FP_Divergence(FP_fm, \"d_fnr\")\n",
        "INFO_VIZ=[\"support\", \"itemsets\",  fp_divergence_fpr.metric, fp_divergence_fpr.t_value_col]\n",
        "INFO_VIZ2=[\"support\", \"itemsets\",  fp_divergence_fnr.metric, fp_divergence_fnr.t_value_col]\n",
        "INFO_VIZ3=[\"support\", \"itemsets\",  fp_divergence_acc.metric, fp_divergence_acc.t_value_col]\n",
        "eps=0.01\n",
        "K=1000\n",
        "d = fp_divergence_fpr.getDivergence(th_redundancy=eps)[INFO_VIZ].head(K)\n",
        "d2 = fp_divergence_fnr.getDivergence(th_redundancy=eps)[INFO_VIZ2].head(K)\n",
        "d3 = fp_divergence_acc.getDivergence(th_redundancy=eps)[INFO_VIZ3].head(K)\n",
        "\n",
        "\n",
        "\n",
        "pd.options.display.max_rows = 200\n",
        "d = fp_divergence_fpr.getDivergence(th_redundancy=0)[INFO_VIZ].head(K)\n",
        "# summerization\n",
        "\n",
        "d = fp_divergence_fpr.getDivergence(th_redundancy=eps)[INFO_VIZ].head(K)\n",
        "d= d[d['d_fpr'] > 0]\n",
        "d2= d2[d2['d_fnr'] > 0]\n",
        "d3= d3[d3['d_accuracy'] > 0]\n",
        "\n",
        "dfpr = fairness_score_computation(d, 'd_fpr')\n",
        "dfnr = fairness_score_computation(d2, 'd_fnr')\n",
        "dacc = fairness_score_computation(d3, 'd_accuracy')\n",
        "d, list(d[\"itemsets\"].iloc[0])\n",
        "# dacc"
      ],
      "metadata": {
        "id": "bYDp7XFE2Q6x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8af5f44-d106-4a9e-c8e1-23720d8bc1ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/divexplorer/FP_DivergenceExplorer.py:489: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  fp = fp.append(row_root, ignore_index=True)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(     support           itemsets     d_fpr  t_value_fp\n",
              " 17  0.136223  (gender=1, age=2)  0.163435   12.824588\n",
              " 13  0.188560            (age=2)  0.099333   10.824825\n",
              " 6   0.352425  (gender=1, age=1)  0.052835    8.192104\n",
              " 2   0.680525         (gender=1)  0.031107    7.040512\n",
              " 4   0.496978            (age=1)  0.015120    3.223398,\n",
              " ['gender=1', 'age=2'])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1QotGAAo9Y7F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d183712-b70e-4de2-95f9-f42597cbc9c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D-xbkRgkQDxA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d9305ec-5a27-4132-8b19-8c6355371f6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kf1ffb1bggPp"
      },
      "source": [
        "# For entire dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aU9MirNCglkA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5786b99d-990f-4b5d-c792-9dd5f9c5d36a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "import itertools\n",
        "def get_unfair_group(list_parse, entire = 1):\n",
        "  unfair_group = []\n",
        "  unfair_dict = {}\n",
        "  names = []\n",
        "  for col in columns_compas:\n",
        "    found = False\n",
        "    for item in list_parse:\n",
        "      attr_given = item.split(\"=\")[0]\n",
        "      if col == attr_given:\n",
        "        unfair_group.append(int(item.split(\"=\")[1]))\n",
        "        names.append(attr_given)\n",
        "        unfair_dict[attr_given] = int(item.split(\"=\")[1])\n",
        "        found = True\n",
        "  # if use the entire dataset\n",
        "  if entire:\n",
        "    return unfair_group, names, columns_compas, unfair_dict\n",
        "        # break\n",
        "    # if found == False:\n",
        "    #   unfair_group.append(-1)\n",
        "  return unfair_group, names, list(set(columns_compas).symmetric_difference(set(names))), unfair_dict\n",
        "def candidate_groups(skew_candidates, unfair_dict, ordering, names):\n",
        "  candidate_combos = []\n",
        "  candidate_ind = {}\n",
        "  num = 0\n",
        "  for i in range(len(skew_candidates)+1):\n",
        "    temp_candidate = list(itertools.combinations(skew_candidates, i))\n",
        "    for tc in temp_candidate:\n",
        "      candidate_ind[num] = list(tc)\n",
        "      num += 1\n",
        "  return candidate_ind\n",
        "\n",
        "def name_val_dict(train_set,names):\n",
        "  names_values = {}\n",
        "  for n in names:\n",
        "    names_values[n] = list(train_set[n].unique())\n",
        "  return names_values\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgdhgqYZjTci",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "98c496c7-061b-4c7a-9854-cde37275a6cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    age  race  gender  income   cnt\n",
              "0     0     0       0       0    41\n",
              "1     0     0       0       1     1\n",
              "2     0     0       1       0    52\n",
              "3     0     0       1       1     4\n",
              "4     0     1       0       0   108\n",
              "5     0     1       0       1     6\n",
              "6     0     1       1       0   153\n",
              "7     0     1       1       1    23\n",
              "8     0     2       0       0   456\n",
              "9     0     2       0       1     8\n",
              "10    0     2       1       0   471\n",
              "11    0     2       1       1    13\n",
              "12    0     3       0       0    46\n",
              "13    0     3       0       1     2\n",
              "14    0     3       1       0    63\n",
              "15    0     3       1       1     1\n",
              "16    0     4       0       0  3189\n",
              "17    0     4       0       1   132\n",
              "18    0     4       1       0  4745\n",
              "19    0     4       1       1   480\n",
              "20    1     0       0       0    40\n",
              "21    1     0       0       1     9\n",
              "22    1     0       1       0    86\n",
              "23    1     0       1       1    19\n",
              "24    1     1       0       0   125\n",
              "25    1     1       0       1    31\n",
              "26    1     1       1       0   180\n",
              "27    1     1       1       1   145\n",
              "28    1     2       0       0   686\n",
              "29    1     2       0       1    66\n",
              "30    1     2       1       0   552\n",
              "31    1     2       1       1   206\n",
              "32    1     3       0       0    38\n",
              "33    1     3       0       1     5\n",
              "34    1     3       1       0    55\n",
              "35    1     3       1       1    18\n",
              "36    1     4       0       0  2832\n",
              "37    1     4       0       1   681\n",
              "38    1     4       1       0  5799\n",
              "39    1     4       1       1  3966\n",
              "40    2     0       0       0    21\n",
              "41    2     0       0       1     1\n",
              "42    2     0       1       0    18\n",
              "43    2     0       1       1     3\n",
              "44    2     1       0       0    32\n",
              "45    2     1       0       1    10\n",
              "46    2     1       1       0    59\n",
              "47    2     1       1       1    51\n",
              "48    2     2       0       0   241\n",
              "49    2     2       0       1    14\n",
              "50    2     2       1       0   180\n",
              "51    2     2       1       1    82\n",
              "52    2     3       0       0     3\n",
              "53    2     3       1       0    10\n",
              "54    2     3       1       1     7\n",
              "55    2     4       0       0  1318\n",
              "56    2     4       0       1   219\n",
              "57    2     4       1       0  2175\n",
              "58    2     4       1       1  1679"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-448fb8ac-f261-47ba-a17f-1d7cb41a767d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>race</th>\n",
              "      <th>gender</th>\n",
              "      <th>income</th>\n",
              "      <th>cnt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4745</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>552</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>5799</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1679</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-448fb8ac-f261-47ba-a17f-1d7cb41a767d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-448fb8ac-f261-47ba-a17f-1d7cb41a767d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-448fb8ac-f261-47ba-a17f-1d7cb41a767d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4aa6c4e6-4724-4207-9b5b-16708c96979b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4aa6c4e6-4724-4207-9b5b-16708c96979b')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4aa6c4e6-4724-4207-9b5b-16708c96979b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "def get_temp(train_set, names, y_label):\n",
        "  names2 = copy.deepcopy(names)\n",
        "  names2.append(y_label)\n",
        "  temp = train_set[names2]\n",
        "  temp['cnt'] = 0\n",
        "  temp2 = temp.groupby(names2)['cnt'].count().reset_index()\n",
        "  temp2['cnt'].sum()\n",
        "  return temp2, names\n",
        "temp2, names = get_temp(train_set, columns_compas, compas_y)\n",
        "temp2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuEIYpHwjg_H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44a28668-6cd6-4d98-80bb-1225e3908cea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "def get_temp_g(train_set, names, y_label):\n",
        "  names2 = copy.deepcopy(names)\n",
        "  names2.append(y_label)\n",
        "  temp = train_set[names2]\n",
        "  temp['cnt'] = 0\n",
        "  temp_g = temp.groupby(names)['cnt'].count().reset_index()\n",
        "  return temp, temp_g"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMrAfuoDtjAQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d28d8a5d-7019-4917-c50d-b07b127420b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[] [] ['age', 'race', 'gender'] {}\n",
            "{0: [], 1: ['age'], 2: ['race'], 3: ['gender'], 4: ['age', 'race'], 5: ['age', 'gender'], 6: ['race', 'gender'], 7: ['age', 'race', 'gender']}\n",
            "{'age': [2, 1, 0], 'race': [4, 1, 2, 0, 3], 'gender': [1, 0]}\n",
            "[7, 6, 5, 4, 3, 2, 1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "unfair_group, unfair_names, skew_candidates, unfair_dict = get_unfair_group([])\n",
        "print(unfair_group, unfair_names, skew_candidates, unfair_dict)\n",
        "all_names = candidate_groups(skew_candidates, unfair_dict, columns_compas, unfair_names)\n",
        "names_values = name_val_dict(train_set, names)\n",
        "print(all_names)\n",
        "print(names_values)\n",
        "# all_names_lst = list(all_names.keys())[len(columns_compas)+1:]\n",
        "all_names_lst = list(all_names.keys())[1:] # CHANGED HERE\n",
        "all_names_lst.reverse()\n",
        "print(all_names_lst)\n",
        "# all_names_lst"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Z2ZjY1-6f1L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4a0efa2-fc1e-4a92-e02f-fdf09d2dba21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "# def rec_combos(all_vals):\n",
        "# https://stackoverflow.com/questions/45472214/itertools-product-uses-too-much-memory\n",
        "def get_combos(names, names_values, curr_combo):\n",
        "  all_vals = []\n",
        "  X = []\n",
        "  for c in curr_combo:\n",
        "    all_vals.append(names_values[c])\n",
        "  X += itertools.islice(itertools.product(*all_vals),1000000)\n",
        "  return X\n",
        "# get_combos(names, names_values, all_names[8191])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7k-CLD9ygjS9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6924f40b-f147-43d3-c836-8548ddd74c0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "# unfair_group, unfair_names, skew_candidates, unfair_dict = get_unfair_group([])\n",
        "# print(unfair_group, unfair_names, skew_candidates, unfair_dict)\n",
        "# combos_dict, all_names = candidate_groups(skew_candidates, unfair_dict, columns_compas, unfair_names)\n",
        "# print(combos_dict, all_names)\n",
        "\n",
        "# the\n",
        "# print(list(all_names.keys())[len(columns_compas)+1:])\n",
        "# all_names_lst = list(all_names.keys())[len(columns_compas)+1:]\n",
        "# all_names_lst.reverse()\n",
        "# print(all_names_lst)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TttueBgvg53C"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4TwHPK4hd0A"
      },
      "source": [
        "## General Helper Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvC9RSgZkD80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7102ceed-44f9-4fb0-95a0-0dedf40b9d88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "def get_one_degree_neighbors(temp2, names, group_lst):\n",
        "    result = []\n",
        "    for i in range(len(group_lst)):\n",
        "        d = copy.copy(temp2)\n",
        "        for k in range(len(group_lst)):\n",
        "            if k != i:\n",
        "                d = d[d[names[k]] == group_lst[k]]\n",
        "            else:\n",
        "                d = d[d[names[k]] != group_lst[k]]\n",
        "        # print(d)\n",
        "        result.append(d)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGQalez0kLkE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "004edf7b-e0ed-480e-bf3b-6c06f226569d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "# compute the pos/neg ration of this neighbor\n",
        "def compute_neighbors(group_lst, result):\n",
        "    # compute the ratio of positive and negative records\n",
        "    start2 = time.time()\n",
        "    pos = 0\n",
        "    neg = 0\n",
        "    for r in result:\n",
        "        total  = r['cnt'].sum()\n",
        "        r = r[r[compas_y] == 1]\n",
        "        pos += r['cnt'].sum()\n",
        "        neg += total - r['cnt'].sum()\n",
        "    if(neg == 0):\n",
        "        return (pos, neg, -1)\n",
        "    end2 = time.time()\n",
        "    # print(\"The time to compute the neighbor counts for \" +  str(group_lst) +\" is \" + str(end2-start2))\n",
        "    return(pos, neg, pos/neg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijabVWcBjtCh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02fc2c38-63bc-4df2-9a5d-b15c1bccb8b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "def compute_diff_add_and_remove(group_lst, temp2, need_positive_or_negative, label, names):\n",
        "    d = copy.copy(temp2)\n",
        "    for i in range(len(group_lst)):\n",
        "        d = d[d[names[i]] == group_lst[i]]\n",
        "        # print(len(d))\n",
        "    total =  d['cnt'].sum()\n",
        "    # Total here was 0: here, errors when this is commented out\n",
        "    if total == 0:\n",
        "      return -1\n",
        "    d = d[d[label] == 1]\n",
        "    pos = d['cnt'].sum()\n",
        "    # print(d, group_lst)\n",
        "    neg = total - pos\n",
        "    result = get_one_degree_neighbors(temp2,names, group_lst)\n",
        "    neighbors = compute_neighbors(group_lst, result)\n",
        "    if(need_positive_or_negative == 1):\n",
        "        # need pos\n",
        "        x = Symbol('x')\n",
        "        # print(pos, neg , neighbors[2])\n",
        "        try:\n",
        "          diff = solve((pos + x)/ (neg - x) - neighbors[2])[0]\n",
        "        except:\n",
        "          return -1\n",
        "        # print(solve((pos + x)/ (neg - x) - neighbors[2]))\n",
        "    else:\n",
        "        #need negative\n",
        "        x = Symbol('x')\n",
        "        try:\n",
        "          diff = solve((pos - x)/ (neg + x) - neighbors[2])[0]\n",
        "        except:\n",
        "          return -1\n",
        "    # print(neighbors[2],pos, neg, diff)\n",
        "    return diff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TH7YGCuh69D7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c224994c-d6e9-4440-c554-b4780dfbd84c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "def compute_diff_add(group_lst, temp2, names, label_y, need_positive_or_negative):\n",
        "    # print(\"compute diff\")\n",
        "    d = copy.copy(temp2)\n",
        "    # print(\"here\", d, names)\n",
        "    for i in range(len(group_lst)):\n",
        "        # print(d, group_lst[i])\n",
        "        d = d[d[names[i]] == group_lst[i]]\n",
        "    total =  d['cnt'].sum()\n",
        "    d = d[d[label_y] == 1]\n",
        "    pos = d['cnt'].sum()\n",
        "    neg = total - pos\n",
        "    result = get_one_degree_neighbors(temp2, names, group_lst)\n",
        "    neighbors = compute_neighbors(group_lst, result)\n",
        "    if(need_positive_or_negative == 1):\n",
        "        # need pos\n",
        "        # if neg == 0:\n",
        "        #     # to avoid zero neg\n",
        "        #     neg = 1\n",
        "        x = Symbol('x')\n",
        "        try:\n",
        "          diff = solve((pos + x)/ neg -  neighbors[2])[0]\n",
        "        except:\n",
        "          return -1\n",
        "        # enough = neighbors[2] * neg\n",
        "        # diff = round_int(enough - pos)\n",
        "        print(neighbors[2], pos, neg, diff)\n",
        "    else:\n",
        "        #need negative\n",
        "        # if(pos == 0):\n",
        "        #     diff  = 0\n",
        "        #     enough = 0\n",
        "        # else:\n",
        "        # enough = pos / (neighbors[2] )\n",
        "        x = Symbol('x')\n",
        "        try:\n",
        "          diff = solve(pos/ (neg + x) -  neighbors[2])[0]\n",
        "        except:\n",
        "          return -1\n",
        "    print(neighbors[2], pos, neg, diff)\n",
        "    return diff\n",
        "\n",
        "def compute_diff_remove(group_lst, temp2, names, label_y, need_positive_or_negative):\n",
        "    d = copy.copy(temp2)\n",
        "    for i in range(len(group_lst)):\n",
        "        # print(d, group_lst[i])\n",
        "        d = d[d[names[i]] == group_lst[i]]\n",
        "    total =  d['cnt'].sum()\n",
        "    d = d[d[label_y] == 1]\n",
        "    pos = d['cnt'].sum()\n",
        "    neg = total - pos\n",
        "    result = get_one_degree_neighbors(temp2, names, group_lst)\n",
        "    neighbors = compute_neighbors(group_lst, result)\n",
        "    if(need_positive_or_negative == 1):\n",
        "        # need pos, remove some neg\n",
        "        x = Symbol('x')\n",
        "        try:\n",
        "          diff = solve( pos/ (neg - x) -  neighbors[2])[0]\n",
        "        except:\n",
        "          return -1\n",
        "        # enough = neighbors[2] * neg\n",
        "        # diff = round_int(enough - pos)\n",
        "        print(neighbors[2], pos, neg, diff)\n",
        "    else:\n",
        "        #need negative\n",
        "        x = Symbol('x')\n",
        "        try:\n",
        "          diff = solve((pos -x )/ neg -  neighbors[2])[0]\n",
        "        except:\n",
        "          return -1\n",
        "        print(neighbors[2], pos, neg, diff)\n",
        "    return diff\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MLMEPsM39to1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf9e77e0-c189-436d-ab4d-ef07e3ba94c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from divexplorer.FP_DivergenceExplorer import FP_DivergenceExplorer\n",
        "\n",
        "def div_results(db, remedy, algo):\n",
        "  columns_compas.extend([compas_y, \"predicted\"])\n",
        "\n",
        "  df = pd.DataFrame(test_set, columns = columns_compas)\n",
        "  # print(test_set.columns)\n",
        "  # print(columns_compas)\n",
        "  # print(df.columns)\n",
        "  # print(columns_compas)\n",
        "\n",
        "  columns_compas.remove(compas_y)\n",
        "  columns_compas.remove('predicted')\n",
        "  class_map={'N': 0, 'P': 1}\n",
        "\n",
        "  min_sup=0.1\n",
        "\n",
        "  # min_sup = 0.05\n",
        "\n",
        "  fp_diver=FP_DivergenceExplorer(df,compas_y, \"predicted\", class_map=class_map)\n",
        "  FP_fm=fp_diver.getFrequentPatternDivergence(min_support=min_sup, metrics=[\"d_fpr\", \"d_fnr\", \"d_accuracy\"])\n",
        "  from divexplorer.FP_Divergence import FP_Divergence\n",
        "  fp_divergence_fpr=FP_Divergence(FP_fm, \"d_fpr\")\n",
        "  fp_divergence_fnr=FP_Divergence(FP_fm, \"d_fnr\")\n",
        "  fp_divergence_acc=FP_Divergence(FP_fm, \"d_accuracy\")\n",
        "  # fp_divergence_fpr=FP_Divergence(FP_fm, \"d_fnr\")\n",
        "  INFO_VIZ=[\"support\", \"itemsets\",  fp_divergence_fpr.metric, fp_divergence_fpr.t_value_col]\n",
        "  INFO_VIZ2=[\"support\", \"itemsets\",  fp_divergence_fnr.metric, fp_divergence_fnr.t_value_col]\n",
        "  INFO_VIZ3=[\"support\", \"itemsets\",  fp_divergence_acc.metric, fp_divergence_acc.t_value_col]\n",
        "\n",
        "  K=200\n",
        "  d = fp_divergence_fpr.getDivergence(th_redundancy=0)[INFO_VIZ].head(K)\n",
        "  # summerization\n",
        "  eps=0.01\n",
        "\n",
        "  d = fp_divergence_fpr.getDivergence(th_redundancy=eps)[INFO_VIZ].head(K)\n",
        "  d2 = fp_divergence_fnr.getDivergence(th_redundancy=eps)[INFO_VIZ2].head(K)\n",
        "  d3 = fp_divergence_acc.getDivergence(th_redundancy=eps)[INFO_VIZ3].head(K)\n",
        "\n",
        "  d= d[d['d_fpr'] > 0]\n",
        "  d2= d2[d2['d_fnr'] > 0]\n",
        "  d3= d3[d3['d_accuracy'] > 0]\n",
        "\n",
        "  dfpr = fairness_score_computation(d, 'd_fpr')\n",
        "  dfnr = fairness_score_computation(d2, 'd_fnr')\n",
        "  dacc = fairness_score_computation(d3, 'd_accuracy')\n",
        "  print(dfpr)\n",
        "  print(dfnr)\n",
        "  print(dacc)\n",
        "  accuracy = accuracy_score(test_label, test_set['predicted'])\n",
        "  print(\"accuracy is \" , accuracy)\n",
        "  writelist = [db,remedy,algo, dfpr, dfnr, dacc, accuracy]\n",
        "  with open('Adult_results.csv', 'a', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(writelist)\n",
        "  print(writelist)\n",
        "  return d,d2,d3"
      ],
      "metadata": {
        "id": "ZNtlejsp3sA_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98f25e5f-f261-42e9-e2ad-aecabe0ef2ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3J0EcLGqrH0-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff6b4be0-f830-49ce-9ef2-58d260064958"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "# def div_results():\n",
        "#   class_map={'N': 0, 'P': 1}\n",
        "#   from divexplorer.FP_DivergenceExplorer import FP_DivergenceExplorer\n",
        "\n",
        "#   #min_sup=0.1\n",
        "\n",
        "#   min_sup = 0.05\n",
        "\n",
        "#   fp_diver=FP_DivergenceExplorer(test_set,compas_y, \"predicted\", class_map=class_map)\n",
        "#   FP_fm=fp_diver.getFrequentPatternDivergence(min_support=min_sup, metrics=[\"d_fpr\", \"d_fnr\", \"d_accuracy\"])\n",
        "#   from divexplorer.FP_Divergence import FP_Divergence\n",
        "#   fp_divergence_fpr=FP_Divergence(FP_fm, \"d_fpr\")\n",
        "#   fp_divergence_fnr=FP_Divergence(FP_fm, \"d_fnr\")\n",
        "#   fp_divergence_acc=FP_Divergence(FP_fm, \"d_accuracy\")\n",
        "#   # fp_divergence_fpr=FP_Divergence(FP_fm, \"d_fnr\")\n",
        "#   INFO_VIZ=[\"support\", \"itemsets\",  fp_divergence_fpr.metric, fp_divergence_fpr.t_value_col]\n",
        "#   INFO_VIZ2=[\"support\", \"itemsets\",  fp_divergence_fnr.metric, fp_divergence_fnr.t_value_col]\n",
        "#   INFO_VIZ3=[\"support\", \"itemsets\",  fp_divergence_acc.metric, fp_divergence_acc.t_value_col]\n",
        "\n",
        "#   K=10\n",
        "#   pd.options.display.max_rows = 200\n",
        "#   d = fp_divergence_fpr.getDivergence(th_redundancy=0)[INFO_VIZ].head(K)\n",
        "#   # summerization\n",
        "#   eps=0.01\n",
        "\n",
        "#   d = fp_divergence_fpr.getDivergence(th_redundancy=eps)[INFO_VIZ].head(K)\n",
        "#   d2 = fp_divergence_fnr.getDivergence(th_redundancy=eps)[INFO_VIZ2].head(K)\n",
        "#   d3 = fp_divergence_acc.getDivergence(th_redundancy=eps)[INFO_VIZ3].head(K)\n",
        "\n",
        "#   print(fairness_score_computation(d, 'd_fpr'))\n",
        "#   print(fairness_score_computation(d2, 'd_fnr'))\n",
        "#   print(fairness_score_computation(d3, 'd_accuracy'))\n",
        "#   accuracy = accuracy_score(test_label, test_predict)\n",
        "#   print(\"accuracy is \" , accuracy)\n",
        "#   return d,d2,d3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iEQ83nxkQr4"
      },
      "source": [
        "## Optimized Helper Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuxfZHSlhXNR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80bc123d-6348-4fc9-a5e9-74a8fea6499b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "# helper function for optimized\n",
        "def compute_neighbors_opt(group_lst,lst_of_counts, pos, neg):\n",
        "    #start2 = time.time()\n",
        "    times = len(group_lst)\n",
        "    pos_cnt = 0\n",
        "    neg_cnt = 0\n",
        "    for i in range(times):\n",
        "        df_groupby = lst_of_counts[i]\n",
        "        temp_group_lst_pos = copy.copy(group_lst)\n",
        "        temp_group_lst_neg = copy.copy(group_lst)\n",
        "        del temp_group_lst_pos[i]\n",
        "        del temp_group_lst_neg[i]\n",
        "        # count positive\n",
        "        temp_group_lst_pos.append(1)\n",
        "        group_tuple_pos = tuple(temp_group_lst_pos)\n",
        "        if group_tuple_pos in df_groupby.keys():\n",
        "            pos_cnt += df_groupby[group_tuple_pos]\n",
        "        else:\n",
        "            pos_cnt += 0\n",
        "        # count negative\n",
        "        temp_group_lst_neg.append(0)\n",
        "        group_tuple_neg = tuple(temp_group_lst_neg)\n",
        "        if group_tuple_neg in df_groupby.keys():\n",
        "            neg_cnt += df_groupby[group_tuple_neg]\n",
        "        else:\n",
        "            neg_cnt += 0\n",
        "    pos_val = pos_cnt - times* pos\n",
        "    neg_val = neg_cnt - times* neg\n",
        "    #end2 = time.time()\n",
        "    #print(\"The time to compute the neighbor counts for \" +  str(group_lst) +\" is \" + str(end2-start2))\n",
        "    if neg_val == -1 or (neg_val == 0 and pos_val == 0):\n",
        "        return (pos_val, neg_val, -1)\n",
        "    if pos_val == 0 or neg_val == 0:\n",
        "        return (pos_val, neg_val, 0)\n",
        "    # print(\"here\", pos_val, neg_val, pos_val/neg_val)\n",
        "\n",
        "    return (pos_val, neg_val, pos_val/neg_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8Iny4kJhm4V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "244534df-7795-4187-9db2-5997ddecb349"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "# get the list of neighbors\n",
        "def get_one_degree_neighbors_opt(group_lst):\n",
        "    start1 = time.time()\n",
        "    result = []\n",
        "    for i in range(len(group_lst)):\n",
        "        d = copy.copy(group_lst)\n",
        "        d[i] = 'x'\n",
        "        result.append(d)\n",
        "    end1 = time.time()\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MmwA0X5hofE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1bd24ee-575c-4014-f9bd-ee5fcf92359e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "def determine_problematic_opt(group_lst, names, temp2, lst_of_counts, label, threshold= 0.2):\n",
        "    #0: ok group, 1: need negative records, 2: need positive records\n",
        "    d = copy.copy(temp2)\n",
        "    for i in range(len(group_lst)):\n",
        "        d = d[d[names[i]] == group_lst[i]]\n",
        "    total =  d['cnt'].sum()\n",
        "    d = d[d[label] == 1]\n",
        "    pos = d['cnt'].sum()\n",
        "    neg = total - pos\n",
        "    neighbors = compute_neighbors_opt(group_lst,lst_of_counts, pos, neg)\n",
        "    if(neighbors[2] == -1):\n",
        "        # there is no neighbors\n",
        "        return 0\n",
        "    if(total > 30):\n",
        "        # need to be large enough, need to adjust with different datasets.\n",
        "        if neg == 0:\n",
        "            if (pos > neighbors[2]):\n",
        "                return 1\n",
        "            if(pos <= neighbors[2]):\n",
        "                return 0\n",
        "        if (pos/(neg) - neighbors[2] > threshold):\n",
        "            # too many positive records\n",
        "            return 1\n",
        "        if (neighbors[2] - pos/(neg) > threshold):\n",
        "            return 2\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A76VlG-bhqqS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f944bc80-2260-4465-99f7-be4b2613f3f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "def compute_problematic_opt(temp2, temp_g, names, label, lst_of_counts):\n",
        "    need_pos = []\n",
        "    need_neg = []\n",
        "    for index, row in temp_g.iterrows():\n",
        "        group_lst = []\n",
        "        for n in names:\n",
        "            group_lst.append(row[n])\n",
        "        problematic = determine_problematic_opt(group_lst, names, temp2, lst_of_counts,label)\n",
        "#         #print(problematic)\n",
        "        if(problematic == 1):\n",
        "            if group_lst not in need_neg:\n",
        "                need_neg.append(group_lst)\n",
        "        if(problematic == 2):\n",
        "            if group_lst not in need_pos:\n",
        "                need_pos.append(group_lst)\n",
        "    return need_pos, need_neg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZd2c6PAhtTH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a3aca18-c654-4337-c779-cab16f739c84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "# build the list of X00\n",
        "def compute_lst_of_counts(temp, names, label):\n",
        "    # get the list of group-by attributes\n",
        "    lst_of_counts = []\n",
        "    for i in range(len(names)):\n",
        "        grp_names = copy.copy(names)\n",
        "        del grp_names[i]\n",
        "        grp_names.append(label)\n",
        "        temp_df = temp.groupby(grp_names)['cnt'].count()\n",
        "        lst_of_counts.append(temp_df)\n",
        "    return lst_of_counts\n",
        "\n",
        "def get_tuple(group_lst):\n",
        "    return tuple(group_lst)\n",
        "\n",
        "\n",
        "def get_temp_g(train_set, names, y_label):\n",
        "  names2 = copy.deepcopy(names)\n",
        "  names2.append(y_label)\n",
        "  temp = train_set[names2]\n",
        "  temp['cnt'] = 0\n",
        "  temp_g = temp.groupby(names)['cnt'].count().reset_index()\n",
        "  return temp, temp_g"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnQ09ffAkalO"
      },
      "source": [
        "# Preferential Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2L4D0kJ7h2lS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f24c7017-c53f-4ed3-e2d5-e0114c20228a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "# from sklearn.naive_bayes import MultinomialNB\n",
        "# def pref_sampling_opt(train_set, cols_given, label, need_pos, need_neg):\n",
        "#     if len(need_pos)+ len(need_neg) > 0:\n",
        "#         temp_train_x = pd.DataFrame(train_set, columns = columns_compas)\n",
        "#         temp_train_label = pd.DataFrame(train_set, columns = [label])\n",
        "#         temp_train_label = temp_train_label[label]\n",
        "#         temp_train_label = temp_train_label.astype('int')\n",
        "#         mnb = MultinomialNB()\n",
        "#         mnb = mnb.fit(temp_train_x, temp_train_label)\n",
        "#         probs = mnb.predict_proba(temp_train_x)[:,0]\n",
        "#         train_set[\"prob\"] = abs(probs - 0.5)\n",
        "#         # get the set of\n",
        "#     new_train_set = pd.DataFrame(columns = list(train_set.columns))\n",
        "#     updated_pos = 0\n",
        "#     for i in need_pos:\n",
        "#         # needs to updated more positive records\n",
        "#         # print(i)\n",
        "#         temp_df = copy.deepcopy(train_set)\n",
        "#         for n in range(len(i)):\n",
        "#           temp_df = temp_df[temp_df[cols_given[n]] == i[n]]\n",
        "#         # update the skew and diff\n",
        "#         idx = list(temp_df.index)\n",
        "#         idx_pos = list(temp_df[(getattr(temp_df, label) == 1)].index)\n",
        "#         if(len(idx_pos) == 0):\n",
        "#             continue\n",
        "#         idx_neg = list(temp_df[(getattr(temp_df, label) == 0)].index)\n",
        "#         pos_ranked = train_set.loc[idx_pos].sort_values(by=\"prob\", ascending=True)\n",
        "#         neg_ranked = train_set.loc[idx_neg].sort_values(by=\"prob\", ascending=True)\n",
        "#         train_set.loc[idx, 'skewed'] = 1\n",
        "#         diff = compute_diff_add_and_remove(i, temp2,  1, compas_y, names)\n",
        "#         train_set.loc[idx, 'diff'] = int(diff)\n",
        "#         cnt = int(train_set.loc[idx_pos[0]][\"diff\"])\n",
        "#         updated_pos += cnt * 2\n",
        "#         # add more records when there are not enough available records\n",
        "#         new_train_set = pd.concat([new_train_set, pos_ranked], ignore_index=True)\n",
        "#         temp_cnt = cnt\n",
        "#         if len(pos_ranked) >= temp_cnt:\n",
        "#             new_train_set = pd.concat([new_train_set,pos_ranked[0:cnt]], ignore_index=True)\n",
        "#         else:\n",
        "#             while(temp_cnt > 0 ):\n",
        "#                 new_train_set = pd.concat([new_train_set,pos_ranked[0:temp_cnt]], ignore_index=True)\n",
        "#             # duplicate the dataframe\n",
        "#                 temp_cnt = temp_cnt - len(pos_ranked)\n",
        "#         # duplicate the top cnt records from the pos\n",
        "#         # remove the top cnt records from the neg\n",
        "#         new_train_set = pd.concat([new_train_set, neg_ranked[cnt:-1]], ignore_index=True)\n",
        "#     print(\"updated {} positive records\".format(str(updated_pos)))\n",
        "#     updated_neg = 0\n",
        "#     # adding more records to the need_neg set\n",
        "#     for i in need_neg:\n",
        "#         # print(i)\n",
        "#         # list of idx belongs to this group\n",
        "#         temp_df = copy.deepcopy(train_set)\n",
        "#         for n in range(len(i)):\n",
        "#           temp_df = temp_df[temp_df[cols_given[n]] == i[n]]\n",
        "#         # update the skew and diff\n",
        "#         idx = list(temp_df.index)\n",
        "#         idx_pos = list(temp_df[(getattr(temp_df, label) == 1)].index)\n",
        "#         idx_neg = list(temp_df[(getattr(temp_df, label) == 0)].index)\n",
        "#         if(len(idx_neg) == 0):\n",
        "#             continue\n",
        "#         pos_ranked = train_set.loc[idx_pos].sort_values(by=\"prob\", ascending=True)\n",
        "#         neg_ranked = train_set.loc[idx_neg].sort_values(by=\"prob\", ascending=True)\n",
        "#         train_set.loc[idx, 'skewed'] = 1\n",
        "#         diff = compute_diff_add_and_remove(i, temp2, 0, compas_y, names)\n",
        "#         train_set.loc[idx, 'diff'] = int(diff)\n",
        "#         cnt = int(train_set.loc[idx_pos[0]][\"diff\"])\n",
        "#         updated_neg += cnt * 2\n",
        "#         # add more records when there are not enough available records\n",
        "#         new_train_set = pd.concat([new_train_set, neg_ranked], ignore_index=True)\n",
        "    #     temp_cnt = cnt\n",
        "    #     if len(neg_ranked) >= temp_cnt:\n",
        "    #         new_train_set = pd.concat([new_train_set,neg_ranked[0:cnt]], ignore_index=True)\n",
        "    #     else:\n",
        "    #         while(temp_cnt > 0 ):\n",
        "    #             new_train_set = pd.concat([new_train_set,neg_ranked[0:temp_cnt]], ignore_index=True)\n",
        "    #         # duplicate the dataframe\n",
        "    #             temp_cnt = temp_cnt - len(neg_ranked)\n",
        "    #     # duplicate the top cnt records from the pos\n",
        "    #     # remove the top cnt records from the neg\n",
        "    #     new_train_set = pd.concat([new_train_set, pos_ranked[cnt:-1]], ignore_index=True)\n",
        "    #     #print(len(new_train_set[new_train_set['income'] == 1]), len(new_train_set[new_train_set['income'] == 0]))\n",
        "    #     # print(train_set.loc[idx_neg])\n",
        "    # print(\"updated {} negative records\".format(str(updated_neg)))\n",
        "    # # add the other irrelavant items:\n",
        "    # idx_irr = list(train_set[train_set['skewed'] == 0].index)\n",
        "    # irr_df = train_set.loc[idx_irr]\n",
        "    # new_train_set = pd.concat([new_train_set, irr_df], ignore_index=True)\n",
        "    # print(\"The new dataset contains {} rows.\".format(str(len(new_train_set))))\n",
        "    # new_train_set.reset_index()\n",
        "    # return new_train_set\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wq3iEkLMTluh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb3853e3-d167-4578-e2e3-1b090c947cbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "def pref_sampling_opt(train_set, cols_given, label, need_pos, need_neg):\n",
        "    if len(need_pos)+ len(need_neg) > 0:\n",
        "        temp_train_x = pd.DataFrame(train_set, columns = columns_all)\n",
        "        temp_train_label = pd.DataFrame(train_set, columns = [label])\n",
        "        temp_train_label = temp_train_label[label]\n",
        "        temp_train_label = temp_train_label.astype('int')\n",
        "        mnb = MultinomialNB()\n",
        "        mnb = mnb.fit(temp_train_x, temp_train_label)\n",
        "        probs = mnb.predict_proba(temp_train_x)[:,0]\n",
        "        train_set[\"prob\"] = abs(probs - 0.5)\n",
        "        # get the set of\n",
        "    new_train_set = pd.DataFrame(columns = list(train_set.columns))\n",
        "    updated_pos = 0\n",
        "    for i in need_pos:\n",
        "        # needs to updated more positive records\n",
        "        # print(i)\n",
        "        temp_df = copy.deepcopy(train_set)\n",
        "        for n in range(len(i)):\n",
        "          temp_df = temp_df[temp_df[cols_given[n]] == i[n]]\n",
        "        # update the skew and diff\n",
        "        idx = list(temp_df.index)\n",
        "        train_set.loc[idx, 'skewed'] = 1\n",
        "        idx_pos = list(temp_df[(getattr(temp_df, label) == 1)].index)\n",
        "        if(len(idx_pos) == 0):\n",
        "          # if there is no positive\n",
        "          idx_neg = list(temp_df[(getattr(temp_df, label) == 0)].index)\n",
        "          neg_ranked = train_set.loc[idx_neg].sort_values(by=\"prob\", ascending=True)\n",
        "          new_train_set = pd.concat([new_train_set, neg_ranked], ignore_index=True)\n",
        "          continue\n",
        "        idx_neg = list(temp_df[(getattr(temp_df, label) == 0)].index)\n",
        "        pos_ranked = train_set.loc[idx_pos].sort_values(by=\"prob\", ascending=True)\n",
        "        neg_ranked = train_set.loc[idx_neg].sort_values(by=\"prob\", ascending=True)\n",
        "        diff = compute_diff_add_and_remove(i, temp2,  1, compas_y, names)\n",
        "        if diff == -1:\n",
        "          new_train_set = pd.concat([new_train_set, pos_ranked], ignore_index=True)\n",
        "          new_train_set = pd.concat([new_train_set, neg_ranked], ignore_index=True)\n",
        "          continue\n",
        "        train_set.loc[idx, 'diff'] = int(diff)\n",
        "        cnt = int(train_set.loc[idx_pos[0]][\"diff\"])\n",
        "        updated_pos += cnt * 2\n",
        "        # add more records when there are not enough available records\n",
        "        new_train_set = pd.concat([new_train_set, pos_ranked], ignore_index=True)\n",
        "        temp_cnt = cnt\n",
        "        if len(pos_ranked) >= temp_cnt:\n",
        "            new_train_set = pd.concat([new_train_set,pos_ranked[0:cnt]], ignore_index=True)\n",
        "        else:\n",
        "            while(temp_cnt > 0 ):\n",
        "                new_train_set = pd.concat([new_train_set,pos_ranked[0:temp_cnt]], ignore_index=True)\n",
        "            # duplicate the dataframe\n",
        "                temp_cnt = temp_cnt - len(pos_ranked)\n",
        "        # duplicate the top cnt records from the pos\n",
        "        # remove the top cnt records from the neg\n",
        "        if cnt == 0:\n",
        "          new_train_set = pd.concat([new_train_set, neg_ranked], ignore_index=True)\n",
        "          # print(\"+++++++++++++++++++++++++++++++++++++++++++++\")\n",
        "          # print(i)\n",
        "          # print(len(pos_ranked)+cnt)\n",
        "          # print(len(neg_ranked))\n",
        "        else:\n",
        "          new_train_set = pd.concat([new_train_set, neg_ranked[cnt-1:-1]], ignore_index=True)\n",
        "          # print(\"+++++++++++++++++++++++++++++++++++++++++++++\")\n",
        "          # print(i)\n",
        "          # print(len(pos_ranked)+cnt)\n",
        "          # print(len(neg_ranked[cnt-1:-1]))\n",
        "    print(\"updated {} positive records\".format(str(updated_pos)))\n",
        "    updated_neg = 0\n",
        "    # adding more records to the need_neg set\n",
        "    for i in need_neg:\n",
        "        # print(i)\n",
        "        # list of idx belongs to this group\n",
        "        temp_df = copy.deepcopy(train_set)\n",
        "        for n in range(len(i)):\n",
        "          temp_df = temp_df[temp_df[cols_given[n]] == i[n]]\n",
        "        # update the skew and diff\n",
        "        idx = list(temp_df.index)\n",
        "        train_set.loc[idx, 'skewed'] = 1\n",
        "        idx_pos = list(temp_df[(getattr(temp_df, label) == 1)].index)\n",
        "        idx_neg = list(temp_df[(getattr(temp_df, label) == 0)].index)\n",
        "        if(len(idx_neg) == 0):\n",
        "          pos_ranked = train_set.loc[idx_pos].sort_values(by=\"prob\", ascending=True)\n",
        "          new_train_set = pd.concat([new_train_set, pos_ranked], ignore_index=True)\n",
        "          continue\n",
        "        pos_ranked = train_set.loc[idx_pos].sort_values(by=\"prob\", ascending=True)\n",
        "        neg_ranked = train_set.loc[idx_neg].sort_values(by=\"prob\", ascending=True)\n",
        "        diff = compute_diff_add_and_remove(i, temp2, 0, compas_y, names)\n",
        "        if diff == -1:\n",
        "          new_train_set = pd.concat([new_train_set, neg_ranked], ignore_index=True)\n",
        "          new_train_set = pd.concat([new_train_set, pos_ranked], ignore_index=True)\n",
        "          continue\n",
        "        train_set.loc[idx, 'diff'] = int(diff)\n",
        "        cnt = int(train_set.loc[idx_pos[0]][\"diff\"])\n",
        "        updated_neg += cnt * 2\n",
        "        # add more records when there are not enough available records\n",
        "        new_train_set = pd.concat([new_train_set, neg_ranked], ignore_index=True)\n",
        "        temp_cnt = cnt\n",
        "        if len(neg_ranked) >= temp_cnt:\n",
        "            new_train_set = pd.concat([new_train_set,neg_ranked[0:cnt]], ignore_index=True)\n",
        "        else:\n",
        "            while(temp_cnt > 0 ):\n",
        "                new_train_set = pd.concat([new_train_set,neg_ranked[0:temp_cnt]], ignore_index=True)\n",
        "            # duplicate the dataframe\n",
        "                temp_cnt = temp_cnt - len(neg_ranked)\n",
        "        # duplicate the top cnt records from the pos\n",
        "        # remove the top cnt records from the neg\n",
        "        if cnt ==0:\n",
        "          new_train_set = pd.concat([new_train_set, pos_ranked], ignore_index=True)\n",
        "          # print(\"+++++++++++++++++++++++++++++++++++++++++++++\")\n",
        "          # print(i)\n",
        "          # print(len(neg_ranked)+cnt)\n",
        "          # print(len(pos_ranked))\n",
        "        else:\n",
        "          new_train_set = pd.concat([new_train_set, pos_ranked[cnt-1:-1]], ignore_index=True)\n",
        "          # print(\"+++++++++++++++++++++++++++++++++++++++++++++\")\n",
        "          # print(i)\n",
        "          # print(len(neg_ranked)+cnt)\n",
        "          # print(len(pos_ranked[cnt-1:-1]))\n",
        "        #print(len(new_train_set[new_train_set['income'] == 1]), len(new_train_set[new_train_set['income'] == 0]))\n",
        "        # print(train_set.loc[idx_neg])\n",
        "    print(\"updated {} negative records\".format(str(updated_neg)))\n",
        "    # add the other irrelavant items:\n",
        "    idx_irr = list(train_set[train_set['skewed'] == 0].index)\n",
        "    irr_df = train_set.loc[idx_irr]\n",
        "    new_train_set = pd.concat([new_train_set, irr_df], ignore_index=True)\n",
        "    print(\"The new dataset contains {} rows.\".format(str(len(new_train_set))))\n",
        "    new_train_set.reset_index()\n",
        "    return new_train_set\n",
        "\n"
      ],
      "metadata": {
        "id": "LCr5MK4oTmKI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d6db941-7d6f-4b50-daaa-164732cb3b7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMrjlEciiv--",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e98236e-39f5-4429-ca1e-735437c06eec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paqO9W2jeTMX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8307bf53-09d2-462e-ed4a-ffc866f0890c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "# unfair_group, unfair_names, skew_candidates, unfair_dict = get_unfair_group([])\n",
        "# print(unfair_group, unfair_names, skew_candidates, unfair_dict)\n",
        "# combos_dict, all_names = candidate_groups(skew_candidates, unfair_dict, columns_compas, unfair_names)\n",
        "# print(combos_dict, all_names)\n",
        "\n",
        "# # the\n",
        "# print(list(all_names.keys())[len(columns_compas)+1:])\n",
        "# all_names_lst = list(all_names.keys())[len(columns_compas)+1:]\n",
        "# all_names_lst.reverse()\n",
        "# print(all_names_lst)\n",
        "\n",
        "def find_top(all_names):\n",
        "  all_names_lst_top = []\n",
        "  for all in range(len(all_names)):\n",
        "    if len(all_names[all]) == 1: # CHANGED HERE\n",
        "      all_names_lst_top.append(all)\n",
        "  return all_names_lst_top"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGdoCnWLlkc4"
      },
      "source": [
        "## Run Algorithm Lattice\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfMMRp-S4vTL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76ea8573-a40b-45fb-b844-c65ebb7fc0d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/////////////\n",
            "7\n",
            "The sets of need pos and neg are\n",
            "[[0, 4, 1], [1, 0, 1], [1, 2, 1], [1, 3, 1]]\n",
            "[[1, 4, 1], [2, 4, 1]]\n",
            "started pref sampling\n",
            "updated 2676 positive records\n",
            "updated 4548 negative records\n",
            "The new dataset contains 31656 rows.\n",
            "0    24710\n",
            "1     6946\n",
            "Name: income, dtype: int64\n",
            "/////////////\n",
            "6\n",
            "The sets of need pos and neg are\n",
            "[]\n",
            "[]\n",
            "started pref sampling\n",
            "updated 0 positive records\n",
            "updated 0 negative records\n",
            "The new dataset contains 31656 rows.\n",
            "0    24710\n",
            "1     6946\n",
            "Name: income, dtype: int64\n",
            "/////////////\n",
            "5\n",
            "The sets of need pos and neg are\n",
            "[[0, 0]]\n",
            "[]\n",
            "started pref sampling\n",
            "updated 1500 positive records\n",
            "updated 0 negative records\n",
            "The new dataset contains 31656 rows.\n",
            "0    23960\n",
            "1     7696\n",
            "Name: income, dtype: int64\n",
            "/////////////\n",
            "4\n",
            "The sets of need pos and neg are\n",
            "[[0, 0], [0, 2], [0, 3], [2, 0]]\n",
            "[[1, 1], [2, 1]]\n",
            "started pref sampling\n",
            "updated 464 positive records\n",
            "updated 180 negative records\n",
            "The new dataset contains 31656 rows.\n",
            "0    23818\n",
            "1     7838\n",
            "Name: income, dtype: int64\n",
            "/////////////\n",
            "3\n",
            "The sets of need pos and neg are\n",
            "[]\n",
            "[]\n",
            "started pref sampling\n",
            "updated 0 positive records\n",
            "updated 0 negative records\n",
            "The new dataset contains 31656 rows.\n",
            "0    23818\n",
            "1     7838\n",
            "Name: income, dtype: int64\n",
            "/////////////\n",
            "2\n",
            "The sets of need pos and neg are\n",
            "[]\n",
            "[]\n",
            "started pref sampling\n",
            "updated 0 positive records\n",
            "updated 0 negative records\n",
            "The new dataset contains 31656 rows.\n",
            "0    23818\n",
            "1     7838\n",
            "Name: income, dtype: int64\n",
            "/////////////\n",
            "1\n",
            "The sets of need pos and neg are\n",
            "[]\n",
            "[]\n",
            "started pref sampling\n",
            "updated 0 positive records\n",
            "updated 0 negative records\n",
            "The new dataset contains 31656 rows.\n",
            "0    23818\n",
            "1     7838\n",
            "Name: income, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#get all of the candidate groups possible with the combos and names\n",
        "\n",
        "new_train_data = copy.deepcopy(train_set)\n",
        "\n",
        "#iterate over all the names to get the temp2 df for each name\n",
        "for a in all_names_lst:\n",
        "# for a in [all_names_lst[0]]: # leaf\n",
        "# for a in [all_names_lst[-1:x]]: # top\n",
        "  print(\"/////////////\")\n",
        "  print(a)\n",
        "  temp2, names = get_temp(new_train_data, all_names[a], compas_y)\n",
        "  temp, temp_g = get_temp_g(new_train_data, names, compas_y)\n",
        "  temp_g = temp_g[temp_g['cnt'] > filter_count]\n",
        "  lst_of_counts = compute_lst_of_counts(temp, names, compas_y)\n",
        "  # start = time.time()\n",
        "  need_pos, need_neg = compute_problematic_opt(temp2, temp_g, names, compas_y, lst_of_counts)\n",
        "  # end = time.time()\n",
        "  # excute_time = end - start\n",
        "  # print(\"The time to compute unfair group is {}\".format(str(excute_time)))\n",
        "  print(\"The sets of need pos and neg are\")\n",
        "  print(need_pos)\n",
        "  print(need_neg)\n",
        "  new_train_data['skewed'] = 0\n",
        "  new_train_data[\"diff\"] = 0\n",
        "  print(\"started pref sampling\")\n",
        "  new_train_data = pref_sampling_opt(new_train_data, names, compas_y, need_pos, need_neg)\n",
        "  # print(new_train_data)\n",
        "  print(new_train_data[compas_y].value_counts())\n",
        "new_train_x = pd.DataFrame(new_train_data, columns = columns_all)\n",
        "new_train_label = pd.DataFrame(new_train_data, columns = [compas_y])\n",
        "new_train_label = new_train_label[compas_y]\n",
        "new_train_label = new_train_label.astype('int')\n",
        "# grid_new = GridSearchCV(tree.DecisionTreeClassifier(), param_grid = param, cv = 6)\n",
        "# grid_new.fit(new_train_x, new_train_label)\n",
        "# print(\"model accuracy\")\n",
        "# print(\"best\", grid_new.best_score_)\n",
        "# test_predict = grid_new.predict(test_x)\n",
        "# print(\"fpr and fnr\")\n",
        "#   # print(fpr_onegroup(new_labels.tolist(), predict))\n",
        "#   # print(fnr_onegroup(new_labels.tolist(), predict))\n",
        "# print(fpr_onegroup(list(test_label), test_predict))\n",
        "# print(fnr_onegroup(list(test_label), test_predict))\n",
        "#   # print(pref_sampling_mod(train_x, train_set, train_label, names, compas_y, need_pos, need_neg))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print()\n",
        "print(\"dt\")\n",
        "gridlg.fit(new_train_x, new_train_label)\n",
        "print(\"best\", gridlg.best_score_)\n",
        "\n",
        "test_predict = gridlg.predict(test_x)\n",
        "print(\"fpr and fnr\")\n",
        "print(fpr_onegroup(list(test_label), test_predict))\n",
        "print(fnr_onegroup(list(test_label), test_predict))\n",
        "accuracy = accuracy_score(test_label, test_set['predicted'])\n",
        "print(\"accuracy\")\n",
        "print(accuracy)\n",
        "test_set['predicted'] = test_predict\n",
        "r,r2,r3 = div_results(\"Adult\",\"Preferential Sampling-Lattice\",\"DT\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGeUp2jAwzzF",
        "outputId": "2612ba2d-50c8-4900-a5a7-50912007c675"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "dt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "25 fits failed out of a total of 50.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "25 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.81194711        nan 0.81589573        nan 0.81573775\n",
            "        nan 0.81580095        nan 0.81583253]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best 0.8158957273183634\n",
            "fpr and fnr\n",
            "0.069140625\n",
            "0.642513529765484\n",
            "accuracy\n",
            "0.813578062804069\n",
            "0.015334351324382734\n",
            "0.0366589097393514\n",
            "0.10187071950066214\n",
            "accuracy is  0.7902845348665782\n",
            "['Adult', 'Preferential Sampling-Lattice', 'DT', 0.015334351324382734, 0.0366589097393514, 0.10187071950066214, 0.7902845348665782]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/divexplorer/FP_DivergenceExplorer.py:489: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  fp = fp.append(row_root, ignore_index=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# run divexplorer to find unfair groups\n",
        "class_map={'N': 0, 'P': 1}\n",
        "from divexplorer.FP_DivergenceExplorer import FP_DivergenceExplorer\n",
        "\n",
        "\n",
        "######\n",
        "# Yin: by consideri the unfair subgroups, we only consider the subgroups given by the protected attributes.\n",
        "\n",
        "columns_compas.extend([compas_y, \"predicted\"])\n",
        "\n",
        "df = pd.DataFrame(test_set, columns = columns_compas)\n",
        "# print(test_set.columns)\n",
        "# print(columns_compas)\n",
        "# print(df.columns)\n",
        "# print(columns_compas)\n",
        "\n",
        "columns_compas.remove(compas_y)\n",
        "columns_compas.remove('predicted')\n",
        "\n",
        "######\n",
        "\n",
        "min_sup=0.1\n",
        "# min_sup = 0.05\n",
        "# min_sup = 0.01\n",
        "\n",
        "fp_diver=FP_DivergenceExplorer(df,compas_y, \"predicted\", class_map=class_map)\n",
        "FP_fm=fp_diver.getFrequentPatternDivergence(min_support=min_sup, metrics=[\"d_fpr\", \"d_fnr\", \"d_accuracy\"])\n",
        "from divexplorer.FP_Divergence import FP_Divergence\n",
        "fp_divergence_fpr=FP_Divergence(FP_fm, \"d_fpr\")\n",
        "fp_divergence_fnr=FP_Divergence(FP_fm, \"d_fnr\")\n",
        "fp_divergence_acc=FP_Divergence(FP_fm, \"d_accuracy\")\n",
        "  # fp_divergence_fpr=FP_Divergence(FP_fm, \"d_fnr\")\n",
        "INFO_VIZ=[\"support\", \"itemsets\",  fp_divergence_fpr.metric, fp_divergence_fpr.t_value_col]\n",
        "INFO_VIZ2=[\"support\", \"itemsets\",  fp_divergence_fnr.metric, fp_divergence_fnr.t_value_col]\n",
        "INFO_VIZ3=[\"support\", \"itemsets\",  fp_divergence_acc.metric, fp_divergence_acc.t_value_col]\n",
        "eps=0.01\n",
        "K=1000\n",
        "d = fp_divergence_fpr.getDivergence(th_redundancy=eps)[INFO_VIZ].head(K)\n",
        "d2 = fp_divergence_fnr.getDivergence(th_redundancy=eps)[INFO_VIZ2].head(K)\n",
        "d3 = fp_divergence_acc.getDivergence(th_redundancy=eps)[INFO_VIZ3].head(K)\n",
        "\n",
        "\n",
        "\n",
        "pd.options.display.max_rows = 200\n",
        "d = fp_divergence_fpr.getDivergence(th_redundancy=0)[INFO_VIZ].head(K)\n",
        "# summerization\n",
        "\n",
        "d = fp_divergence_fpr.getDivergence(th_redundancy=eps)[INFO_VIZ].head(K)\n",
        "d= d[d['d_fpr'] > 0]\n",
        "d2= d2[d2['d_fnr'] > 0]\n",
        "d3= d3[d3['d_accuracy'] > 0]\n",
        "\n",
        "dfpr = fairness_score_computation(d, 'd_fpr')\n",
        "dfnr = fairness_score_computation(d2, 'd_fnr')\n",
        "dacc = fairness_score_computation(d3, 'd_accuracy')\n",
        "d, list(d[\"itemsets\"].iloc[0])\n",
        "# dacc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aF9omMjIzNkM",
        "outputId": "3b6bcb60-ee0d-44d3-ecf9-5cdbca120d42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/divexplorer/FP_DivergenceExplorer.py:489: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  fp = fp.append(row_root, ignore_index=True)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(    support           itemsets     d_fpr  t_value_fp\n",
              " 6  0.352425  (gender=1, age=1)  0.025030    4.218148\n",
              " 4  0.496978            (age=1)  0.013106    2.753566,\n",
              " ['gender=1', 'age=1'])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gerryfair\n",
        "\n",
        "auditor = gerryfair.model.Auditor(X_prime_test, y_test, 'FP')\n",
        "[violated_group, fairness_violation] = auditor.audit(predictions)\n",
        "fairness_violation\n"
      ],
      "metadata": {
        "id": "g3jU6nBfxN62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euVPjcb_lqe2"
      },
      "source": [
        "### Results Lattice"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# d,d1,d2 = div_results(\"Adult\",\"Preferential Sampling-Lattice\",\"DT\")\n",
        "\n",
        "print()\n",
        "print(\"dt\")\n",
        "griddt.fit(new_train_x, new_train_label)\n",
        "print(\"best\", griddt.best_score_)\n",
        "\n",
        "test_predict = griddt.predict(test_x)\n",
        "print(\"fpr and fnr\")\n",
        "print(fpr_onegroup(list(test_label), test_predict))\n",
        "print(fnr_onegroup(list(test_label), test_predict))\n",
        "accuracy = accuracy_score(test_label, test_set['predicted'])\n",
        "print(\"accuracy\")\n",
        "print(accuracy)\n",
        "test_set['predicted'] = test_predict\n",
        "r,r2,r3 = div_results(\"Adult\",\"Preferential Sampling-Lattice\",\"DT\")\n",
        "\n",
        "print()\n",
        "print(\"rf\")\n",
        "gridrf.fit(new_train_x, new_train_label)\n",
        "print(\"best\", gridrf.best_score_)\n",
        "test_predict = gridrf.predict(test_x)\n",
        "print(\"fpr and fnr\")\n",
        "print(fpr_onegroup(list(test_label), test_predict))\n",
        "print(fnr_onegroup(list(test_label), test_predict))\n",
        "accuracy = accuracy_score(test_label, test_set['predicted'])\n",
        "print(\"accuracy\")\n",
        "print(accuracy)\n",
        "test_set['predicted'] = test_predict\n",
        "r,r2,r3 = div_results(\"Adult\",\"Preferential Sampling-Lattice\",\"RF\")\n",
        "\n",
        "print()\n",
        "print(\"logistic\")\n",
        "gridlg.fit(new_train_x, new_train_label)\n",
        "print(\"best\", gridlg.best_score_)\n",
        "test_predict = gridlg.predict(test_x)\n",
        "print(\"fpr and fnr\")\n",
        "print(fpr_onegroup(list(test_label), test_predict))\n",
        "print(fnr_onegroup(list(test_label), test_predict))\n",
        "accuracy = accuracy_score(test_label, test_set['predicted'])\n",
        "print(\"accuracy\")\n",
        "print(accuracy)\n",
        "test_set['predicted'] = test_predict\n",
        "r,r2,r3 = div_results(\"Adult\",\"Preferential Sampling-Lattice\",\"LG\")\n",
        "\n",
        "print()\n",
        "print(\"svm\")\n",
        "clf.fit(new_train_x, new_train_label)\n",
        "# print(\"best\", svc.best_score_)\n",
        "test_predict = clf.predict(test_x)\n",
        "print(\"fpr and fnr\")\n",
        "print(fpr_onegroup(list(test_label), test_predict))\n",
        "print(fnr_onegroup(list(test_label), test_predict))\n",
        "accuracy = accuracy_score(test_label, test_set['predicted'])\n",
        "print(\"accuracy\")\n",
        "print(accuracy)\n",
        "test_set['predicted'] = test_predict\n",
        "s,s2,s3 = div_results(\"Adult\",\"Preferential Sampling-Lattice\",\"SVM\")"
      ],
      "metadata": {
        "id": "GDvLtC3BlDpa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4a1dc2f-99bf-4aab-a977-58ee615ebe51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "dt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best 0.8034808016345684\n",
            "fpr and fnr\n",
            "0.07705078125\n",
            "0.5378833433553818\n",
            "accuracy\n",
            "0.809671236915819\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/divexplorer/FP_DivergenceExplorer.py:489: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  fp = fp.append(row_root, ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0661633812697622\n",
            "0.27828020866353176\n",
            "0.0909356301316899\n",
            "accuracy is  0.8099660916998378\n",
            "['Adult', 'Preferential Sampling-Lattice', 'DT', 0.0661633812697622, 0.27828020866353176, 0.0909356301316899, 0.8099660916998378]\n",
            "\n",
            "rf\n",
            "best 0.771128050334999\n",
            "fpr and fnr\n",
            "0.0775390625\n",
            "0.5865904990980156\n",
            "accuracy\n",
            "0.8099660916998378\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/divexplorer/FP_DivergenceExplorer.py:489: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  fp = fp.append(row_root, ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.127547750281574\n",
            "0.04085320544997096\n",
            "0.12164786070856574\n",
            "accuracy is  0.7976559044670499\n",
            "['Adult', 'Preferential Sampling-Lattice', 'RF', 0.127547750281574, 0.04085320544997096, 0.12164786070856574, 0.7976559044670499]\n",
            "\n",
            "logistic\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "25 fits failed out of a total of 50.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "25 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.81949724        nan 0.82012897        nan 0.81899175\n",
            "        nan 0.81896016        nan 0.81896016]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best 0.8201289656238349\n",
            "fpr and fnr\n",
            "0.0482421875\n",
            "0.6575466025255562\n",
            "accuracy\n",
            "0.7976559044670499\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/divexplorer/FP_DivergenceExplorer.py:489: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  fp = fp.append(row_root, ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.03366016269431645\n",
            "0.12867891750334245\n",
            "0.11255427161359353\n",
            "accuracy is  0.8023735810113519\n",
            "['Adult', 'Preferential Sampling-Lattice', 'LG', 0.03366016269431645, 0.12867891750334245, 0.11255427161359353, 0.8023735810113519]\n",
            "\n",
            "svm\n",
            "fpr and fnr\n",
            "0.06533203125\n",
            "0.6975345760673481\n",
            "accuracy\n",
            "0.8023735810113519\n",
            "0.05942626517844015\n",
            "0.05482937317860928\n",
            "0.1470532322428726\n",
            "accuracy is  0.7796697626418989\n",
            "['Adult', 'Preferential Sampling-Lattice', 'SVM', 0.05942626517844015, 0.05482937317860928, 0.1470532322428726, 0.7796697626418989]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/divexplorer/FP_DivergenceExplorer.py:489: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  fp = fp.append(row_root, ignore_index=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWyPWk3RuP59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4609d77-18b7-4e77-fc4c-769abe203e52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "# run divexplorer to find unfair groups\n",
        "# class_map={'N': 0, 'P': 1}\n",
        "# from divexplorer.FP_DivergenceExplorer import FP_DivergenceExplorer\n",
        "\n",
        "# #min_sup=0.1\n",
        "\n",
        "# min_sup = 0.05\n",
        "\n",
        "# fp_diver=FP_DivergenceExplorer(test_set,compas_y, \"predicted\", class_map=class_map)\n",
        "# FP_fm=fp_diver.getFrequentPatternDivergence(min_support=min_sup, metrics=[\"d_fpr\", \"d_fnr\", \"d_accuracy\"])\n",
        "# from divexplorer.FP_Divergence import FP_Divergence\n",
        "# fp_divergence_fpr=FP_Divergence(FP_fm, \"d_fpr\")\n",
        "# fp_divergence_fnr=FP_Divergence(FP_fm, \"d_fnr\")\n",
        "# fp_divergence_acc=FP_Divergence(FP_fm, \"d_accuracy\")\n",
        "# # fp_divergence_fpr=FP_Divergence(FP_fm, \"d_fnr\")\n",
        "# INFO_VIZ=[\"support\", \"itemsets\",  fp_divergence_fpr.metric, fp_divergence_fpr.t_value_col]\n",
        "# INFO_VIZ2=[\"support\", \"itemsets\",  fp_divergence_fnr.metric, fp_divergence_fnr.t_value_col]\n",
        "# INFO_VIZ3=[\"support\", \"itemsets\",  fp_divergence_acc.metric, fp_divergence_acc.t_value_col]\n",
        "\n",
        "# K=10\n",
        "# pd.options.display.max_rows = 200\n",
        "# d = fp_divergence_fpr.getDivergence(th_redundancy=0)[INFO_VIZ].head(K)\n",
        "# # summerization\n",
        "# eps=0.01\n",
        "\n",
        "# d = fp_divergence_fpr.getDivergence(th_redundancy=eps)[INFO_VIZ].head(K)\n",
        "# d2 = fp_divergence_fnr.getDivergence(th_redundancy=eps)[INFO_VIZ2].head(K)\n",
        "# d3 = fp_divergence_acc.getDivergence(th_redundancy=eps)[INFO_VIZ3].head(K)\n",
        "\n",
        "# d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fn8KEHI3ERym",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da7e84e1-6029-46f3-f235-9e3791d0030b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaHdC0EaqzZz"
      },
      "source": [
        "## Run Algorithm Leaf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-kS0DqRq2_S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00a5783a-680d-440f-a3ae-462ea0ea17d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nh523URuq7AD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "709af237-7f27-48af-ff0b-a2a005186bd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/////////////\n",
            "7\n",
            "The sets of need pos and neg are\n",
            "[[0, 4, 1], [1, 0, 1], [1, 3, 1]]\n",
            "[[1, 4, 1], [2, 4, 1]]\n",
            "started pref sampling\n",
            "updated 2534 positive records\n",
            "updated 4548 negative records\n",
            "The new dataset contains 31656 rows.\n",
            "0    24781\n",
            "1     6875\n",
            "Name: income, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#get all of the candidate groups possible with the combos and names\n",
        "\n",
        "new_train_data = copy.deepcopy(train_set)\n",
        "\n",
        "#iterate over all the names to get the temp2 df for each name\n",
        "for a in [all_names_lst[0]]:\n",
        "# for a in [all_names_lst[0]]: # leaf\n",
        "# for a in [all_names_lst[-1:x]]: # top\n",
        "  # get_combos(names, names_values, all_names[a])\n",
        "  print(\"/////////////\")\n",
        "  print(a)\n",
        "  temp2, names = get_temp(new_train_data, all_names[a], compas_y)\n",
        "  temp, temp_g = get_temp_g(new_train_data, names, compas_y)\n",
        "  # YIN: updated, only keep the records in temp_g that have a size > 30.\n",
        "  temp_g = temp_g[temp_g['cnt'] > filter_count]\n",
        "  lst_of_counts = compute_lst_of_counts(temp, names, compas_y)\n",
        "  start = time.time()\n",
        "  need_pos, need_neg = compute_problematic_opt(temp2, temp_g, names, compas_y, lst_of_counts)\n",
        "  end = time.time()\n",
        "  excute_time = end - start\n",
        "  # print(\"The time to compute unfair group is {}\".format(str(excute_time)))\n",
        "  print(\"The sets of need pos and neg are\")\n",
        "  print(need_pos)\n",
        "  print(need_neg)\n",
        "  new_train_data['skewed'] = 0\n",
        "  new_train_data[\"diff\"] = 0\n",
        "  print(\"started pref sampling\")\n",
        "  new_train_data = pref_sampling_opt(new_train_data, names, compas_y, need_pos, need_neg)\n",
        "  # print(new_train_data)\n",
        "  print(new_train_data[compas_y].value_counts())\n",
        "new_train_x = pd.DataFrame(new_train_data, columns = columns_all)\n",
        "new_train_label = pd.DataFrame(new_train_data, columns = [compas_y])\n",
        "new_train_label = new_train_label[compas_y]\n",
        "new_train_label = new_train_label.astype('int')\n",
        "# print(fpr_onegroup(new_labels.tolist(), predict))\n",
        "# print(fnr_onegroup(new_labels.tolist(), predict))\n",
        "# print(pref_sampling_mod(train_x, train_set, train_label, names, compas_y, need_pos, need_neg))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27rcEGB9rhrf"
      },
      "source": [
        "### Results Leaf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print()\n",
        "print(\"dt\")\n",
        "griddt.fit(new_train_x, new_train_label)\n",
        "print(\"best\", griddt.best_score_)\n",
        "\n",
        "test_predict = griddt.predict(test_x)\n",
        "print(\"fpr and fnr\")\n",
        "print(fpr_onegroup(list(test_label), test_predict))\n",
        "print(fnr_onegroup(list(test_label), test_predict))\n",
        "accuracy = accuracy_score(test_label, test_set['predicted'])\n",
        "print(\"accuracy\")\n",
        "print(accuracy)\n",
        "test_set['predicted'] = test_predict\n",
        "r,r2,r3 = div_results(\"Adult\",\"Preferential Sampling-Leaf\",\"DT\")\n",
        "\n",
        "print()\n",
        "print(\"rf\")\n",
        "gridrf.fit(new_train_x, new_train_label)\n",
        "print(\"best\", gridrf.best_score_)\n",
        "test_predict = gridrf.predict(test_x)\n",
        "print(\"fpr and fnr\")\n",
        "print(fpr_onegroup(list(test_label), test_predict))\n",
        "print(fnr_onegroup(list(test_label), test_predict))\n",
        "accuracy = accuracy_score(test_label, test_set['predicted'])\n",
        "print(\"accuracy\")\n",
        "print(accuracy)\n",
        "test_set['predicted'] = test_predict\n",
        "r,r2,r3 = div_results(\"Adult\",\"Preferential Sampling-Leaf\",\"RF\")\n",
        "\n",
        "print()\n",
        "print(\"logistic\")\n",
        "gridlg.fit(new_train_x, new_train_label)\n",
        "print(\"best\", gridlg.best_score_)\n",
        "test_predict = gridlg.predict(test_x)\n",
        "print(\"fpr and fnr\")\n",
        "print(fpr_onegroup(list(test_label), test_predict))\n",
        "print(fnr_onegroup(list(test_label), test_predict))\n",
        "accuracy = accuracy_score(test_label, test_set['predicted'])\n",
        "print(\"accuracy\")\n",
        "print(accuracy)\n",
        "test_set['predicted'] = test_predict\n",
        "r,r2,r3 = div_results(\"Adult\",\"Preferential Sampling-Leaf\",\"LG\")\n",
        "\n",
        "print()\n",
        "print(\"svm\")\n",
        "clf.fit(new_train_x, new_train_label)\n",
        "# print(\"best\", svc.best_score_)\n",
        "test_predict = clf.predict(test_x)\n",
        "print(\"fpr and fnr\")\n",
        "print(fpr_onegroup(list(test_label), test_predict))\n",
        "print(fnr_onegroup(list(test_label), test_predict))\n",
        "accuracy = accuracy_score(test_label, test_set['predicted'])\n",
        "print(\"accuracy\")\n",
        "print(accuracy)\n",
        "test_set['predicted'] = test_predict\n",
        "s,s2,s3 = div_results(\"Adult\",\"Preferential Sampling-Leaf\",\"SVM\")"
      ],
      "metadata": {
        "id": "4-1zYQIWsDdn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2b8f049-c4ec-43dc-809c-3f11226e5965"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "dt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best 0.8037966077138703\n",
            "fpr and fnr\n",
            "0.07705078125\n",
            "0.5378833433553818\n",
            "accuracy\n",
            "0.7796697626418989\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/divexplorer/FP_DivergenceExplorer.py:489: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  fp = fp.append(row_root, ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0661633812697622\n",
            "0.27828020866353176\n",
            "0.0909356301316899\n",
            "accuracy is  0.8099660916998378\n",
            "['Adult', 'Preferential Sampling-Leaf', 'DT', 0.0661633812697622, 0.27828020866353176, 0.0909356301316899, 0.8099660916998378]\n",
            "\n",
            "rf\n",
            "best 0.77662461273843\n",
            "fpr and fnr\n",
            "0.079296875\n",
            "0.582080577269994\n",
            "accuracy\n",
            "0.8099660916998378\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/divexplorer/FP_DivergenceExplorer.py:489: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  fp = fp.append(row_root, ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.09780390978091087\n",
            "0.039917294926960556\n",
            "0.1230058260879714\n",
            "accuracy is  0.7974347633790358\n",
            "['Adult', 'Preferential Sampling-Leaf', 'RF', 0.09780390978091087, 0.039917294926960556, 0.1230058260879714, 0.7974347633790358]\n",
            "\n",
            "logistic\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "25 fits failed out of a total of 50.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "25 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.82142425        nan 0.82256144        nan 0.82259307\n",
            "        nan 0.82243513        nan 0.82243512]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best 0.8225930662555168\n",
            "fpr and fnr\n",
            "0.04951171875\n",
            "0.6551413108839447\n",
            "accuracy\n",
            "0.7974347633790358\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/divexplorer/FP_DivergenceExplorer.py:489: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  fp = fp.append(row_root, ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.03404996231170872\n",
            "0.12924282721209576\n",
            "0.11255711887709842\n",
            "accuracy is  0.8020050125313283\n",
            "['Adult', 'Preferential Sampling-Leaf', 'LG', 0.03404996231170872, 0.12924282721209576, 0.11255711887709842, 0.8020050125313283]\n",
            "\n",
            "svm\n",
            "fpr and fnr\n",
            "0.06533203125\n",
            "0.6975345760673481\n",
            "accuracy\n",
            "0.8020050125313283\n",
            "0.05942626517844015\n",
            "0.05482937317860928\n",
            "0.1470532322428726\n",
            "accuracy is  0.7796697626418989\n",
            "['Adult', 'Preferential Sampling-Leaf', 'SVM', 0.05942626517844015, 0.05482937317860928, 0.1470532322428726, 0.7796697626418989]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/divexplorer/FP_DivergenceExplorer.py:489: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  fp = fp.append(row_root, ignore_index=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM1zX7q-ruBj"
      },
      "source": [
        "## Run Algorithm Top"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mzp7XkuksXyD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "338ca695-164a-45eb-c860-91a4e46bc07b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/////////////\n",
            "1\n",
            "The sets of need pos and neg are\n",
            "[]\n",
            "[]\n",
            "started pref sampling\n",
            "updated 0 positive records\n",
            "updated 0 negative records\n",
            "The new dataset contains 31656 rows.\n",
            "0    23774\n",
            "1     7882\n",
            "Name: income, dtype: int64\n",
            "/////////////\n",
            "2\n",
            "The sets of need pos and neg are\n",
            "[]\n",
            "[]\n",
            "started pref sampling\n",
            "updated 0 positive records\n",
            "updated 0 negative records\n",
            "The new dataset contains 31656 rows.\n",
            "0    23774\n",
            "1     7882\n",
            "Name: income, dtype: int64\n",
            "/////////////\n",
            "3\n",
            "The sets of need pos and neg are\n",
            "[]\n",
            "[]\n",
            "started pref sampling\n",
            "updated 0 positive records\n",
            "updated 0 negative records\n",
            "The new dataset contains 31656 rows.\n",
            "0    23774\n",
            "1     7882\n",
            "Name: income, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#get all of the candidate groups possible with the combos and names\n",
        "\n",
        "new_train_data = copy.deepcopy(train_set)\n",
        "all_names_lst_top = find_top(all_names)\n",
        "#iterate over all the names to get the temp2 df for each name\n",
        "for a in all_names_lst_top:\n",
        "# for a in [all_names_lst[0]]: # leaf\n",
        "# for a in [all_names_lst[-1:x]]: # top\n",
        "  print(\"/////////////\")\n",
        "  print(a)\n",
        "  temp2, names = get_temp(new_train_data, all_names[a], compas_y)\n",
        "  temp, temp_g = get_temp_g(new_train_data, names, compas_y)\n",
        "  temp_g = temp_g[temp_g['cnt'] > filter_count]\n",
        "  lst_of_counts = compute_lst_of_counts(temp, names, compas_y)\n",
        "  start = time.time()\n",
        "  need_pos, need_neg = compute_problematic_opt(temp2, temp_g, names, compas_y, lst_of_counts)\n",
        "  end = time.time()\n",
        "  excute_time = end - start\n",
        "  # print(\"The time to compute unfair group is {}\".format(str(excute_time)))\n",
        "  print(\"The sets of need pos and neg are\")\n",
        "  print(need_pos)\n",
        "  print(need_neg)\n",
        "  new_train_data['skewed'] = 0\n",
        "  new_train_data[\"diff\"] = 0\n",
        "  print(\"started pref sampling\")\n",
        "  new_train_data = pref_sampling_opt(new_train_data, names, compas_y, need_pos, need_neg)\n",
        "  # print(new_train_data)\n",
        "  print(new_train_data[compas_y].value_counts())\n",
        "new_train_x = pd.DataFrame(new_train_data, columns = columns_all)\n",
        "new_train_label = pd.DataFrame(new_train_data, columns = [compas_y])\n",
        "new_train_label = new_train_label[compas_y]\n",
        "new_train_label = new_train_label.astype('int')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVRTFfXmr2NG"
      },
      "source": [
        "### Results Top"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print()\n",
        "print(\"dt\")\n",
        "griddt.fit(new_train_x, new_train_label)\n",
        "print(\"best\", griddt.best_score_)\n",
        "\n",
        "test_predict = griddt.predict(test_x)\n",
        "print(\"fpr and fnr\")\n",
        "print(fpr_onegroup(list(test_label), test_predict))\n",
        "print(fnr_onegroup(list(test_label), test_predict))\n",
        "accuracy = accuracy_score(test_label, test_set['predicted'])\n",
        "print(\"accuracy\")\n",
        "print(accuracy)\n",
        "test_set['predicted'] = test_predict\n",
        "r,r2,r3 = div_results(\"Adult\",\"Preferential Sampling-Top\",\"DT\")\n",
        "\n",
        "print()\n",
        "print(\"rf\")\n",
        "gridrf.fit(new_train_x, new_train_label)\n",
        "print(\"best\", gridrf.best_score_)\n",
        "test_predict = gridrf.predict(test_x)\n",
        "print(\"fpr and fnr\")\n",
        "print(fpr_onegroup(list(test_label), test_predict))\n",
        "print(fnr_onegroup(list(test_label), test_predict))\n",
        "accuracy = accuracy_score(test_label, test_set['predicted'])\n",
        "print(\"accuracy\")\n",
        "print(accuracy)\n",
        "test_set['predicted'] = test_predict\n",
        "r,r2,r3 = div_results(\"Adult\",\"Preferential Sampling-Top\",\"RF\")\n",
        "\n",
        "print()\n",
        "print(\"logistic\")\n",
        "gridlg.fit(new_train_x, new_train_label)\n",
        "print(\"best\", gridlg.best_score_)\n",
        "test_predict = gridlg.predict(test_x)\n",
        "print(\"fpr and fnr\")\n",
        "print(fpr_onegroup(list(test_label), test_predict))\n",
        "print(fnr_onegroup(list(test_label), test_predict))\n",
        "accuracy = accuracy_score(test_label, test_set['predicted'])\n",
        "print(\"accuracy\")\n",
        "print(accuracy)\n",
        "test_set['predicted'] = test_predict\n",
        "r,r2,r3 = div_results(\"Adult\",\"Preferential Sampling-Top\",\"LG\")\n",
        "\n",
        "print()\n",
        "print(\"svm\")\n",
        "clf.fit(new_train_x, new_train_label)\n",
        "# print(\"best\", svc.best_score_)\n",
        "test_predict = clf.predict(test_x)\n",
        "print(\"fpr and fnr\")\n",
        "print(fpr_onegroup(list(test_label), test_predict))\n",
        "print(fnr_onegroup(list(test_label), test_predict))\n",
        "accuracy = accuracy_score(test_label, test_set['predicted'])\n",
        "print(\"accuracy\")\n",
        "print(accuracy)\n",
        "test_set['predicted'] = test_predict\n",
        "s,s2,s3 = div_results(\"Adult\",\"Preferential Sampling-Top\",\"SVM\")"
      ],
      "metadata": {
        "id": "7cjoGWE-sFf9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c150b7e0-1ce1-4f8c-9851-5f6b3e887b4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "dt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best 0.8296373179213316\n",
            "fpr and fnr\n",
            "0.07646484375\n",
            "0.4555021046301864\n",
            "accuracy\n",
            "0.7796697626418989\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/divexplorer/FP_DivergenceExplorer.py:489: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  fp = fp.append(row_root, ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.08712843162482836\n",
            "0.2719532754801828\n",
            "0.09931833251469081\n",
            "accuracy is  0.8306059265811587\n",
            "['Adult', 'Preferential Sampling-Top', 'DT', 0.08712843162482836, 0.2719532754801828, 0.09931833251469081, 0.8306059265811587]\n",
            "\n",
            "rf\n",
            "best 0.8327331055471812\n",
            "fpr and fnr\n",
            "0.06513671875\n",
            "0.4582080577269994\n",
            "accuracy\n",
            "0.8306059265811587\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/divexplorer/FP_DivergenceExplorer.py:489: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  fp = fp.append(row_root, ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.07599193485791308\n",
            "0.3292511605445749\n",
            "0.10121818538941299\n",
            "accuracy is  0.8384932920536635\n",
            "['Adult', 'Preferential Sampling-Top', 'RF', 0.07599193485791308, 0.3292511605445749, 0.10121818538941299, 0.8384932920536635]\n",
            "\n",
            "logistic\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "25 fits failed out of a total of 50.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "25 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.80976744        nan 0.81103103        nan 0.81106263\n",
            "        nan 0.81103104        nan 0.81099945]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best 0.8110626270894963\n",
            "fpr and fnr\n",
            "0.06513671875\n",
            "0.5598316295850871\n",
            "accuracy\n",
            "0.8384932920536635\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/divexplorer/FP_DivergenceExplorer.py:489: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  fp = fp.append(row_root, ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.08829771103258652\n",
            "0.5106937724270221\n",
            "0.10772055278034307\n",
            "accuracy is  0.813578062804069\n",
            "['Adult', 'Preferential Sampling-Top', 'LG', 0.08829771103258652, 0.5106937724270221, 0.10772055278034307, 0.813578062804069]\n",
            "\n",
            "svm\n",
            "fpr and fnr\n",
            "0.06611328125\n",
            "0.5727600721587492\n",
            "accuracy\n",
            "0.813578062804069\n",
            "0.13515936984199434\n",
            "0.27202350978389084\n",
            "0.11030122765525739\n",
            "accuracy is  0.809671236915819\n",
            "['Adult', 'Preferential Sampling-Top', 'SVM', 0.13515936984199434, 0.27202350978389084, 0.11030122765525739, 0.809671236915819]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/divexplorer/FP_DivergenceExplorer.py:489: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  fp = fp.append(row_root, ignore_index=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeNaCEQ26n7V"
      },
      "source": [
        "# Duplication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-x05Dlbuc62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fed11e3-83d5-436b-f686-809efe065bd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "def round_int(x):\n",
        "    if x in [float(\"-inf\"),float(\"inf\")]: return 0\n",
        "    return int(round(x))\n",
        "\n",
        "\n",
        "def make_duplicate(d, group_lst, diff, label_y, names, need_positive_or_negative):\n",
        "    #print(\"make samples\")\n",
        "    selected = copy.deepcopy(d)\n",
        "    print(\"names \", names, group_lst)\n",
        "    for i in range(len(group_lst)):\n",
        "        att_name = names[i]\n",
        "        selected = selected[(selected[att_name] == group_lst[i])]\n",
        "    selected = selected[(selected[label_y] == need_positive_or_negative)]\n",
        "    # print(\"=============\")\n",
        "    # print(group_lst, names)\n",
        "    # print(selected)\n",
        "    if len(selected) == 0:\n",
        "        return pd.DataFrame()\n",
        "    # print(len(temp))\n",
        "    # randomly generated diff samples:\n",
        "    # print(len(selected), diff)\n",
        "    while(len(selected) < diff):\n",
        "        # duplicate the dataframe\n",
        "        select_copy = selected.copy(deep=True)\n",
        "        selected = pd.concat([selected, select_copy])\n",
        "        # print(len(selected))\n",
        "        #generated = temp\n",
        "        # the number needed is more than the not needed numbers.\n",
        "    #print(len(temp))\n",
        "    generated = selected.sample(n = diff, replace = False, axis = 0)\n",
        "    # print(generated)\n",
        "    return generated\n",
        "\n",
        "\n",
        "def naive_duplicate(d, temp2, names, need_pos, need_neg, label_y):\n",
        "    # add more records for all groups\n",
        "    # The smote algorithm to boost the coverage\n",
        "    for r in need_pos:\n",
        "        # print(\"adding more positive\")\n",
        "    # add more positive records\n",
        "        # determine how many points to add\n",
        "        print(\"pos_vals\", r)\n",
        "        diff = compute_diff_add(r, temp2, names, label_y, 1)\n",
        "        if diff == -1:\n",
        "          continue\n",
        "        diff = round_int(diff)\n",
        "        # add more records\n",
        "        print(\"Adding \" + str(diff) +\" positive records\")\n",
        "        samples_to_add = make_duplicate(d, r, diff, label_y, names, need_positive_or_negative = 1)\n",
        "        d = pd.concat([d, samples_to_add], ignore_index=True)\n",
        "    for k in need_neg:\n",
        "        print(\"neg_vals\", k)\n",
        "        # print(\"adding more negative\")\n",
        "        diff = compute_diff_add(k, temp2, names, label_y, need_positive_or_negative = 0)\n",
        "        if diff == -1:\n",
        "          continue\n",
        "        diff = round_int(diff)\n",
        "        print(\"Adding \" + str(diff) +\" negative records\")\n",
        "        samples_to_add = make_duplicate(d, k, diff, label_y, names, need_positive_or_negative = 0)\n",
        "        d = pd.concat([d, samples_to_add], ignore_index=True)\n",
        "    return d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_HqjDkKtclY"
      },
      "source": [
        "## Run Algorithm Lattice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptgbqFIO8PDF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "19a47774-5f4e-4c08-f032-f5ea5870b642"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "?????/////\n",
            "7\n",
            "The sets of need pos and neg are\n",
            "[[0, 4, 1], [1, 0, 1], [1, 3, 1]]\n",
            "[[1, 4, 1], [2, 4, 1]]\n",
            "started duplication\n",
            "pos_vals [0, 4, 1]\n",
            "0.48882540749453873 480 4745 1839.47655856159\n",
            "0.48882540749453873 480 4745 1839.47655856159\n",
            "Adding 1839 positive records\n",
            "names  ['age', 'race', 'gender'] [0, 4, 1]\n",
            "pos_vals [1, 0, 1]\n",
            "0.6497909199522103 19 86 36.8820191158901\n",
            "0.6497909199522103 19 86 36.8820191158901\n",
            "Adding 37 positive records\n",
            "names  ['age', 'race', 'gender'] [1, 0, 1]\n",
            "pos_vals [1, 3, 1]\n",
            "0.6464030915576694 18 55 17.5521700356718\n",
            "0.6464030915576694 18 55 17.5521700356718\n",
            "Adding 18 positive records\n",
            "names  ['age', 'race', 'gender'] [1, 3, 1]\n",
            "neg_vals [1, 4, 1]\n",
            "0.30381176470588234 3966 5799 7255.13568773236\n",
            "Adding 7255 negative records\n",
            "names  ['age', 'race', 'gender'] [1, 4, 1]\n",
            "neg_vals [2, 4, 1]\n",
            "0.3964053095885893 1679 2175 2060.56385191348\n",
            "Adding 2061 negative records\n",
            "names  ['age', 'race', 'gender'] [2, 4, 1]\n",
            "label y  0    33090\n",
            "1     9776\n",
            "Name: income, dtype: int64\n",
            "?????/////\n",
            "6\n",
            "The sets of need pos and neg are\n",
            "[]\n",
            "[]\n",
            "started duplication\n",
            "label y  0    33090\n",
            "1     9776\n",
            "Name: income, dtype: int64\n",
            "?????/////\n",
            "5\n",
            "The sets of need pos and neg are\n",
            "[]\n",
            "[]\n",
            "started duplication\n",
            "label y  0    33090\n",
            "1     9776\n",
            "Name: income, dtype: int64\n",
            "?????/////\n",
            "4\n",
            "The sets of need pos and neg are\n",
            "[]\n",
            "[[2, 1]]\n",
            "started duplication\n",
            "neg_vals [2, 1]\n",
            "0.33520400424692853 61 91 90.9787330316740\n",
            "Adding 91 negative records\n",
            "names  ['age', 'race'] [2, 1]\n",
            "label y  0    33181\n",
            "1     9776\n",
            "Name: income, dtype: int64\n",
            "?????/////\n",
            "3\n",
            "The sets of need pos and neg are\n",
            "[]\n",
            "[]\n",
            "started duplication\n",
            "label y  0    33181\n",
            "1     9776\n",
            "Name: income, dtype: int64\n",
            "?????/////\n",
            "2\n",
            "The sets of need pos and neg are\n",
            "[]\n",
            "[]\n",
            "started duplication\n",
            "label y  0    33181\n",
            "1     9776\n",
            "Name: income, dtype: int64\n",
            "?????/////\n",
            "1\n",
            "The sets of need pos and neg are\n",
            "[]\n",
            "[]\n",
            "started duplication\n",
            "label y  0    33181\n",
            "1     9776\n",
            "Name: income, dtype: int64\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-cd9ff53240a9>\u001b[0m in \u001b[0;36m<cell line: 41>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mnew_train_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_train_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcompas_y\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mnew_train_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_train_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mgrid_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0mgrid_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_train_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_train_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tree' is not defined"
          ]
        }
      ],
      "source": [
        "#get all of the candidate groups possible with the combos and names\n",
        "\n",
        "new_train_data = copy.deepcopy(train_set)\n",
        "\n",
        "#iterate over all the names to get the temp2 df for each name\n",
        "for a in all_names_lst:\n",
        "  print(\"?????/////\")\n",
        "  print(a)\n",
        "  temp2, names = get_temp(new_train_data, all_names[a], compas_y)\n",
        "  temp, temp_g = get_temp_g(new_train_data, names, compas_y)\n",
        "  temp_g = temp_g[temp_g['cnt'] > filter_count]\n",
        "  lst_of_counts = compute_lst_of_counts(temp, names, compas_y)\n",
        "  # start = time.time()\n",
        "  need_pos, need_neg = compute_problematic_opt(temp2, temp_g, names, compas_y, lst_of_counts)\n",
        "  # end = time.time()\n",
        "  # excute_time = end - start\n",
        "  # print(\"The time to compute unfair group is {}\".format(str(excute_time)))\n",
        "  print(\"The sets of need pos and neg are\")\n",
        "  print(need_pos)\n",
        "  print(need_neg)\n",
        "  new_train_data['skewed'] = 0\n",
        "  new_train_data[\"diff\"] = 0\n",
        "  print(\"started duplication\")\n",
        "  new_train_data = naive_duplicate(new_train_data, temp2, names, need_pos, need_neg, compas_y)\n",
        "  # new_train_x = pd.DataFrame(new_train_data, columns = columns_compas)\n",
        "  # new_train_label = pd.DataFrame(new_train_data, columns = [compas_y])\n",
        "  # new_train_label = new_train_label[compas_y]\n",
        "  # new_train_label = new_train_label.astype('int')\n",
        "  # grid_new = DecisionTreeClassifier(criterion=\"entropy\",max_depth=6, random_state=17)\n",
        "  # grid_new.fit(new_train_x, new_train_label)\n",
        "  # test_predict = grid_new.predict(test_x)\n",
        "  # print(\"new trainset length\", len(new_train_x))\n",
        "  # print(\"fpr and fnr\")\n",
        "  # print(fpr_onegroup(list(test_label), test_predict))\n",
        "  # print(fnr_onegroup(list(test_label), test_predict))\n",
        "  print(\"label y \", new_train_data[compas_y].value_counts())\n",
        "new_train_x = pd.DataFrame(new_train_data, columns = columns_all)\n",
        "new_train_label = pd.DataFrame(new_train_data, columns = [compas_y])\n",
        "new_train_label = new_train_label[compas_y]\n",
        "new_train_label = new_train_label.astype('int')\n",
        "grid_new = GridSearchCV(tree.DecisionTreeClassifier(), param_grid = param, cv = 6)\n",
        "grid_new.fit(new_train_x, new_train_label)\n",
        "print(\"model accuracy\")\n",
        "print(\"best\", grid_new.best_score_)\n",
        "test_predict = grid_new.predict(test_x)\n",
        "print(\"fpr and fnr\")\n",
        "# print(fpr_onegroup(new_labels.tolist(), predict))\n",
        "# print(fnr_onegroup(new_labels.tolist(), predict))\n",
        "print(fpr_onegroup(list(test_label), test_predict))\n",
        "print(fnr_onegroup(list(test_label), test_predict))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuQ3lc7aU9NG"
      },
      "source": [
        "### Results Lattice\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wid4ViElCw6i"
      },
      "outputs": [],
      "source": [
        "test_set['predicted'] = test_predict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d,d1,d2 = div_results(\"Adult\",\"Duplication-Lattice\",\"DT\")\n",
        "print()\n",
        "print(\"rf\")\n",
        "gridrf.fit(new_train_x, new_train_label)\n",
        "print(\"best\", gridrf.best_score_)\n",
        "test_predict = gridrf.predict(test_x)\n",
        "print(\"fpr and fnr\")\n",
        "# print(fpr_onegroup(new_labels.tolist(), predict))\n",
        "# print(fnr_onegroup(new_labels.tolist(), predict))\n",
        "print(fpr_onegroup(list(test_label), test_predict))\n",
        "print(fnr_onegroup(list(test_label), test_predict))\n",
        "test_set['predicted'] = test_predict\n",
        "r,r2,r3 = div_results(\"Adult\",\"Duplication-Lattice\",\"RF\")\n",
        "\n",
        "print()\n",
        "print(\"logistic\")\n",
        "gridl.fit(new_train_x, new_train_label)\n",
        "print(\"best\", gridl.best_score_)\n",
        "test_predict = gridl.predict(test_x)\n",
        "print(\"fpr and fnr\")\n",
        "# print(fpr_onegroup(new_labels.tolist(), predict))\n",
        "# print(fnr_onegroup(new_labels.tolist(), predict))\n",
        "print(fpr_onegroup(list(test_label), test_predict))\n",
        "print(fnr_onegroup(list(test_label), test_predict))\n",
        "test_set['predicted'] = test_predict\n",
        "r,r2,r3 = div_results(\"Adult\",\"Duplication-Lattice\",\"L\")\n",
        "\n",
        "print()\n",
        "print(\"svm\")\n",
        "svc.fit(new_train_x, new_train_label)\n",
        "# print(\"best\", svc.best_score_)\n",
        "test_predict = svc.predict(test_x)\n",
        "print(\"fpr and fnr\")\n",
        "# print(fpr_onegroup(new_labels.tolist(), predict))\n",
        "# print(fnr_onegroup(new_labels.tolist(), predict))\n",
        "print(fpr_onegroup(list(test_label), test_predict))\n",
        "print(fnr_onegroup(list(test_label), test_predict))\n",
        "test_set['predicted'] = test_predict\n",
        "s,s2,s3 = div_results(\"Adult\",\"Duplication-Lattice\",\"SVM\")"
      ],
      "metadata": {
        "id": "VTqA6ic3sHXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVcvdRuMQu26"
      },
      "outputs": [],
      "source": [
        "# # run divexplorer to find unfair groups\n",
        "# class_map={'N': 0, 'P': 1}\n",
        "# from divexplorer.FP_DivergenceExplorer import FP_DivergenceExplorer\n",
        "\n",
        "# #min_sup=0.1\n",
        "\n",
        "# min_sup = 0.05\n",
        "\n",
        "# fp_diver=FP_DivergenceExplorer(test_set,\"class\", \"predicted\", class_map=class_map)\n",
        "# FP_fm=fp_diver.getFrequentPatternDivergence(min_support=min_sup, metrics=[\"d_fpr\", \"d_fnr\", \"d_accuracy\"])\n",
        "# from divexplorer.FP_Divergence import FP_Divergence\n",
        "# fp_divergence_fpr=FP_Divergence(FP_fm, \"d_fpr\")\n",
        "# # fp_divergence_fpr=FP_Divergence(FP_fm, \"d_fnr\")\n",
        "# INFO_VIZ=[\"support\", \"itemsets\",  fp_divergence_fpr.metric, fp_divergence_fpr.t_value_col]\n",
        "\n",
        "# K=10\n",
        "# pd.options.display.max_rows = 200\n",
        "# d = fp_divergence_fpr.getDivergence(th_redundancy=0)[INFO_VIZ].head(K)\n",
        "# # summerization\n",
        "# eps=0.01\n",
        "\n",
        "# d = fp_divergence_fpr.getDivergence(th_redundancy=eps)[INFO_VIZ].head(K)\n",
        "\n",
        "# d\n",
        "# d, d2, d3 = div_results()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7whA1nLnvev0"
      },
      "outputs": [],
      "source": [
        "def fairness_score_computation(d, metrics):\n",
        "    sum_of_score = 0\n",
        "    for idx, row in d.iterrows():\n",
        "      sum_of_score += row['support'] * row[metrics]\n",
        "    return sum_of_score\n",
        "\n",
        "print(fairness_score_computation(d, 'd_fpr'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJ4Kag3MuFuf"
      },
      "source": [
        "## Run Algorithm Leaf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWAzykiouMXt"
      },
      "outputs": [],
      "source": [
        "#get all of the candidate groups possible with the combos and names\n",
        "\n",
        "new_train_data = copy.deepcopy(train_set)\n",
        "\n",
        "#iterate over all the names to get the temp2 df for each name\n",
        "for a in [all_names_lst[0]]:\n",
        "  print(\"?????/////\")\n",
        "  print(a)\n",
        "  temp2, names = get_temp(new_train_data, all_names[a], compas_y)\n",
        "  temp, temp_g = get_temp_g(new_train_data, names, compas_y)\n",
        "  temp_g = temp_g[temp_g['cnt'] > filter_count]\n",
        "  lst_of_counts = compute_lst_of_counts(temp, names, compas_y)\n",
        "  # start = time.time()\n",
        "  need_pos, need_neg = compute_problematic_opt(temp2, temp_g, names, compas_y, lst_of_counts)\n",
        "  # end = time.time()\n",
        "  # excute_time = end - start\n",
        "  # print(\"The time to compute unfair group is {}\".format(str(excute_time)))\n",
        "  print(\"The sets of need pos and neg are\")\n",
        "  print(need_pos)\n",
        "  print(need_neg)\n",
        "  new_train_data['skewed'] = 0\n",
        "  new_train_data[\"diff\"] = 0\n",
        "  print(\"started duplication\")\n",
        "  new_train_data = naive_duplicate(new_train_data, temp2, names, need_pos, need_neg, compas_y)\n",
        "  # new_train_x = pd.DataFrame(new_train_data, columns = columns_compas)\n",
        "  # new_train_label = pd.DataFrame(new_train_data, columns = [compas_y])\n",
        "  # new_train_label = new_train_label[compas_y]\n",
        "  # new_train_label = new_train_label.astype('int')\n",
        "  # grid_new = DecisionTreeClassifier(criterion=\"entropy\",max_depth=6, random_state=17)\n",
        "  # grid_new.fit(new_train_x, new_train_label)\n",
        "  # test_predict = grid_new.predict(test_x)\n",
        "  # print(\"new trainset length\", len(new_train_x))\n",
        "  # print(\"fpr and fnr\")\n",
        "  # print(fpr_onegroup(list(test_label), test_predict))\n",
        "  # print(fnr_onegroup(list(test_label), test_predict))\n",
        "  print(\"label y\", new_train_data[compas_y].value_counts())\n",
        "new_train_x = pd.DataFrame(new_train_data, columns = columns_all)\n",
        "new_train_label = pd.DataFrame(new_train_data, columns = [compas_y])\n",
        "new_train_label = new_train_label[compas_y]\n",
        "new_train_label = new_train_label.astype('int')\n",
        "grid_new = GridSearchCV(tree.DecisionTreeClassifier(), param_grid = param, cv = 6)\n",
        "grid_new.fit(new_train_x, new_train_label)\n",
        "print(\"model accuracy\")\n",
        "print(\"best\", grid_new.best_score_)\n",
        "test_predict = grid_new.predict(test_x)\n",
        "print(\"fpr and fnr\")\n",
        "# print(fpr_onegroup(new_labels.tolist(), predict))\n",
        "# print(fnr_onegroup(new_labels.tolist(), predict))\n",
        "print(fpr_onegroup(list(test_label), test_predict))\n",
        "print(fnr_onegroup(list(test_label), test_predict))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyiaC-BPuT-N"
      },
      "source": [
        "### Results Leaf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIpq68mRuWVd"
      },
      "outputs": [],
      "source": [
        "test_set['predicted'] = test_predict\n",
        "# d,d2,d3 = div_results()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d,d1,d2 = div_results(\"Adult\",\"Duplication-Leaf\",\"DT\")\n",
        "print()\n",
        "print(\"rf\")\n",
        "gridrf.fit(new_train_x, new_train_label)\n",
        "print(\"best\", gridrf.best_score_)\n",
        "test_predict = gridrf.predict(test_x)\n",
        "print(\"fpr and fnr\")\n",
        "# print(fpr_onegroup(new_labels.tolist(), predict))\n",
        "# print(fnr_onegroup(new_labels.tolist(), predict))\n",
        "print(fpr_onegroup(list(test_label), test_predict))\n",
        "print(fnr_onegroup(list(test_label), test_predict))\n",
        "test_set['predicted'] = test_predict\n",
        "r,r2,r3 = div_results(\"Adult\",\"Duplication-Leaf\",\"RF\")\n",
        "\n",
        "print()\n",
        "print(\"logistic\")\n",
        "gridl.fit(new_train_x, new_train_label)\n",
        "print(\"best\", gridl.best_score_)\n",
        "test_predict = gridl.predict(test_x)\n",
        "print(\"fpr and fnr\")\n",
        "# print(fpr_onegroup(new_labels.tolist(), predict))\n",
        "# print(fnr_onegroup(new_labels.tolist(), predict))\n",
        "print(fpr_onegroup(list(test_label), test_predict))\n",
        "print(fnr_onegroup(list(test_label), test_predict))\n",
        "test_set['predicted'] = test_predict\n",
        "r,r2,r3 = div_results(\"Adult\",\"Duplication-Leaf\",\"L\")\n",
        "\n",
        "print()\n",
        "print(\"svm\")\n",
        "svc.fit(new_train_x, new_train_label)\n",
        "# print(\"best\", svc.best_score_)\n",
        "test_predict = svc.predict(test_x)\n",
        "print(\"fpr and fnr\")\n",
        "# print(fpr_onegroup(new_labels.tolist(), predict))\n",
        "# print(fnr_onegroup(new_labels.tolist(), predict))\n",
        "print(fpr_onegroup(list(test_label), test_predict))\n",
        "print(fnr_onegroup(list(test_label), test_predict))\n",
        "test_set['predicted'] = test_predict\n",
        "s,s2,s3 = div_results(\"Adult\",\"Duplication-Leaf\",\"SVM\")"
      ],
      "metadata": {
        "id": "FVCnBjPxsJgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HcExt2xVCUKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ud6LiPvubLz"
      },
      "source": [
        "## Run Algorithm Top"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itgAs3JCuj6B"
      },
      "outputs": [],
      "source": [
        "#get all of the candidate groups possible with the combos and names\n",
        "\n",
        "new_train_data = copy.deepcopy(train_set)\n",
        "\n",
        "#iterate over all the names to get the temp2 df for each name\n",
        "all_names_lst_top = find_top(all_names)\n",
        "for a in all_names_lst_top:\n",
        "  print(\"?????/////\")\n",
        "  print(a)\n",
        "  temp2, names = get_temp(new_train_data, all_names[a], compas_y)\n",
        "  temp, temp_g = get_temp_g(new_train_data, names, compas_y)\n",
        "  temp_g = temp_g[temp_g['cnt'] > filter_count]\n",
        "  lst_of_counts = compute_lst_of_counts(temp, names, compas_y)\n",
        "  # start = time.time()\n",
        "  need_pos, need_neg = compute_problematic_opt(temp2, temp_g, names, compas_y, lst_of_counts)\n",
        "  # end = time.time()\n",
        "  # excute_time = end - start\n",
        "  # print(\"The time to compute unfair group is {}\".format(str(excute_time)))\n",
        "  print(\"The sets of need pos and neg are\")\n",
        "  print(need_pos)\n",
        "  print(need_neg)\n",
        "  new_train_data['skewed'] = 0\n",
        "  new_train_data[\"diff\"] = 0\n",
        "  print(\"started duplication\")\n",
        "  new_train_data = naive_duplicate(new_train_data, temp2, names, need_pos, need_neg, compas_y)\n",
        "  # new_train_x = pd.DataFrame(new_train_data, columns = columns_compas)\n",
        "  # new_train_label = pd.DataFrame(new_train_data, columns = [compas_y])\n",
        "  # new_train_label = new_train_label[compas_y]\n",
        "  # new_train_label = new_train_label.astype('int')\n",
        "  # grid_new = DecisionTreeClassifier(criterion=\"entropy\",max_depth=6, random_state=17)\n",
        "  # grid_new.fit(new_train_x, new_train_label)\n",
        "  # test_predict = grid_new.predict(test_x)\n",
        "  # print(\"new trainset length\", len(new_train_x))\n",
        "  # print(\"fpr and fnr\")\n",
        "  # print(fpr_onegroup(list(test_label), test_predict))\n",
        "  # print(fnr_onegroup(list(test_label), test_predict))\n",
        "  print(\"Label y\", new_train_data[compas_y].value_counts())\n",
        "new_train_x = pd.DataFrame(new_train_data, columns = columns_all)\n",
        "new_train_label = pd.DataFrame(new_train_data, columns = [compas_y])\n",
        "new_train_label = new_train_label[compas_y]\n",
        "new_train_label = new_train_label.astype('int')\n",
        "grid_new = GridSearchCV(tree.DecisionTreeClassifier(), param_grid = param, cv = 6)\n",
        "grid_new.fit(new_train_x, new_train_label)\n",
        "print(\"model accuracy\")\n",
        "print(\"best\", grid_new.best_score_)\n",
        "test_predict = grid_new.predict(test_x)\n",
        "print(\"fpr and fnr\")\n",
        "# print(fpr_onegroup(new_labels.tolist(), predict))\n",
        "# print(fnr_onegroup(new_labels.tolist(), predict))\n",
        "print(fpr_onegroup(list(test_label), test_predict))\n",
        "print(fnr_onegroup(list(test_label), test_predict))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtQcykniuqUC"
      },
      "source": [
        "### Results Top"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXyGMq6Fuunb"
      },
      "outputs": [],
      "source": [
        "test_set['predicted'] = test_predict\n",
        "# d,d2,d3 = div_results()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d,d1,d2 = div_results(\"Adult\",\"Duplication-Top\",\"DT\")\n",
        "print()\n",
        "print(\"rf\")\n",
        "gridrf.fit(new_train_x, new_train_label)\n",
        "print(\"best\", gridrf.best_score_)\n",
        "test_predict = gridrf.predict(test_x)\n",
        "print(\"fpr and fnr\")\n",
        "# print(fpr_onegroup(new_labels.tolist(), predict))\n",
        "# print(fnr_onegroup(new_labels.tolist(), predict))\n",
        "print(fpr_onegroup(list(test_label), test_predict))\n",
        "print(fnr_onegroup(list(test_label), test_predict))\n",
        "test_set['predicted'] = test_predict\n",
        "r,r2,r3 = div_results(\"Adult\",\"Duplication-Top\",\"RF\")\n",
        "\n",
        "print()\n",
        "print(\"logistic\")\n",
        "gridl.fit(new_train_x, new_train_label)\n",
        "print(\"best\", gridl.best_score_)\n",
        "test_predict = gridl.predict(test_x)\n",
        "print(\"fpr and fnr\")\n",
        "# print(fpr_onegroup(new_labels.tolist(), predict))\n",
        "# print(fnr_onegroup(new_labels.tolist(), predict))\n",
        "print(fpr_onegroup(list(test_label), test_predict))\n",
        "print(fnr_onegroup(list(test_label), test_predict))\n",
        "test_set['predicted'] = test_predict\n",
        "r,r2,r3 = div_results(\"Adult\",\"Duplication-Top\",\"L\")\n",
        "\n",
        "print()\n",
        "print(\"svm\")\n",
        "svc.fit(new_train_x, new_train_label)\n",
        "# print(\"best\", svc.best_score_)\n",
        "test_predict = svc.predict(test_x)\n",
        "print(\"fpr and fnr\")\n",
        "# print(fpr_onegroup(new_labels.tolist(), predict))\n",
        "# print(fnr_onegroup(new_labels.tolist(), predict))\n",
        "print(fpr_onegroup(list(test_label), test_predict))\n",
        "print(fnr_onegroup(list(test_label), test_predict))\n",
        "test_set['predicted'] = test_predict\n",
        "s,s2,s3 = div_results(\"Adult\",\"Duplication-Top\",\"SVM\")"
      ],
      "metadata": {
        "id": "-PXEjrEDsOEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SfzogxQ-Q5d"
      },
      "source": [
        "#Down-sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9m0e5GbVL-d"
      },
      "outputs": [],
      "source": [
        "def round_int(x):\n",
        "    if x in [float(\"-inf\"),float(\"inf\")]: return 0\n",
        "    return int(round(x))\n",
        "\n",
        "\n",
        "def make_remove(d, group_lst, diff, names, label_y, need_positive_or_negative):\n",
        "    #print(\"make samples\")\n",
        "    temp = copy.deepcopy(d)\n",
        "    for i in range(len(group_lst)):\n",
        "        att_name = names[i]\n",
        "        temp = temp[(temp[att_name] == group_lst[i])]\n",
        "    temp = temp[(temp[label_y] == need_positive_or_negative)]\n",
        "    # randomly generated diff samples\n",
        "        #generated = temp\n",
        "        # the number needed is more than the not needed numbers.\n",
        "    # print(len(temp))\n",
        "    # print(diff)\n",
        "    if(diff>len(temp)):\n",
        "        diff = len(temp)\n",
        "    generated = temp.sample(n = diff, replace = False, axis = 0)\n",
        "    #print(generated.index)\n",
        "    return generated.index\n",
        "\n",
        "\n",
        "def naive_downsampling(d, temp2, names, need_pos, need_neg, label_y):\n",
        "    # add more records for all groups\n",
        "    # The smote algorithm to boost the coverage\n",
        "    for r in need_pos:\n",
        "        print(\"removing more negative\")\n",
        "    # add more positive records\n",
        "        # determine how many points to add\n",
        "        print(r)\n",
        "        diff = compute_diff_remove(r, temp2, names, label_y, need_positive_or_negative = 1)\n",
        "        if diff == -1:\n",
        "          continue\n",
        "        diff = round_int(diff)\n",
        "        # add more records\n",
        "        print(\"Removed \" + str(diff) +\" negative records\")\n",
        "        samples_to_remove = make_remove(d, r, diff, names, label_y, need_positive_or_negative = 0)\n",
        "        d.drop(index  = samples_to_remove, inplace = True)\n",
        "        # print(len(d))\n",
        "    for k in need_neg:\n",
        "        print(k)\n",
        "        diff = compute_diff_remove(k, temp2, names, label_y, need_positive_or_negative = 0)\n",
        "        if diff == -1:\n",
        "          continue\n",
        "        diff = round_int(diff)\n",
        "        print(\"Removed \" + str(diff) +\" positive records\")\n",
        "        samples_to_remove = make_remove(d, k, diff, names, label_y, need_positive_or_negative = 1)\n",
        "        d.drop(index  = samples_to_remove, inplace = True)\n",
        "    return d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcUYj8wJwrHK"
      },
      "source": [
        "## Run Algorithm Lattice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTfYbyt4pcRq"
      },
      "outputs": [],
      "source": [
        "#get all of the candidate groups possible with the combos and names\n",
        "\n",
        "new_train_data = copy.deepcopy(train_set)\n",
        "\n",
        "#iterate over all the names to get the temp2 df for each name\n",
        "for a in all_names_lst:\n",
        "  print(\"?????/////\")\n",
        "  print(a)\n",
        "  temp2, names = get_temp(new_train_data, all_names[a], compas_y)\n",
        "  temp, temp_g = get_temp_g(new_train_data, names, compas_y)\n",
        "  temp_g = temp_g[temp_g['cnt'] > filter_count]\n",
        "  # print(temp_g)\n",
        "  # print(temp2, names)\n",
        "  # print(temp)\n",
        "  lst_of_counts = compute_lst_of_counts(temp, names, compas_y)\n",
        "  start = time.time()\n",
        "  need_pos, need_neg = compute_problematic_opt(temp2, temp_g, names, compas_y, lst_of_counts)\n",
        "  end = time.time()\n",
        "  excute_time = end - start\n",
        "  print(\"The time to compute unfair group is {}\".format(str(excute_time)))\n",
        "  print(\"The sets of need pos and neg are\")\n",
        "  print(need_pos)\n",
        "  print(need_neg)\n",
        "  new_train_data['skewed'] = 0\n",
        "  new_train_data[\"diff\"] = 0\n",
        "  # print(\"started duplication\")\n",
        "  new_train_data = naive_downsampling(new_train_data, temp2, names, need_pos, need_neg, compas_y)\n",
        "  print(new_train_data[compas_y].value_counts())\n",
        "new_train_x = pd.DataFrame(new_train_data, columns = columns_all)\n",
        "new_train_label = pd.DataFrame(new_train_data, columns = [compas_y])\n",
        "new_train_label = new_train_label[compas_y]\n",
        "new_train_label = new_train_label.astype('int')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhS42vuGsI2q"
      },
      "source": [
        "### Results Lattice"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print()\n",
        "print(\"dt\")\n",
        "griddt.fit(new_train_x, new_train_label)\n",
        "print(\"best\", griddt.best_score_)\n",
        "\n",
        "test_predict = griddt.predict(test_x)\n",
        "print(\"fpr and fnr\")\n",
        "print(fpr_onegroup(list(test_label), test_predict))\n",
        "print(fnr_onegroup(list(test_label), test_predict))\n",
        "accuracy = accuracy_score(test_label, test_set['predicted'])\n",
        "print(\"accuracy\")\n",
        "print(accuracy)\n",
        "test_set['predicted'] = test_predict\n",
        "r,r2,r3 = div_results(\"Adult\",\"Down Sampling-Lattice\",\"DT\")\n",
        "\n",
        "print()\n",
        "print(\"rf\")\n",
        "gridrf.fit(new_train_x, new_train_label)\n",
        "print(\"best\", gridrf.best_score_)\n",
        "test_predict = gridrf.predict(test_x)\n",
        "print(\"fpr and fnr\")\n",
        "print(fpr_onegroup(list(test_label), test_predict))\n",
        "print(fnr_onegroup(list(test_label), test_predict))\n",
        "accuracy = accuracy_score(test_label, test_set['predicted'])\n",
        "print(\"accuracy\")\n",
        "print(accuracy)\n",
        "test_set['predicted'] = test_predict\n",
        "r,r2,r3 = div_results(\"Adult\",\"Down Sampling-Lattice\",\"RF\")\n",
        "\n",
        "print()\n",
        "print(\"logistic\")\n",
        "gridlg.fit(new_train_x, new_train_label)\n",
        "print(\"best\", gridlg.best_score_)\n",
        "test_predict = gridlg.predict(test_x)\n",
        "print(\"fpr and fnr\")\n",
        "print(fpr_onegroup(list(test_label), test_predict))\n",
        "print(fnr_onegroup(list(test_label), test_predict))\n",
        "accuracy = accuracy_score(test_label, test_set['predicted'])\n",
        "print(\"accuracy\")\n",
        "print(accuracy)\n",
        "test_set['predicted'] = test_predict\n",
        "r,r2,r3 = div_results(\"Adult\",\"Down Sampling-Lattice\",\"LG\")\n",
        "\n",
        "print()\n",
        "print(\"svm\")\n",
        "clf.fit(new_train_x, new_train_label)\n",
        "# print(\"best\", svc.best_score_)\n",
        "test_predict = clf.predict(test_x)\n",
        "print(\"fpr and fnr\")\n",
        "print(fpr_onegroup(list(test_label), test_predict))\n",
        "print(fnr_onegroup(list(test_label), test_predict))\n",
        "accuracy = accuracy_score(test_label, test_set['predicted'])\n",
        "print(\"accuracy\")\n",
        "print(accuracy)\n",
        "test_set['predicted'] = test_predict\n",
        "s,s2,s3 = div_results(\"Adult\",\"Down Sampling-Lattice\",\"SVM\")"
      ],
      "metadata": {
        "id": "dbzCpafMsVko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqfnIjDrwk9i"
      },
      "source": [
        "## Run Algorithm Leaf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvmlKdUqwl_1"
      },
      "outputs": [],
      "source": [
        "#get all of the candidate groups possible with the combos and names\n",
        "\n",
        "new_train_data = copy.deepcopy(train_set)\n",
        "\n",
        "#iterate over all the names to get the temp2 df for each name\n",
        "for a in [all_names_lst[0]]:\n",
        "  print(\"?????/////\")\n",
        "  print(a)\n",
        "  temp2, names = get_temp(new_train_data, all_names[a], compas_y)\n",
        "  temp, temp_g = get_temp_g(new_train_data, names, compas_y)\n",
        "  temp_g = temp_g[temp_g['cnt'] > filter_count]\n",
        "  # print(temp_g)\n",
        "  # print(temp2, names)\n",
        "  # print(temp)\n",
        "  lst_of_counts = compute_lst_of_counts(temp, names, compas_y)\n",
        "  start = time.time()\n",
        "  need_pos, need_neg = compute_problematic_opt(temp2, temp_g, names, compas_y, lst_of_counts)\n",
        "  end = time.time()\n",
        "  excute_time = end - start\n",
        "  print(\"The time to compute unfair group is {}\".format(str(excute_time)))\n",
        "  print(\"The sets of need pos and neg are\")\n",
        "  print(need_pos)\n",
        "  print(need_neg)\n",
        "  new_train_data['skewed'] = 0\n",
        "  new_train_data[\"diff\"] = 0\n",
        "  # print(\"started duplication\")\n",
        "  new_train_data = naive_downsampling(new_train_data, temp2, names, need_pos, need_neg, compas_y)\n",
        "  print(new_train_data[compas_y].value_counts())\n",
        "new_train_x = pd.DataFrame(new_train_data, columns = columns_all)\n",
        "new_train_label = pd.DataFrame(new_train_data, columns = [compas_y])\n",
        "new_train_label = new_train_label[compas_y]\n",
        "new_train_label = new_train_label.astype('int')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-O26rF9xYc-"
      },
      "source": [
        "### Results Leaf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print()\n",
        "print(\"dt\")\n",
        "griddt.fit(new_train_x, new_train_label)\n",
        "print(\"best\", griddt.best_score_)\n",
        "\n",
        "test_predict = griddt.predict(test_x)\n",
        "print(\"fpr and fnr\")\n",
        "print(fpr_onegroup(list(test_label), test_predict))\n",
        "print(fnr_onegroup(list(test_label), test_predict))\n",
        "accuracy = accuracy_score(test_label, test_set['predicted'])\n",
        "print(\"accuracy\")\n",
        "print(accuracy)\n",
        "test_set['predicted'] = test_predict\n",
        "r,r2,r3 = div_results(\"Adult\",\"Down Sampling-Leaf\",\"DT\")\n",
        "\n",
        "print()\n",
        "print(\"rf\")\n",
        "gridrf.fit(new_train_x, new_train_label)\n",
        "print(\"best\", gridrf.best_score_)\n",
        "test_predict = gridrf.predict(test_x)\n",
        "print(\"fpr and fnr\")\n",
        "print(fpr_onegroup(list(test_label), test_predict))\n",
        "print(fnr_onegroup(list(test_label), test_predict))\n",
        "accuracy = accuracy_score(test_label, test_set['predicted'])\n",
        "print(\"accuracy\")\n",
        "print(accuracy)\n",
        "test_set['predicted'] = test_predict\n",
        "r,r2,r3 = div_results(\"Adult\",\"Down Sampling-Leaf\",\"RF\")\n",
        "\n",
        "print()\n",
        "print(\"logistic\")\n",
        "gridlg.fit(new_train_x, new_train_label)\n",
        "print(\"best\", gridlg.best_score_)\n",
        "test_predict = gridlg.predict(test_x)\n",
        "print(\"fpr and fnr\")\n",
        "print(fpr_onegroup(list(test_label), test_predict))\n",
        "print(fnr_onegroup(list(test_label), test_predict))\n",
        "accuracy = accuracy_score(test_label, test_set['predicted'])\n",
        "print(\"accuracy\")\n",
        "print(accuracy)\n",
        "test_set['predicted'] = test_predict\n",
        "r,r2,r3 = div_results(\"Adult\",\"Down Sampling-Leaf\",\"LG\")\n",
        "\n",
        "print()\n",
        "print(\"svm\")\n",
        "clf.fit(new_train_x, new_train_label)\n",
        "# print(\"best\", svc.best_score_)\n",
        "test_predict = clf.predict(test_x)\n",
        "print(\"fpr and fnr\")\n",
        "print(fpr_onegroup(list(test_label), test_predict))\n",
        "print(fnr_onegroup(list(test_label), test_predict))\n",
        "accuracy = accuracy_score(test_label, test_set['predicted'])\n",
        "print(\"accuracy\")\n",
        "print(accuracy)\n",
        "test_set['predicted'] = test_predict\n",
        "s,s2,s3 = div_results(\"Adult\",\"Down Sampling-Leaf\",\"SVM\")"
      ],
      "metadata": {
        "id": "7HWsJpa-sgzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEjc2orXxeJU"
      },
      "source": [
        "## Run Algorithm Top"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1ttSPisxj-R"
      },
      "outputs": [],
      "source": [
        "#get all of the candidate groups possible with the combos and names\n",
        "\n",
        "new_train_data = copy.deepcopy(train_set)\n",
        "\n",
        "#iterate over all the names to get the temp2 df for each name\n",
        "all_names_lst_top = find_top(all_names)\n",
        "for a in all_names_lst_top:\n",
        "  print(\"?????/////\")\n",
        "  print(a)\n",
        "  temp2, names = get_temp(new_train_data, all_names[a], compas_y)\n",
        "  temp, temp_g = get_temp_g(new_train_data, names, compas_y)\n",
        "  temp_g = temp_g[temp_g['cnt'] > filter_count]\n",
        "  # print(temp_g)\n",
        "  # print(temp2, names)\n",
        "  # print(temp)\n",
        "  lst_of_counts = compute_lst_of_counts(temp, names, compas_y)\n",
        "  start = time.time()\n",
        "  need_pos, need_neg = compute_problematic_opt(temp2, temp_g, names, compas_y, lst_of_counts)\n",
        "  end = time.time()\n",
        "  excute_time = end - start\n",
        "  print(\"The time to compute unfair group is {}\".format(str(excute_time)))\n",
        "  print(\"The sets of need pos and neg are\")\n",
        "  print(need_pos)\n",
        "  print(need_neg)\n",
        "  new_train_data['skewed'] = 0\n",
        "  new_train_data[\"diff\"] = 0\n",
        "  # print(\"started duplication\")\n",
        "  new_train_data = naive_downsampling(new_train_data, temp2, names, need_pos, need_neg, compas_y)\n",
        "  print(new_train_data[compas_y].value_counts())\n",
        "new_train_x = pd.DataFrame(new_train_data, columns = columns_all)\n",
        "new_train_label = pd.DataFrame(new_train_data, columns = [compas_y])\n",
        "new_train_label = new_train_label[compas_y]\n",
        "new_train_label = new_train_label.astype('int')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQOAJrGyxrsr"
      },
      "source": [
        "### Results Top"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print()\n",
        "print(\"dt\")\n",
        "griddt.fit(new_train_x, new_train_label)\n",
        "print(\"best\", griddt.best_score_)\n",
        "\n",
        "test_predict = griddt.predict(test_x)\n",
        "print(\"fpr and fnr\")\n",
        "print(fpr_onegroup(list(test_label), test_predict))\n",
        "print(fnr_onegroup(list(test_label), test_predict))\n",
        "accuracy = accuracy_score(test_label, test_set['predicted'])\n",
        "print(\"accuracy\")\n",
        "print(accuracy)\n",
        "test_set['predicted'] = test_predict\n",
        "r,r2,r3 = div_results(\"Adult\",\"Down Sampling-Top\",\"DT\")\n",
        "\n",
        "print()\n",
        "print(\"rf\")\n",
        "gridrf.fit(new_train_x, new_train_label)\n",
        "print(\"best\", gridrf.best_score_)\n",
        "test_predict = gridrf.predict(test_x)\n",
        "print(\"fpr and fnr\")\n",
        "print(fpr_onegroup(list(test_label), test_predict))\n",
        "print(fnr_onegroup(list(test_label), test_predict))\n",
        "accuracy = accuracy_score(test_label, test_set['predicted'])\n",
        "print(\"accuracy\")\n",
        "print(accuracy)\n",
        "test_set['predicted'] = test_predict\n",
        "r,r2,r3 = div_results(\"Adult\",\"Down Sampling-Top\",\"RF\")\n",
        "\n",
        "print()\n",
        "print(\"logistic\")\n",
        "gridlg.fit(new_train_x, new_train_label)\n",
        "print(\"best\", gridlg.best_score_)\n",
        "test_predict = gridlg.predict(test_x)\n",
        "print(\"fpr and fnr\")\n",
        "print(fpr_onegroup(list(test_label), test_predict))\n",
        "print(fnr_onegroup(list(test_label), test_predict))\n",
        "accuracy = accuracy_score(test_label, test_set['predicted'])\n",
        "print(\"accuracy\")\n",
        "print(accuracy)\n",
        "test_set['predicted'] = test_predict\n",
        "r,r2,r3 = div_results(\"Adult\",\"Down Sampling-Top\",\"LG\")\n",
        "\n",
        "print()\n",
        "print(\"svm\")\n",
        "clf.fit(new_train_x, new_train_label)\n",
        "# print(\"best\", svc.best_score_)\n",
        "test_predict = clf.predict(test_x)\n",
        "print(\"fpr and fnr\")\n",
        "print(fpr_onegroup(list(test_label), test_predict))\n",
        "print(fnr_onegroup(list(test_label), test_predict))\n",
        "accuracy = accuracy_score(test_label, test_set['predicted'])\n",
        "print(\"accuracy\")\n",
        "print(accuracy)\n",
        "test_set['predicted'] = test_predict\n",
        "s,s2,s3 = div_results(\"Adult\",\"Down Sampling-Top\",\"SVM\")"
      ],
      "metadata": {
        "id": "WyrmV-7cshtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWbCzMkrnT-0"
      },
      "source": [
        "# Massaging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tINoeYnSnWn4"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "def round_int(x):\n",
        "    if x in [float(\"-inf\"),float(\"inf\")]: return 0\n",
        "    return int(round(x))\n",
        "\n",
        "def get_depromotion(d, diff, group_lst, names, label_y, flag_depro):\n",
        "    # double check\n",
        "    print(\"start depromotion\")\n",
        "    # print(len(d))\n",
        "    print(names)\n",
        "    # input_test = d.drop(columns = [label_y])\n",
        "    # COMPAS COLUMNS = ]]\n",
        "    # input_test = pd.DataFrame(train_set, columns = names)\n",
        "    input_test = pd.DataFrame(d, columns = columns_compas)\n",
        "    # should it be d instead?\n",
        "    # print(group_lst, names)\n",
        "    clf = MultinomialNB()\n",
        "    # temp_train_label = pd.DataFrame(train_set, columns = [label_y])\n",
        "    temp_train_label = pd.DataFrame(d, columns = [label_y])\n",
        "    temp_train_label = temp_train_label[label_y]\n",
        "    temp_train_label = temp_train_label.astype('int')\n",
        "    clf = clf.fit(input_test, temp_train_label)\n",
        "    prob  = clf.predict_proba(input_test)[:,0]\n",
        "    select = copy.deepcopy(d)\n",
        "    select['prob'] = prob # the higher the probablity is, the more likely for it to be 0\n",
        "    # filter out those belongs to this group\n",
        "    for i in range(len(group_lst)):\n",
        "        att_name = names[i]\n",
        "        select = select[(select[att_name] == group_lst[i])]\n",
        "    select = select[(select[label_y] == flag_depro)]\n",
        "    # rank them according to the probability\n",
        "    # filp the records and remove the records from d\n",
        "    if (flag_depro == 0):\n",
        "        select.sort_values(by=\"prob\", ascending=True, inplace=True)\n",
        "        select[label_y] = 1\n",
        "    else:\n",
        "        select.sort_values(by=\"prob\", ascending=False, inplace=True)\n",
        "        select[label_y] = 0\n",
        "    head = select.head(diff)\n",
        "    index_list = []\n",
        "    index_list = list(head.index)\n",
        "    d.drop(index_list,inplace = True)\n",
        "    head.drop(columns = ['prob'],inplace = True)\n",
        "    #print(head.head())\n",
        "    #print(d.head())\n",
        "    return head\n",
        "\n",
        "\n",
        "\n",
        "def naive_massaging(d, temp2, names, need_pos, need_neg,label_y):\n",
        "    # add more records for all groups\n",
        "    # The smote algorithm to boost the coverage\n",
        "    for r in need_pos:\n",
        "        print(\"adding more positive\")\n",
        "    # add more positive records\n",
        "        # determine how many points to add\n",
        "        print(r)\n",
        "        diff = compute_diff_add_and_remove(r, temp2, 1, label_y, names)\n",
        "        diff =  round_int(diff)\n",
        "        # add more records\n",
        "        #0 for promotion\n",
        "        samples_to_add = get_depromotion(d, diff, r, names, label_y, flag_depro = 0)\n",
        "        print(\"Changed \" + str(len(samples_to_add)) +\" records\")\n",
        "        d = pd.concat([d, samples_to_add])\n",
        "        print(len(d))\n",
        "    for k in need_neg:\n",
        "        print(k)\n",
        "        print(\"adding more negative\")\n",
        "        diff = compute_diff_add_and_remove(k, temp2, 0, label_y, names)\n",
        "        diff =  round_int(diff)\n",
        "        #1 for demotion\n",
        "        samples_to_add = get_depromotion(d, diff, k, names, label_y, flag_depro = 1)\n",
        "        print(\"Changed \" + str(len(samples_to_add)) +\" records\")\n",
        "        d = pd.concat([d, samples_to_add])\n",
        "        print(len(d))\n",
        "    return d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6JgxVuTx8cj"
      },
      "source": [
        "## Run Algorithm Lattice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CvRKxY6ne5o"
      },
      "outputs": [],
      "source": [
        "#get all of the candidate groups possible with the combos and names\n",
        "\n",
        "new_train_data = copy.deepcopy(train_set)\n",
        "\n",
        "#iterate over all the names to get the temp2 df for each name\n",
        "for a in all_names_lst:\n",
        "  print(\"?????/////\")\n",
        "  print(a)\n",
        "  temp2, names = get_temp(new_train_data, all_names[a], compas_y)\n",
        "  temp, temp_g = get_temp_g(new_train_data, names, compas_y)\n",
        "  temp_g = temp_g[temp_g['cnt'] > filter_count]\n",
        "  # print(temp_g)\n",
        "  # print(temp2, names)\n",
        "  # print(temp)\n",
        "  lst_of_counts = compute_lst_of_counts(temp, names, compas_y)\n",
        "  start = time.time()\n",
        "  need_pos, need_neg = compute_problematic_opt(temp2, temp_g, names, compas_y, lst_of_counts)\n",
        "  end = time.time()\n",
        "  excute_time = end - start\n",
        "  print(\"The time to compute unfair group is {}\".format(str(excute_time)))\n",
        "  print(\"The sets of need pos and neg are\")\n",
        "  print(need_pos)\n",
        "  print(need_neg)\n",
        "  new_train_data['skewed'] = 0\n",
        "  new_train_data[\"diff\"] = 0\n",
        "  # print(\"started duplication\")\n",
        "  new_train_data = naive_massaging(new_train_data, temp2, names, need_pos, need_neg, compas_y)\n",
        "  # print(new_train_data)\n",
        "  # new_train_x = pd.DataFrame(new_train_data, columns = columns_compas)\n",
        "  # new_train_label = pd.DataFrame(new_train_data, columns = [compas_y])\n",
        "  # new_train_label = new_train_label[compas_y]\n",
        "  # new_train_label = new_train_label.astype('int')\n",
        "  # grid_new = DecisionTreeClassifier(criterion=\"entropy\",max_depth=6, random_state=17)\n",
        "  # grid_new.fit(new_train_x, new_train_label)\n",
        "  # test_predict = grid_new.predict(test_x)\n",
        "  # print(\"new trainset length\", len(new_train_x))\n",
        "  # print(\"fpr\", fpr_onegroup(list(test_label), test_predict))\n",
        "  # print(\"fnr\", fnr_onegroup(list(test_label), test_predict))\n",
        "  print(new_train_data[compas_y].value_counts())\n",
        "new_train_x = pd.DataFrame(new_train_data, columns = columns_all)\n",
        "new_train_label = pd.DataFrame(new_train_data, columns = [compas_y])\n",
        "new_train_label = new_train_label[compas_y]\n",
        "new_train_label = new_train_label.astype('int')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6hqSHlurjGO"
      },
      "source": [
        "### Results Lattice"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print()\n",
        "print(\"dt\")\n",
        "griddt.fit(new_train_x, new_train_label)\n",
        "print(\"best\", griddt.best_score_)\n",
        "\n",
        "test_predict = griddt.predict(test_x)\n",
        "print(\"fpr and fnr\")\n",
        "print(fpr_onegroup(list(test_label), test_predict))\n",
        "print(fnr_onegroup(list(test_label), test_predict))\n",
        "accuracy = accuracy_score(test_label, test_set['predicted'])\n",
        "print(\"accuracy\")\n",
        "print(accuracy)\n",
        "test_set['predicted'] = test_predict\n",
        "r,r2,r3 = div_results(\"Adult\",\"Massage-Lattice\",\"DT\")\n",
        "\n",
        "print()\n",
        "print(\"rf\")\n",
        "gridrf.fit(new_train_x, new_train_label)\n",
        "print(\"best\", gridrf.best_score_)\n",
        "test_predict = gridrf.predict(test_x)\n",
        "print(\"fpr and fnr\")\n",
        "print(fpr_onegroup(list(test_label), test_predict))\n",
        "print(fnr_onegroup(list(test_label), test_predict))\n",
        "accuracy = accuracy_score(test_label, test_set['predicted'])\n",
        "print(\"accuracy\")\n",
        "print(accuracy)\n",
        "test_set['predicted'] = test_predict\n",
        "r,r2,r3 = div_results(\"Adult\",\"Massage-Lattice\",\"RF\")\n",
        "\n",
        "print()\n",
        "print(\"logistic\")\n",
        "gridlg.fit(new_train_x, new_train_label)\n",
        "print(\"best\", gridlg.best_score_)\n",
        "test_predict = gridlg.predict(test_x)\n",
        "print(\"fpr and fnr\")\n",
        "print(fpr_onegroup(list(test_label), test_predict))\n",
        "print(fnr_onegroup(list(test_label), test_predict))\n",
        "accuracy = accuracy_score(test_label, test_set['predicted'])\n",
        "print(\"accuracy\")\n",
        "print(accuracy)\n",
        "test_set['predicted'] = test_predict\n",
        "r,r2,r3 = div_results(\"Adult\",\"Massage-Lattice\",\"LG\")\n",
        "\n",
        "print()\n",
        "print(\"svm\")\n",
        "clf.fit(new_train_x, new_train_label)\n",
        "# print(\"best\", svc.best_score_)\n",
        "test_predict = clf.predict(test_x)\n",
        "print(\"fpr and fnr\")\n",
        "print(fpr_onegroup(list(test_label), test_predict))\n",
        "print(fnr_onegroup(list(test_label), test_predict))\n",
        "accuracy = accuracy_score(test_label, test_set['predicted'])\n",
        "print(\"accuracy\")\n",
        "print(accuracy)\n",
        "test_set['predicted'] = test_predict\n",
        "s,s2,s3 = div_results(\"Adult\",\"Massage-Lattice\",\"SVM\")"
      ],
      "metadata": {
        "id": "OvXIDnsDsiz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r3"
      ],
      "metadata": {
        "id": "4r6mFOV43PzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eWTUWZzVDWZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfBfO1hhytVS"
      },
      "source": [
        "## Run Algorithm Leaf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6oMxp2hzVXD"
      },
      "outputs": [],
      "source": [
        "#get all of the candidate groups possible with the combos and names\n",
        "\n",
        "new_train_data = copy.deepcopy(train_set)\n",
        "\n",
        "#iterate over all the names to get the temp2 df for each name\n",
        "for a in [all_names_lst[0]]:\n",
        "  print(\"?????/////\")\n",
        "  print(a)\n",
        "  temp2, names = get_temp(new_train_data, all_names[a], compas_y)\n",
        "  temp, temp_g = get_temp_g(new_train_data, names, compas_y)\n",
        "  temp_g = temp_g[temp_g['cnt'] > filter_count]\n",
        "  # print(temp_g)\n",
        "  # print(temp2, names)\n",
        "  # print(temp)\n",
        "  lst_of_counts = compute_lst_of_counts(temp, names, compas_y)\n",
        "  start = time.time()\n",
        "  need_pos, need_neg = compute_problematic_opt(temp2, temp_g, names, compas_y, lst_of_counts)\n",
        "  end = time.time()\n",
        "  excute_time = end - start\n",
        "  print(\"The time to compute unfair group is {}\".format(str(excute_time)))\n",
        "  print(\"The sets of need pos and neg are\")\n",
        "  print(need_pos)\n",
        "  print(need_neg)\n",
        "  new_train_data['skewed'] = 0\n",
        "  new_train_data[\"diff\"] = 0\n",
        "  # print(\"started duplication\")\n",
        "  new_train_data = naive_massaging(new_train_data, temp2, names, need_pos, need_neg, compas_y)\n",
        "  # print(new_train_data)\n",
        "  # new_train_x = pd.DataFrame(new_train_data, columns = columns_compas)\n",
        "  # new_train_label = pd.DataFrame(new_train_data, columns = [compas_y])\n",
        "  # new_train_label = new_train_label[compas_y]\n",
        "  # new_train_label = new_train_label.astype('int')\n",
        "  # grid_new = DecisionTreeClassifier(criterion=\"entropy\",max_depth=6, random_state=17)\n",
        "  # grid_new.fit(new_train_x, new_train_label)\n",
        "  # test_predict = grid_new.predict(test_x)\n",
        "  # print(\"new trainset length\", len(new_train_x))\n",
        "  # print(\"fpr\", fpr_onegroup(list(test_label), test_predict))\n",
        "  # print(\"fnr\", fnr_onegroup(list(test_label), test_predict))\n",
        "  print(new_train_data[compas_y].value_counts())\n",
        "new_train_x = pd.DataFrame(new_train_data, columns = columns_all)\n",
        "new_train_label = pd.DataFrame(new_train_data, columns = [compas_y])\n",
        "new_train_label = new_train_label[compas_y]\n",
        "new_train_label = new_train_label.astype('int')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CM-vpguwyt2H"
      },
      "source": [
        "### Results Leaf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print()\n",
        "print(\"dt\")\n",
        "griddt.fit(new_train_x, new_train_label)\n",
        "print(\"best\", griddt.best_score_)\n",
        "\n",
        "test_predict = griddt.predict(test_x)\n",
        "print(\"fpr and fnr\")\n",
        "print(fpr_onegroup(list(test_label), test_predict))\n",
        "print(fnr_onegroup(list(test_label), test_predict))\n",
        "accuracy = accuracy_score(test_label, test_set['predicted'])\n",
        "print(\"accuracy\")\n",
        "print(accuracy)\n",
        "test_set['predicted'] = test_predict\n",
        "r,r2,r3 = div_results(\"Adult\",\"Massage-Leaf\",\"DT\")\n",
        "\n",
        "print()\n",
        "print(\"rf\")\n",
        "gridrf.fit(new_train_x, new_train_label)\n",
        "print(\"best\", gridrf.best_score_)\n",
        "test_predict = gridrf.predict(test_x)\n",
        "print(\"fpr and fnr\")\n",
        "print(fpr_onegroup(list(test_label), test_predict))\n",
        "print(fnr_onegroup(list(test_label), test_predict))\n",
        "accuracy = accuracy_score(test_label, test_set['predicted'])\n",
        "print(\"accuracy\")\n",
        "print(accuracy)\n",
        "test_set['predicted'] = test_predict\n",
        "r,r2,r3 = div_results(\"Adult\",\"Massage-Leaf\",\"RF\")\n",
        "\n",
        "print()\n",
        "print(\"logistic\")\n",
        "gridlg.fit(new_train_x, new_train_label)\n",
        "print(\"best\", gridlg.best_score_)\n",
        "test_predict = gridlg.predict(test_x)\n",
        "print(\"fpr and fnr\")\n",
        "print(fpr_onegroup(list(test_label), test_predict))\n",
        "print(fnr_onegroup(list(test_label), test_predict))\n",
        "accuracy = accuracy_score(test_label, test_set['predicted'])\n",
        "print(\"accuracy\")\n",
        "print(accuracy)\n",
        "test_set['predicted'] = test_predict\n",
        "r,r2,r3 = div_results(\"Adult\",\"Massage-Leaf\",\"LG\")\n",
        "\n",
        "print()\n",
        "print(\"svm\")\n",
        "clf.fit(new_train_x, new_train_label)\n",
        "# print(\"best\", svc.best_score_)\n",
        "test_predict = clf.predict(test_x)\n",
        "print(\"fpr and fnr\")\n",
        "print(fpr_onegroup(list(test_label), test_predict))\n",
        "print(fnr_onegroup(list(test_label), test_predict))\n",
        "accuracy = accuracy_score(test_label, test_set['predicted'])\n",
        "print(\"accuracy\")\n",
        "print(accuracy)\n",
        "test_set['predicted'] = test_predict\n",
        "s,s2,s3 = div_results(\"Adult\",\"Massage-Leaf\",\"SVM\")"
      ],
      "metadata": {
        "id": "g0N6uIH9slwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6oj1yzvyuNv"
      },
      "source": [
        "## Run Algorithm Top"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXWlcnvmzaef"
      },
      "outputs": [],
      "source": [
        "#get all of the candidate groups possible with the combos and names\n",
        "\n",
        "new_train_data = copy.deepcopy(train_set)\n",
        "\n",
        "#iterate over all the names to get the temp2 df for each name\n",
        "all_names_lst_top = find_top(all_names)\n",
        "for a in all_names_lst_top:\n",
        "  print(\"?????/////\")\n",
        "  print(a)\n",
        "  temp2, names = get_temp(new_train_data, all_names[a], compas_y)\n",
        "  temp, temp_g = get_temp_g(new_train_data, names, compas_y)\n",
        "  temp_g = temp_g[temp_g['cnt'] > filter_count]\n",
        "  # print(temp_g)\n",
        "  # print(temp2, names)\n",
        "  # print(temp)\n",
        "  lst_of_counts = compute_lst_of_counts(temp, names, compas_y)\n",
        "  start = time.time()\n",
        "  need_pos, need_neg = compute_problematic_opt(temp2, temp_g, names, compas_y, lst_of_counts)\n",
        "  end = time.time()\n",
        "  excute_time = end - start\n",
        "  print(\"The time to compute unfair group is {}\".format(str(excute_time)))\n",
        "  print(\"The sets of need pos and neg are\")\n",
        "  print(need_pos)\n",
        "  print(need_neg)\n",
        "  new_train_data['skewed'] = 0\n",
        "  new_train_data[\"diff\"] = 0\n",
        "  # print(\"started duplication\")\n",
        "  new_train_data = naive_massaging(new_train_data, temp2, names, need_pos, need_neg, compas_y)\n",
        "  # print(new_train_data)\n",
        "  # new_train_x = pd.DataFrame(new_train_data, columns = columns_compas)\n",
        "  # new_train_label = pd.DataFrame(new_train_data, columns = [compas_y])\n",
        "  # new_train_label = new_train_label[compas_y]\n",
        "  # new_train_label = new_train_label.astype('int')\n",
        "  # grid_new = DecisionTreeClassifier(criterion=\"entropy\",max_depth=6, random_state=17)\n",
        "  # grid_new.fit(new_train_x, new_train_label)\n",
        "  # test_predict = grid_new.predict(test_x)\n",
        "  # print(\"new trainset length\", len(new_train_x))\n",
        "  # print(\"fpr\", fpr_onegroup(list(test_label), test_predict))\n",
        "  # print(\"fnr\", fnr_onegroup(list(test_label), test_predict))\n",
        "  print(new_train_data[compas_y].value_counts())\n",
        "new_train_x = pd.DataFrame(new_train_data, columns = columns_all)\n",
        "new_train_label = pd.DataFrame(new_train_data, columns = [compas_y])\n",
        "new_train_label = new_train_label[compas_y]\n",
        "new_train_label = new_train_label.astype('int')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YylnNCsvyuuU"
      },
      "source": [
        "### Results Top"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print()\n",
        "print(\"dt\")\n",
        "griddt.fit(new_train_x, new_train_label)\n",
        "print(\"best\", griddt.best_score_)\n",
        "\n",
        "test_predict = griddt.predict(test_x)\n",
        "print(\"fpr and fnr\")\n",
        "print(fpr_onegroup(list(test_label), test_predict))\n",
        "print(fnr_onegroup(list(test_label), test_predict))\n",
        "accuracy = accuracy_score(test_label, test_set['predicted'])\n",
        "print(\"accuracy\")\n",
        "print(accuracy)\n",
        "test_set['predicted'] = test_predict\n",
        "r,r2,r3 = div_results(\"Adult\",\"Massage-Top\",\"DT\")\n",
        "\n",
        "print()\n",
        "print(\"rf\")\n",
        "gridrf.fit(new_train_x, new_train_label)\n",
        "print(\"best\", gridrf.best_score_)\n",
        "test_predict = gridrf.predict(test_x)\n",
        "print(\"fpr and fnr\")\n",
        "print(fpr_onegroup(list(test_label), test_predict))\n",
        "print(fnr_onegroup(list(test_label), test_predict))\n",
        "accuracy = accuracy_score(test_label, test_set['predicted'])\n",
        "print(\"accuracy\")\n",
        "print(accuracy)\n",
        "test_set['predicted'] = test_predict\n",
        "r,r2,r3 = div_results(\"Adult\",\"Massage-Top\",\"RF\")\n",
        "\n",
        "print()\n",
        "print(\"logistic\")\n",
        "gridlg.fit(new_train_x, new_train_label)\n",
        "print(\"best\", gridlg.best_score_)\n",
        "test_predict = gridlg.predict(test_x)\n",
        "print(\"fpr and fnr\")\n",
        "print(fpr_onegroup(list(test_label), test_predict))\n",
        "print(fnr_onegroup(list(test_label), test_predict))\n",
        "accuracy = accuracy_score(test_label, test_set['predicted'])\n",
        "print(\"accuracy\")\n",
        "print(accuracy)\n",
        "test_set['predicted'] = test_predict\n",
        "r,r2,r3 = div_results(\"Adult\",\"Massage-Top\",\"LG\")\n",
        "\n",
        "print()\n",
        "print(\"svm\")\n",
        "clf.fit(new_train_x, new_train_label)\n",
        "# print(\"best\", svc.best_score_)\n",
        "test_predict = clf.predict(test_x)\n",
        "print(\"fpr and fnr\")\n",
        "print(fpr_onegroup(list(test_label), test_predict))\n",
        "print(fnr_onegroup(list(test_label), test_predict))\n",
        "accuracy = accuracy_score(test_label, test_set['predicted'])\n",
        "print(\"accuracy\")\n",
        "print(accuracy)\n",
        "test_set['predicted'] = test_predict\n",
        "s,s2,s3 = div_results(\"Adult\",\"Massage-Top\",\"SVM\")"
      ],
      "metadata": {
        "id": "NWUjzEK5smxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hJzCNT06Jj3Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}